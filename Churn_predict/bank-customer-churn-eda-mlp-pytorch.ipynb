{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy pandas matplotlib seaborn scikit-learn imbalanced-learn torch optuna joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, \n",
    "    f1_score, accuracy_score, roc_curve, auc, make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import logging\n",
    "import warnings\n",
    "from torch.optim import lr_scheduler\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "########################### para KAGGLE ################################################################################################################\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicinário de Dados \n",
    "\n",
    "\n",
    "| **Variável**         | **Tipo**   | **Descrição**                                                                                                                                     |\n",
    "|-----------------------|------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| RowNumber            | int64      | Número do registro (linhas), sem efeito na construção de modelos.                                                                                |\n",
    "| CustomerId           | int64      | ID do cliente, sem efeito sobre o estudo.                                                                                                       |\n",
    "| Surname              | object     | Sobrenome do cliente, sem impacto na análise.                                                                                                   |\n",
    "| CreditScore          | int64      | Pontuação de crédito, pode indicar tendência de permanência de clientes com pontuação alta.                                                     |\n",
    "| Geography            | object     | Localização do cliente, pode influenciar a decisão de evasão.                                                                                   |\n",
    "| Gender               | object     | Gênero do cliente, possível influência na evasão.                                                                                               |\n",
    "| Age                  | int64      | Idade do cliente, clientes mais velhos tendem a permanecer.                                                                                     |\n",
    "| Tenure               | int64      | Anos que o cliente está no banco, clientes novos têm maior chance de evasão.                                                                    |\n",
    "| Balance              | float64    | Saldo na conta, pessoas com saldos altos são menos propensas a sair.                                                                            |\n",
    "| NumOfProducts        | int64      | Número de produtos adquiridos pelo cliente.                                                                                                    |\n",
    "| HasCrCard            | int64      | Indica se o cliente tem cartão de crédito, clientes com cartão são menos propensos à evasão.                                                    |\n",
    "| IsActiveMember       | int64      | Clientes ativos têm menor chance de evasão.                                                                                                    |\n",
    "| EstimatedSalary      | float64    | Salário estimado, clientes com salários mais altos tendem a permanecer.                                                                         |\n",
    "| Exited               | int64      | Indica se o cliente saiu ou não do banco, variável de predição (“churn”).                                                                       |\n",
    "| Complain             | int64      | Indica se o cliente fez reclamação.                                                                                                             |\n",
    "| Satisfaction Score   | int64      | Pontuação de satisfação com a resolução de reclamação.                                                                                          |\n",
    "| Card Type            | object     | Tipo de cartão que o cliente possui.                                                                                                            |\n",
    "| Points Earned        | int64      | Pontos ganhos pelo cliente.                                                                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:51:49.217695Z",
     "iopub.status.busy": "2024-12-21T05:51:49.217244Z",
     "iopub.status.idle": "2024-12-21T05:51:49.222777Z",
     "shell.execute_reply": "2024-12-21T05:51:49.221466Z",
     "shell.execute_reply.started": "2024-12-21T05:51:49.217658Z"
    }
   },
   "source": [
    "#  Análise Exploratória (EDA) & Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base de dados\n",
    "# base_original = pd.read_csv('/kaggle/input/Customer-Churn-Records.csv', sep=',') #KAGGLE\n",
    "base_original = pd.read_csv('C:/Users/jgeov/iCloudDrive/Treinamento/Treinamento Data Science/Projetos/Customer-Churn-Records.csv',sep=',') #LOCAL\n",
    "\n",
    "#configs para nao quebrar linhas no print do  df\n",
    "pd.set_option('display.expand_frame_repr', False) \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#primeiras linhas \n",
    "base_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensões da base de dados\n",
    "print(\"Numero de linhas:\", base_original.shape[0]) \n",
    "print(\"Numero de colunas:\", base_original.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando tipos dados originais \n",
    "base_original.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checando se há valores nulos \n",
    "base_original.isnull().sum()  \n",
    "#valores nulos nao encontrados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo estatistico da base original\n",
    "base_original.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpando variavéis que não tem interferencia na analise, sao meramente\n",
    "#identificadoras: RowNumber, CustomerId e Surname\n",
    "\n",
    "df = base_original[['CreditScore',\n",
    "                    'Gender',\n",
    "                    'Geography',\n",
    "                    'Age','Tenure',\n",
    "                    'Balance',\n",
    "                   'NumOfProducts',\n",
    "                    'HasCrCard',\n",
    "                    'IsActiveMember',\n",
    "                   'EstimatedSalary',\n",
    "                    'Complain',\n",
    "                    'Satisfaction Score',\n",
    "                   'Card Type',\n",
    "                    'Point Earned',\n",
    "                    'Exited'\n",
    "                   ]]\n",
    "\n",
    "# Resumo estatístico das variáveis quantitativas\n",
    "quanti = df[['EstimatedSalary', 'Balance', 'CreditScore', 'Age', 'Tenure', 'Point Earned']]\n",
    "resumo_estati_quant = quanti.describe().style.format(lambda x: f'{x:,.1f}'.replace(',', 'X').replace('.', ',').replace('X', '.')) # Formatação com 1 casa decimal e separadores invertidos\n",
    "\n",
    "resumo_estati_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Resumo estatistico das varaiveis Quali (\"tabelas\" de frequencias)\n",
    "\n",
    "quali = df[['HasCrCard', 'IsActiveMember', 'Geography','Gender'\n",
    "            ,'Complain','Exited','Card Type']]\n",
    "\n",
    "quali = quali.astype('object')\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['HasCrCard'].value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['IsActiveMember'].value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['Geography'].value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['Gender'].value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['Complain'].value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['Card Type'].value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(quali['Exited'].value_counts())\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando tipos das variaveis quali (para morrer de certeza) \n",
    "quali.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando Frequencia das variaveis categoricas \n",
    "\n",
    "def add_value_labels(ax):\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        color = p.get_facecolor()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height / 2.,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='center', fontsize=20, color='white', fontweight='bold',\n",
    "                bbox=dict(facecolor=color, edgecolor='none', alpha=0.7,\n",
    "                          boxstyle='round,pad=0.4', linewidth=1))\n",
    "\n",
    "plt.figure(figsize=(20, 25))\n",
    "\n",
    "# Geography\n",
    "plt.subplot(5, 2, 1)\n",
    "ax1 = plt.gca()\n",
    "ax1.set_title('Geography', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Geography', hue='Geography', palette='viridis', data=base_original, ax=ax1, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax1)\n",
    "\n",
    "# Gender\n",
    "plt.subplot(5, 2, 2)\n",
    "ax2 = plt.gca()\n",
    "ax2.set_title('Gender', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Gender', hue='Gender', palette='viridis', data=base_original, ax=ax2, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax2)\n",
    "\n",
    "# Complain\n",
    "plt.subplot(5, 2, 3)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('Complain', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Complain', hue='Complain', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "# HasCrCard\n",
    "plt.subplot(5, 2, 4)\n",
    "ax5 = plt.gca()\n",
    "ax5.set_title('HasCrCard', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='HasCrCard', hue='HasCrCard', palette='viridis', data=base_original, ax=ax5, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax5)\n",
    "\n",
    "# IsActiveMember\n",
    "plt.subplot(5, 2, 5)\n",
    "ax6 = plt.gca()\n",
    "ax6.set_title('IsActiveMember', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='IsActiveMember', hue='IsActiveMember', palette='viridis', data=base_original, ax=ax6, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax6)\n",
    "\n",
    "# Card Type\n",
    "plt.subplot(5, 2, 6)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('Card Type', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Card Type', hue='Card Type', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "# Exited\n",
    "plt.subplot(5, 2, 7)\n",
    "ax7 = plt.gca()\n",
    "ax7.set_title('Exited: churn variable', fontsize=22, fontweight='bold')\n",
    "custom_palette = ['green', 'red']\n",
    "sns.countplot(x='Exited', hue='Exited', palette=custom_palette, data=base_original, ax=ax7, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax7)\n",
    "\n",
    "ax7.set_xticks([0, 1])\n",
    "ax7.set_xticklabels(['Não', 'Sim'], fontsize=15, fontweight='bold')\n",
    "\n",
    "# Ajustando espaçamento\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Visualizando a Variável alvo em relação as demais variáveis \n",
    "\n",
    "plt.figure(figsize=(20, 25)) #tamanho do painel grafico\n",
    "\n",
    "#funcao de adicao de legenda no canto superior direito e garante rotulos \n",
    "def add_legend(ax):\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if not handles:\n",
    "        \n",
    "        # Se não houver handles, forca a adicao\n",
    "        handles = [plt.Rectangle((0,0),1,1, color=c) for c in ['green', 'red']]\n",
    "        labels = ['Not Exited', 'Exited']\n",
    "        \n",
    "    # Adiciona a legenda fora da área das barras\n",
    "    ax.legend(handles, labels, loc='upper left', fontsize=14, title='Exited', title_fontsize='13',  \n",
    "              bbox_to_anchor=(1.0, 1)) \n",
    "\n",
    "# Geography\n",
    "plt.subplot(5, 2, 1)\n",
    "counts = base_original.groupby(['Geography', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Geography', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Geography', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Gender\n",
    "plt.subplot(5, 2, 2)\n",
    "counts = base_original.groupby(['Gender', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca()) \n",
    "plt.title('Exited by Gender', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Gender', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# NumOfProducts\n",
    "plt.subplot(5, 2, 3)\n",
    "counts = base_original.groupby(['NumOfProducts', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by NumOfProducts', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('NumOfProducts', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# HasCrCard\n",
    "plt.subplot(5, 2, 4)\n",
    "counts = base_original.groupby(['HasCrCard', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by HasCrCard', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('HasCrCard', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# IsActiveMember\n",
    "plt.subplot(5, 2, 5)\n",
    "counts = base_original.groupby(['IsActiveMember', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca()) \n",
    "plt.title('Exited by IsActiveMember', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('IsActiveMember', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Complain\n",
    "plt.subplot(5, 2, 6)\n",
    "counts = base_original.groupby(['Complain', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Complain', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Complain', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Satisfaction Score\n",
    "plt.subplot(5, 2, 7)\n",
    "counts = base_original.groupby(['Satisfaction Score', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Satisfaction Score', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Satisfaction Score', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Card Type\n",
    "plt.subplot(5, 2, 8)\n",
    "counts = base_original.groupby(['Card Type', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Card Type', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Card Type', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Ajustar a distância entre os gráficos\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variaveis Dummies e Correlações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumizando\n",
    "\n",
    "# Suprime todos os warnings de futuro (deixa mais clean) \n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "# Lista de colunas a serem transformadas\n",
    "cols_to_transform = ['HasCrCard', 'IsActiveMember', 'Geography', 'Gender', 'Card Type']\n",
    "\n",
    "# Convertendo para string\n",
    "df.loc[:, cols_to_transform] = df.loc[:, cols_to_transform].astype(str)\n",
    "\n",
    "# one-hot encoding \n",
    "df_dummies = pd.get_dummies(df, columns=cols_to_transform, dtype=int, drop_first=False)\n",
    "\n",
    "# variável alvo 'Exited' para o tipo numérico\n",
    "df_dummies['Exited'] = df_dummies['Exited'].astype('int64')\n",
    "\n",
    "print(df_dummies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlação das variaveis Numéricas \n",
    "\n",
    "# Matrix com Mapa de calor \n",
    "correlation_matrix = df_dummies.corr().round(2)\n",
    "correlation_matrix\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\",\n",
    "                      cmap=plt.cm.Blues,\n",
    "                      annot_kws={'size': 15}, vmin=-1, vmax=1)\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=17)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=17)\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=17)\n",
    "plt.title('Correlação das Variáveis Quantitativas', fontsize=25)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# As variaveis a seguir apresentaram correlacao alta, para evitar multicolinearidade foram removidas \n",
    "df_dummies = df_dummies.drop(columns=['HasCrCard_0','IsActiveMember_0','Gender_Female','Complain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separação Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X ---> Variáveis explicativas \n",
    "#Y ---> Evento de estudo (variável TARGET, evento de estudo, ^y etc..)\n",
    "\n",
    "X = df_dummies .drop('Exited', axis=1)\n",
    "y =  df_dummies['Exited']\n",
    "\n",
    "\n",
    "#separando em treino e teste \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Visualizando a proporção de eventos de churn (TARGET) nas bases de TREINO e TESTE \n",
    "\n",
    "# Contando os valores \n",
    "churn_counts_train = y_train.value_counts()\n",
    "churn_counts_test = y_test.value_counts()\n",
    "\n",
    "\n",
    "# Criando o plot \n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# Adicionando título \n",
    "fig.suptitle('Proporção da Variável Churn entre Treino e Teste', fontsize=35)\n",
    "#definindo a paleta de cor \n",
    "cmap = plt.get_cmap('viridis', 2)\n",
    "\n",
    "\n",
    "\n",
    "# Gráfico da base de treino\n",
    "bars_train = axs[0].bar(churn_counts_train.index, churn_counts_train.values, color=cmap(range(2)))\n",
    "axs[0].set_title('Base de Treino', fontsize=25)\n",
    "axs[0].set_xlabel('Churn', fontsize=20)\n",
    "axs[0].set_ylabel('Contagem', fontsize=20)\n",
    "axs[0].set_xticks([0, 1])\n",
    "axs[0].set_xticklabels(['0', '1'], fontsize=20)\n",
    "# Ocultando os valores do eixo y\n",
    "axs[0].set_yticklabels([])\n",
    "\n",
    "# Adicionando rótulos de dados nas barras da base de treino com valor absoluto e percentual\n",
    "total_train = churn_counts_train.sum()\n",
    "for bar in bars_train:\n",
    "    count = int(bar.get_height())\n",
    "    percentage = round(count / total_train * 100)  # Arredonda a porcentagem\n",
    "    label = f'{count} ({percentage}%)'  # Exibe o valor absoluto e o percentual\n",
    "    axs[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, \n",
    "                label, ha='center', color='gray', fontsize=25, weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Gráfico da base de teste\n",
    "bars_test = axs[1].bar(churn_counts_test.index, churn_counts_test.values, color=cmap(range(2)))\n",
    "axs[1].set_title('Base de Teste', fontsize=25)\n",
    "axs[1].set_xlabel('Churn', fontsize=20)\n",
    "axs[1].set_ylabel('Contagem', fontsize=20)\n",
    "axs[1].set_xticks([0, 1])\n",
    "axs[1].set_xticklabels(['0', '1'], fontsize=20)\n",
    "# Ocultando os valores do eixo y\n",
    "axs[1].set_yticklabels([])\n",
    "\n",
    "# Adicionando rótulos de dados nas barras da base de teste com valor absoluto e percentual\n",
    "total_test = churn_counts_test.sum()\n",
    "for bar in bars_test:\n",
    "    count = int(bar.get_height())\n",
    "    percentage = round(count / total_test * 100)  # Arredonda a porcentagem\n",
    "    label = f'{count} ({percentage}%)'  # Exibe o valor absoluto e o percentual\n",
    "    axs[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, \n",
    "                label, ha='center', color='gray', fontsize=25, weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Deixa espaço para o título principal\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando Multicolinearidade na base de treino\n",
    "\n",
    "#as variaveis com alto correlacao foram removidas na primeira plotagem de matriz de corr, isso reduz o ruido no modelo\n",
    "\n",
    "# todo esse processo de tirar as variaveis que tem alta correlacao com a variavel target deve ser analisado se realmente é necessario em cada cenario\n",
    "# existem modelos que capturam bem isso e não são afetados pela multicolinearidade (pelo menos lidam bem) \n",
    "\n",
    "teste_multco_treino = pd.concat([X_train,y_train], axis = 1)\n",
    "\n",
    "correlation_matrix_treino = teste_multco_treino.corr().round(2)\n",
    "correlation_matrix_treino\n",
    "\n",
    "# Matrix com  mapa de calor \n",
    "plt.figure(figsize=(30, 20))\n",
    "heatmap = sns.heatmap(correlation_matrix_treino, annot=True, fmt=\".2f\",\n",
    "                      cmap=plt.cm.viridis_r, # paleta de cores viridis (ou viridis_r para o inverso de cores) é uma paleta especial \n",
    "                                             # para facilitar a visualizacao por pessoas com dificuldades visuais, como os daltonicos. \n",
    "                      annot_kws={'size': 15}, vmin=-1, vmax=1)\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=17)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=17)\n",
    "plt.title('Correlação das Variáveis Quantitativas na Base de Treino',fontsize=25)\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise e tratamento de Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% analise de outliers das variaveis na base de treino \n",
    "\n",
    "\n",
    "#Outliers podem prejudicar MLPs, pois distorcem o treinamento e causam grandes gradientes, \n",
    "#o que pode dificultar a minimizacao da funcao de perda e levar a sobreajuste (overfitting). Isso afeta a capacidade de generalizacao do modelo. \n",
    "#é importante remover ou transformar outliers e usar tecnicas de regularizacao para mitigar isso \n",
    "\n",
    "#lembrando que isso so ocorre com variaveis numericas, vai ficar claro no barplot, onde poderemos ver as medidas de posicao \n",
    "variaveis = [\n",
    "    'CreditScore',\n",
    "    'Age',\n",
    "    'Tenure',\n",
    "    'Balance',\n",
    "    'NumOfProducts',\n",
    "    'EstimatedSalary',\n",
    "    'Satisfaction Score',\n",
    "    'Point Earned' \n",
    "]\n",
    "\n",
    "# definindo tamenho dos subplots \n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# boxplots para cada variavel\n",
    "for i, var in enumerate(variaveis):\n",
    "    plt.subplot(3, 3, i + 1)  # 3 linhas e 3 colunas\n",
    "    sns.boxplot(y=teste_multco_treino[var],\n",
    "               boxprops=dict(facecolor='lightblue'))  # Cor interna do boxplot)  \n",
    "    plt.title(f'Boxplot {var}', fontsize=12)\n",
    "    \n",
    "#  título\n",
    "plt.suptitle('Análise de Outliers nas Variáveis(treino) - antes de \"winsorization\" ', fontsize=20)\n",
    "\n",
    "# Ajuste de layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95]) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##aplicando procedimento de truncamento ou winsorization nas variaveis com outliers\n",
    "\n",
    "#O procedimento de truncamento ou winsorization consiste em limitar os valores extremos de um conjunto de dados \n",
    "#(vai ficar claro nas duas aplicacoes de barplot a seguir ). \n",
    "#Para isso, e calculado um intervalo de valores aceitos com base no primeiro quartil (Q1) e no terceiro quartil (Q3)\n",
    "#Valores abaixo do limite inferior ou acima do limite superior sao ajustados para os respectivos limites, corrigindo assim os outliers \n",
    "#\"sem perda\" dos dados originais\n",
    "\n",
    "\n",
    "# Função que aplica winsorization\n",
    "def tratar_outliers(df, coluna):\n",
    "    Q1 = df[coluna].quantile(0.25)\n",
    "    Q3 = df[coluna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    # Substitui outliers pelo limite inferior ou superior\n",
    "    df[coluna] = np.where(df[coluna] < limite_inferior, limite_inferior, df[coluna])\n",
    "    df[coluna] = np.where(df[coluna] > limite_superior, limite_superior, df[coluna])\n",
    "\n",
    "\n",
    "\n",
    "# Aplicando a função nas variáveis \n",
    "variaveis_para_tratar = ['Age', 'CreditScore', 'NumOfProducts']\n",
    "for variavel in variaveis_para_tratar:\n",
    "    tratar_outliers(teste_multco_treino, variavel)\n",
    "\n",
    "\n",
    "# subplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "\n",
    "# boxplots separados para cada variável\n",
    "for i, var in enumerate(variaveis):\n",
    "    plt.subplot(3, 3, i + 1)  # 3 linhas e 3 colunas\n",
    "    sns.boxplot(y=teste_multco_treino[var],\n",
    "               boxprops=dict(facecolor='green'))  # Cor interna do boxplot)  # Usar o nome da variável diretamente\n",
    "    plt.title(f'Boxplot {var}', fontsize=12)\n",
    "\n",
    "\n",
    "# título geral\n",
    "plt.suptitle('Análise de Outliers nas Variáveis(treino) - depois de \"winsorization\" ', fontsize=20)\n",
    "\n",
    "\n",
    "# Ajustando layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando novamente as bases de treino e teste depois de tratar os outliers APENAS na base de TREINO  \n",
    "\n",
    "#treino\n",
    "X_train=teste_multco_treino.drop('Exited', axis=1)\n",
    "y_train=teste_multco_treino['Exited']\n",
    "\n",
    "#teste\n",
    "base_corrige_teste = pd.concat([X_test,y_test], axis = 1)\n",
    "X_test=base_corrige_teste.drop('Exited', axis=1)\n",
    "y_test=base_corrige_teste['Exited']\n",
    "\n",
    "\n",
    "print('---------------------------------------')\n",
    "print(X_train.isnull().sum())\n",
    "print('---------------------------------------')\n",
    "print('---------------------------------------')\n",
    "print(X_train.dtypes)\n",
    "print('---------------------------------------')\n",
    "print('---------------------------------------')\n",
    "print(y_train.value_counts())\n",
    "print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem: aplicando MLP com Pytorch\n",
    "\n",
    "O PyTorch é uma biblioteca de aprendizado profundo de código aberto, usada principalmente para o treinamento de redes neurais artificiais. Ele oferece flexibilidade e eficiência, permitindo a criação de modelos complexos e o treinamento acelerado com GPUs. O PyTorch é popular entre os pesquisadores devido à sua abordagem dinâmica de construção de grafos computacionais, facilitando a experimentação.\n",
    "\n",
    "Para mais informações, você pode acessar a documentação oficial do PyTorch:\n",
    "https://pytorch.org/docs/stable/\n",
    "\n",
    "\n",
    "OBS: o uso de GPU precisa ser corrigido, a conversao dos tensores esta apresentando problemas. \n",
    "Visto isso, o codigo tambem executa em CPU, basta nao ativar a GPU (CUDA) que ele seleciona a CPU automaticamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------')\n",
    "print(\"MLP - Multi-Layer Perceptron \")\n",
    "print('------------------------')\n",
    "\n",
    "# Desativando os warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='optuna')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='optuna')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# início\n",
    "start_time_utc = datetime.utcnow() - timedelta(hours=3)\n",
    "print('------------------------')\n",
    "print(\"Início:\", start_time_utc)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "######################################## PRE-PROCESSAMENTO E PREPARACAO NOS DADOS ################################################################### \n",
    "\n",
    "\n",
    "# Definir as variáveis de treinamento\n",
    "X_train = teste_multco_treino.drop('Exited', axis=1)\n",
    "y_train = teste_multco_treino['Exited']\n",
    "\n",
    "# Verificar e corrigir desalinhamento de índices em X_test e y_test antes da concatenação\n",
    "# Verificar e corrigir desalinhamento de índices em X_test e y_test antes da concatenação\n",
    "if not X_test.index.equals(y_test.index):\n",
    "    print(\"Índices de X_test e y_test não estavam alinhados. Realinhando y_test.\")\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "else:\n",
    "    print(\"Índices de X_test e y_test já estavam alinhados.\")\n",
    "\n",
    "# Concatenar os dados corrigidos para criar a base de teste\n",
    "base_corrige_teste = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Redefinir X_test e y_test com base na base corrigida\n",
    "X_test = base_corrige_teste.drop('Exited', axis=1)\n",
    "y_test = base_corrige_teste['Exited']\n",
    "\n",
    "\n",
    "# Configurando dispositivo para PyTorch, aqui ele verifica a disponibilidade de GPU do kaggle e usa se tiver \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Testando opções de scaler (essas outras foram testadas mas nao foram melhores a principio)\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler = RobustScaler()\n",
    "\n",
    "# Escalonando os dados de treinamento e o conjunto de teste \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertendo para tensores PyTorch e movendo para o dispositivo\n",
    "bar = tqdm(total=1, desc=\"Processando tensores\", ncols=100, position=0) #barra de progresso plotada, ajuda a monitorar\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)  \n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)  \n",
    "\n",
    "bar.update(1) #incrementa barra\n",
    "bar.close()#fecha barra\n",
    "\n",
    "# Verificando os shapes dos dados depois de tensores criados e dispositivo ativado\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape} | device: {X_train_tensor.device}\")\n",
    "print(f\"y_train_tensor shape: {y_train_tensor.shape} | device: {y_train_tensor.device}\")\n",
    "print(f\"X_test_tensor shape: {X_test_tensor.shape} | device: {X_test_tensor.device}\")\n",
    "print(f\"y_test_tensor shape: {y_test_tensor.shape} | device: {y_test_tensor.device}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################# MODELAGEM ################################################################### \n",
    "\n",
    "bar = tqdm(total=1, desc=\"Criando modelo\", ncols=100, position=0)# Inicializando a barra de progresso\n",
    "\n",
    "# Função para salvar o modelo usando o método 'torch.save'\n",
    "def save_best_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Modelo salvo em {filepath}\")\n",
    "\n",
    "\n",
    "# Definir a classe MLP do modelo\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, activation, dropout_rate=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_size = X_train_tensor.shape[1]  \n",
    "        for units in hidden_layer_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, units))\n",
    "            self.layers.append(nn.Dropout(p=dropout_rate))  #Aplica dropout\n",
    "            input_size = units\n",
    "        self.output = nn.Linear(input_size, 1)  #Camada de saída\n",
    "        self.activation_fn = self.get_activation_function(activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation_fn(layer(x))  # Ativação após cada camada linear\n",
    "        x = self.output(x)  \n",
    "        return x\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        activation_dict = {\n",
    "            'relu': torch.relu,\n",
    "            'tanh': torch.tanh,\n",
    "            'sigmoid': torch.sigmoid,\n",
    "            'selu': torch.selu,\n",
    "            'gelu': torch.nn.functional.gelu,\n",
    "            'leaky_relu': torch.nn.functional.leaky_relu,\n",
    "            'swish': torch.nn.functional.silu,\n",
    "        }\n",
    "        return activation_dict.get(activation, torch.relu)  # Default e Relu so pra morrer de certeza que nao va da erro e a relu e a mais comum\n",
    "\n",
    "# Função para criar o modelo\n",
    "def create_model(\n",
    "    hidden_layer_sizes,\n",
    "    activation,\n",
    "    solver,\n",
    "    alpha,\n",
    "    learning_rate_init,\n",
    "    max_iter,\n",
    "    batch_size,\n",
    "    momentum,\n",
    "    learning_rate_scheduler,\n",
    "    weight_decay,\n",
    "    step_size,\n",
    "    gamma,\n",
    "    early_stopping,\n",
    "    validation_fraction,\n",
    "    dropout_rate,\n",
    "    patience,\n",
    "    imbalance_ratio=1.0  \n",
    "):\n",
    "\n",
    "    # Calcular pesos da classe para aplicacao da funcao de peso para classe minoritaria (BCEWithLogitsLoss)\n",
    "    count_neg = (y_train_tensor == 0).sum().item()  # Número da classe 0\n",
    "    count_pos = (y_train_tensor == 1).sum().item()  # Número da classe 1\n",
    "    pos_weight = torch.tensor([count_neg / count_pos], device=device)  # Ponderação para a classe minoritária\n",
    "        \n",
    "    # Instanciar o modelo\n",
    "    model = MLP(hidden_layer_sizes, activation, dropout_rate).to(device)\n",
    "    \n",
    "    # Função de perda com peso para a classe minoritária\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Configurar o otimizador\n",
    "    optimizers = {\n",
    "        'adam': optim.Adam(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay),\n",
    "        'sgd': optim.SGD(model.parameters(), lr=learning_rate_init, momentum=momentum, weight_decay=weight_decay),\n",
    "        'rmsprop': optim.RMSprop(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay),\n",
    "        'nadam': optim.NAdam(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay),\n",
    "    }\n",
    "    optimizer = optimizers.get(solver, optim.Adam(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay))\n",
    "    \n",
    "    # Configurar o scheduler\n",
    "    schedulers = {\n",
    "        'StepLR': lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma),\n",
    "        'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iter),\n",
    "        'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=gamma, patience=16, verbose=True),\n",
    "        'CosineAnnealingWarmRestarts': lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    }\n",
    "    scheduler = schedulers.get(learning_rate_scheduler, None)\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n",
    "\n",
    "bar.update(1)  \n",
    "bar.close()\n",
    "\n",
    "\n",
    "# Função de objetivo para o Optuna\n",
    "bar = tqdm(total=1, desc=\"Criando hiperparametros Optuna E validação cruzada\", ncols=100, position=0) \n",
    "\n",
    "bar2=None\n",
    "def objective(trial):\n",
    "    global bar2\n",
    "    try:\n",
    "        # Definindo os parâmetros a serem otimizados\n",
    "        hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(100, 50), (200, 100), (300, 200), (500, 400), (600, 500), (700, 600), (800, 700)])\n",
    "        activation = trial.suggest_categorical('activation', ['leaky_relu', 'swish','gelu'])\n",
    "        solver = trial.suggest_categorical('solver', ['adam', 'sgd','rmsprop'])\n",
    "        alpha = trial.suggest_loguniform('alpha', 1e-4, 1e-2)\n",
    "        learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-6, 1e-4)\n",
    "        max_iter = trial.suggest_categorical('max_iter', [2000, 3000, 4000, 5000, 6000])\n",
    "        batch_size = trial.suggest_categorical('batch_size', [256, 512])\n",
    "        momentum = trial.suggest_uniform('momentum', 0.7, 0.9)\n",
    "        learning_rate_scheduler = trial.suggest_categorical('learning_rate_scheduler', ['ReduceLROnPlateau','StepLR', None])\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 1e-4, 1e-2)\n",
    "        step_size = trial.suggest_int('step_size', 20, 60)\n",
    "        gamma = trial.suggest_uniform('gamma', 0.5, 0.999)\n",
    "        early_stopping = trial.suggest_categorical('early_stopping', [True])\n",
    "        validation_fraction = trial.suggest_uniform('validation_fraction', 0.2, 0.5)\n",
    "        dropout_rate = trial.suggest_uniform('dropout_rate', 0.5, 0.8)\n",
    "        patience = trial.suggest_int('patience', 10, 20)\n",
    "\n",
    "\n",
    "        \n",
    "        # Criação do modelo\n",
    "        model, criterion, optimizer, scheduler = create_model(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            max_iter=max_iter,\n",
    "            batch_size=batch_size,\n",
    "            momentum=momentum,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            weight_decay=weight_decay,\n",
    "            step_size=step_size,\n",
    "            gamma=gamma,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            dropout_rate=dropout_rate,\n",
    "            patience=patience)\n",
    "        \n",
    "        \n",
    "        # Função de treinamento e validação cruzada\n",
    "\n",
    "        def train_and_evaluate_cross_validation(epochs, patience):\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            auc_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            \n",
    "            # Instanciando o undersampler\n",
    "            #undersampler = RandomUnderSampler(random_state=42)\n",
    "            \n",
    "            # Instanciando o SMOTE\n",
    "            # smote = SMOTE(random_state=42)  \n",
    "  \n",
    "            # Instanciando o SMOTE ADASYN\n",
    "            adasyn = ADASYN(random_state=42)\n",
    "        \n",
    "            for train_index, val_index in skf.split(X_train_tensor.cpu().numpy(), y_train_tensor.cpu().numpy()):\n",
    "                X_train_cv, X_val_cv = X_train_tensor[train_index].to(device), X_train_tensor[val_index].to(device)\n",
    "                y_train_cv, y_val_cv = y_train_tensor[train_index].to(device), y_train_tensor[val_index].to(device)\n",
    "\n",
    "                #  # aplicar undersampler :\n",
    "                #X_train_cv_res, y_train_cv_res = undersampler.fit_resample(X_train_cv.cpu().numpy(), y_train_cv.cpu().numpy())\n",
    "                \n",
    "                # aplicar SMOTE :\n",
    "                # X_train_cv_res, y_train_cv_res = smote.fit_resample(X_train_cv.cpu().numpy(), y_train_cv.cpu().numpy())\n",
    "                \n",
    "                # aplicar SMOTE ADASYN:\n",
    "                X_train_cv_res, y_train_cv_res = adasyn.fit_resample(X_train_cv.cpu().numpy(), y_train_cv.cpu().numpy())\n",
    "\n",
    "                \n",
    "                X_train_cv_res = torch.tensor(X_train_cv_res).float().to(device)\n",
    "                y_train_cv_res = torch.tensor(y_train_cv_res).float().to(device)\n",
    "        \n",
    "                # Treinamento do modelo\n",
    "                model.to(device)\n",
    "                model.train()\n",
    "                train_dataset = TensorDataset(X_train_cv_res, y_train_cv_res)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "                # Implementação de Early Stopping\n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(X_batch).squeeze()\n",
    "                        loss = criterion(output, y_batch.to(device))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "        \n",
    "                    # Avaliação da perda de validação para early stopping\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_preds = model(X_val_cv).squeeze().cpu().numpy()\n",
    "                        val_labels = y_val_cv.cpu().numpy()\n",
    "                        val_loss = criterion(torch.tensor(val_preds), torch.tensor(val_labels).float()).item()\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            patience_counter = 0\n",
    "                        else:\n",
    "                            patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "                \n",
    "                    # Avaliação no conjunto de validação após treinamento\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_logits = model(X_val_cv).squeeze().cpu().numpy()  # Obter logits brutos\n",
    "                        val_labels = y_val_cv.cpu().numpy()\n",
    "                        \n",
    "                        # função sigmoide para converter logits em probabilidades\n",
    "                        val_preds = torch.sigmoid(torch.tensor(val_logits)).numpy()\n",
    "                        \n",
    "                        # Fixando o threshold em 0.5\n",
    "                        threshold = 0.5\n",
    "                        \n",
    "                        # Convertendo probabilidades em previsões binárias\n",
    "                        val_preds_binary = (val_preds > threshold).astype(int)\n",
    "                        \n",
    "                        #  métricas com as previsões ajustadas\n",
    "                        auc = roc_auc_score(val_labels, val_preds)  \n",
    "                        recall = recall_score(val_labels, val_preds_binary)\n",
    "                        f1 = f1_score(val_labels, val_preds_binary)\n",
    "                        \n",
    "                        # Armazenando as métricas\n",
    "                        f1_scores.append(f1)\n",
    "                        auc_scores.append(auc)\n",
    "                        recall_scores.append(recall)\n",
    "        \n",
    "            return np.mean(auc_scores), np.mean(recall_scores), np.mean(f1_scores)\n",
    "        \n",
    "        #validação cruzada\n",
    "        mean_auc, mean_recall, mean_f1 = train_and_evaluate_cross_validation(epochs=250, patience=16)\n",
    "\n",
    "        \n",
    "        #métrica composta\n",
    "        score = 0.85 * mean_auc + 0.15 * mean_recall\n",
    "        \n",
    "        # resultados ao Optuna\n",
    "        trial.report(score, 0)  #  step 0\n",
    "        trial.report(mean_recall, 1)  #step 1\n",
    "        trial.report(mean_f1, 2)     #  step 2\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} foi interrompido devido ao pruning.\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Salvando o modelo no trial para ser acessado após a otimização\n",
    "        trial.set_user_attr('model', model)\n",
    "        \n",
    "        \n",
    "        bar2.update(1)\n",
    "        \n",
    "        # Retorna a métrica composta\n",
    "        return score \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Captura o tipo de erro e a mensagem de erro\n",
    "        error_type = type(e).__name__\n",
    "        error_message = str(e)\n",
    "        \n",
    "        # Captura o traceback completo para mais detalhes\n",
    "        error_traceback = traceback.format_exc()\n",
    "    \n",
    "        # Exibe o erro completo no console\n",
    "        print(f\"Trial {trial.number} falhou com erro: {error_type}\")\n",
    "        print(f\"Mensagem do erro: {error_message}\")\n",
    "        print(\"Traceback completo:\")\n",
    "        print(error_traceback)\n",
    "    \n",
    "        bar2.update(1) # Atualiza a barra de progresso após cada trial\n",
    "    \n",
    "        # Retorna valor negativo infinito para continuar a otimização\n",
    "        return float('-inf')\n",
    "     \n",
    "    # Salvar o modelo após a otimização\n",
    "    # torch.save(model.state_dict(), 'best_model.pth')  ---- salva apenas os parametros, \n",
    "    #se o modelo for ser usado fora desse processamento/codigo\n",
    "    # o ideal é salva-lo em de outra forma (torch.save(model, 'best_model.pth'))\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    \n",
    "bar.update(1)\n",
    "bar.close()  # Fechar a barra\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################## Definindo o estudo do Optuna############################################\n",
    "n_trials = 35\n",
    "bar2 = tqdm(total=n_trials, desc=\"Otimização em andamento\", ncols=100, position=0)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\") \n",
    "study.optimize(objective, n_trials=n_trials, catch=(Exception,))\n",
    "\n",
    "best_model = study.best_trial.user_attrs['model']\n",
    "\n",
    "# Salvando o melhor modelo otimizado\n",
    "save_best_model(best_model, 'best_model.pth')\n",
    "\n",
    "# Resultados\n",
    "best_params = study.best_params\n",
    "print(f\"Melhores parâmetros: {best_params}\")\n",
    "\n",
    "bar2.close()\n",
    "bar.close()\n",
    "\n",
    "print('######## fim de estudo do OPTUNA ###########')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Inicializando a barra de progresso\n",
    "epochs = 250 #quantidade de epocas\n",
    "\n",
    "\n",
    "\n",
    "bar = tqdm(total=epochs, desc=\"########### Treinamento final ##############\", ncols=100, position=0)\n",
    "\n",
    "\n",
    "# Pré-processamento: Divisão dos dados e normalização\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Convertendo os dados para tensores do PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "# Criando os datasets para treinamento e validação\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Criando os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "# Função de treinamento de uma época\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clipping de gradientes para evitar explosão\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step()\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "# Função de validação\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "    return val_loss / len(val_loader.dataset)\n",
    "\n",
    "# Inicializando o modelo, otimizador e scheduler\n",
    "model, criterion, optimizer, scheduler = create_model(**best_params)\n",
    "model.to(device)  # Movendo o modelo para o dispositivo\n",
    "\n",
    "# Listas para armazenar as perdas\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Loop de treinamento final\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Ajuste do scheduler com a perda de validação, se aplicável\n",
    "    if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Salvando o melhor modelo\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\") #aqui salva o OrderedDict (sopmente os parametros, bom pra usar no mesmo codigo enquanto reina e testa o 80/20)\n",
    "        torch.save(model, 'best_model_inteiro.pth') #salva o modelo INTEIRO com dict e tudo, bom para aplicar o modelo em producao\n",
    "        torch.save(scaler, 'scaler.pth')  # Salvar scaler usado no treino\n",
    "\n",
    "\n",
    "    bar.update(1)\n",
    "\n",
    "bar.close()\n",
    "\n",
    "# Carregar o melhor modelo após o treinamento\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Plotando as perdas por época\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', color='blue', linestyle='-', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange', linestyle='-', marker='o')\n",
    "plt.title('Training and Validation Loss per Epoch', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "print(\"###########################Treinamento final concluído com sucesso!######################################\")\n",
    "print(f\"Menor perda de validação: {best_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Avaliação final\n",
    "print(\"###########################Avaliando no conjunto de teste!######################################\")\n",
    "# Carregando o melhor modelo\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Avaliando no conjunto de teste\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(X_test_tensor.to(device)).cpu().numpy().squeeze()  # Isso são logits\n",
    "    y_pred_prob_test = torch.sigmoid(torch.tensor(y_pred_logits)).cpu().numpy()  # Aplicando a sigmoide para obter probabilidades\n",
    "\n",
    "\n",
    "\n",
    "#threshold \n",
    "# Fixando o threshold em 0.5\n",
    "threshold = 0.5\n",
    "y_pred_class_test = (y_pred_prob_test > threshold).astype(int)\n",
    "\n",
    "# Calculando métricas\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_class_test)\n",
    "precision = precision_score(y_test, y_pred_class_test)\n",
    "recall = recall_score(y_test, y_pred_class_test)\n",
    "f1 = f1_score(y_test, y_pred_class_test)\n",
    "\n",
    "# Exibindo métricas\n",
    "print(f\"ROC AUC (Teste): {roc_auc:.4f}\")\n",
    "print(f\"Acurácia (Teste): {accuracy:.4f}\")\n",
    "print(f\"Precisão (Teste): {precision:.4f}\")\n",
    "print(f\"Recall (Teste): {recall:.4f}\")\n",
    "print(f\"F1-Score (Teste): {f1:.4f}\")\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_class_test)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Exited', 'Exited'], yticklabels=['Não Exited', 'Exited'])\n",
    "plt.xlabel('Predição')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão no teste')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Cálculo da curva ROC para o conjunto de teste\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_prob_test)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Gráfico ROC teste\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='red', label=f'ROC Curve - Teste (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('Curva ROC - Teste')\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################FAZENDO PREDIÇÕES SIMULANDO UMA APLICAÇÃO EM PRODUÇÃO EM DADOS NOVOS ##############################\n",
    "\n",
    "#NO CASO COMO NAO TEM DADOS NOVOS, FOI APLICADO NA BASE INTEIRA ORIGINAL\n",
    "print(\"##########Simulando predições em 'produção'#####################\")\n",
    "\n",
    "# Carregar o modelo treinado inteiro e em modo de avaliação\n",
    "model = torch.load('best_model_inteiro.pth')\n",
    "model.eval()  \n",
    "\n",
    "# Carregar o scaler utilizado no treinamento\n",
    "scaler = torch.load('scaler.pth')\n",
    "\n",
    "# Carregar os novos dados\n",
    "df_new = base_corrige_teste\n",
    "\n",
    "#df_new = df_new.head(5000) #para testar em apenas 5000 observacoes \n",
    "\n",
    "# Separar as features x e ^y\n",
    "X_new = df_new.drop(columns=['Exited'])  \n",
    "y_true = df_new['Exited'].values  # Valores reais para avaliação\n",
    "\n",
    "# Aplicar o mesmo pré-processamento (normalização)\n",
    "X_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Converter para tensor PyTorch\n",
    "input_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "# Obter predições do modelo\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(input_tensor).numpy()  # Logits previstos\n",
    "    y_pred_prob = torch.sigmoid(torch.tensor(y_pred_logits)).numpy()  # Aplicando sigmoide para obter probabilidades\n",
    "\n",
    "# Converter probabilidades em classes (0 ou 1)\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calcular matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_class)\n",
    "\n",
    "# Visualizar matriz de confusão \n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão - Dados de simulacao de Producao')\n",
    "plt.show()\n",
    "\n",
    "# Calcular a curva ROC e a AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotar a curva ROC\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Linha de referência\n",
    "plt.xlabel('Falso Positivo Rate')\n",
    "plt.ylabel('Verdadeiro Positivo Rate')\n",
    "plt.title('Curva ROC - Dados de simulacao de Producao')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calcular a acurácia total\n",
    "accuracy_positive = accuracy_score(y_true, y_pred_class)\n",
    "print(f\"Acurácia total - Dados de simulacao de Producao: {accuracy_positive * 100:.2f}%\")\n",
    "\n",
    "# Calcular o recall da classe positiva\n",
    "positive_class_recall = recall_score(y_true, y_pred_class)\n",
    "print(f\"Recall da classe positiva - Dados de simulacao de Producao: {positive_class_recall * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Fim\n",
    "end_time_utc = datetime.utcnow() - timedelta(hours=3)\n",
    "print('------------------------')\n",
    "print(\"Início:\", start_time_utc)\n",
    "print(\"Fim:   \", end_time_utc)\n",
    "print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3197960,
     "sourceId": 5550559,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
