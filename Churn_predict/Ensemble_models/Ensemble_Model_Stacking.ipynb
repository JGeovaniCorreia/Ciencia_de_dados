{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Customer Churn -  Ensemble Learning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![churn_bank.jpg](attachment:churn_bank.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:11.067609Z",
     "iopub.status.busy": "2025-01-06T02:58:11.067254Z",
     "iopub.status.idle": "2025-01-06T02:58:17.183118Z",
     "shell.execute_reply": "2025-01-06T02:58:17.182230Z",
     "shell.execute_reply.started": "2025-01-06T02:58:11.067582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import scipy.sparse\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from xgboost import plot_importance\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler #, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score,\n",
    "    roc_curve, confusion_matrix, auc\n",
    ")\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "import pickle\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n",
    "\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### para KAGGLE ################################################################################################################\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "########################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicin√°rio de Dados \n",
    "\n",
    "\n",
    "| **Vari√°vel**         | **Tipo**   | **Descri√ß√£o**                                                                                                                                     |\n",
    "|-----------------------|------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| RowNumber            | int64      | N√∫mero do registro (linhas), sem efeito na constru√ß√£o de modelos.                                                                                |\n",
    "| CustomerId           | int64      | ID do cliente, sem efeito sobre o estudo.                                                                                                       |\n",
    "| Surname              | object     | Sobrenome do cliente, sem impacto na an√°lise.                                                                                                   |\n",
    "| CreditScore          | int64      | Pontua√ß√£o de cr√©dito, pode indicar tend√™ncia de perman√™ncia de clientes com pontua√ß√£o alta.                                                     |\n",
    "| Geography            | object     | Localiza√ß√£o do cliente, pode influenciar a decis√£o de evas√£o.                                                                                   |\n",
    "| Gender               | object     | G√™nero do cliente, poss√≠vel influ√™ncia na evas√£o.                                                                                               |\n",
    "| Age                  | int64      | Idade do cliente, clientes mais velhos tendem a permanecer.                                                                                     |\n",
    "| Tenure               | int64      | Anos que o cliente est√° no banco, clientes novos t√™m maior chance de evas√£o.                                                                    |\n",
    "| Balance              | float64    | Saldo na conta, pessoas com saldos altos s√£o menos propensas a sair.                                                                            |\n",
    "| NumOfProducts        | int64      | N√∫mero de produtos adquiridos pelo cliente.                                                                                                    |\n",
    "| HasCrCard            | int64      | Indica se o cliente tem cart√£o de cr√©dito, clientes com cart√£o s√£o menos propensos √† evas√£o.                                                    |\n",
    "| IsActiveMember       | int64      | Clientes ativos t√™m menor chance de evas√£o.                                                                                                    |\n",
    "| EstimatedSalary      | float64    | Sal√°rio estimado, clientes com sal√°rios mais altos tendem a permanecer.                                                                         |\n",
    "| Exited               | int64      | Indica se o cliente saiu ou n√£o do banco, vari√°vel de predi√ß√£o (‚Äúchurn‚Äù).                                                                       |\n",
    "| Complain             | int64      | Indica se o cliente fez reclama√ß√£o.                                                                                                             |\n",
    "| Satisfaction Score   | int64      | Pontua√ß√£o de satisfa√ß√£o com a resolu√ß√£o de reclama√ß√£o.                                                                                          |\n",
    "| Card Type            | object     | Tipo de cart√£o que o cliente possui.                                                                                                            |\n",
    "| Points Earned        | int64      | Pontos ganhos pelo cliente.                                                                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria (EDA) & Data Wrangling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:17.184914Z",
     "iopub.status.busy": "2025-01-06T02:58:17.184385Z",
     "iopub.status.idle": "2025-01-06T02:58:17.242602Z",
     "shell.execute_reply": "2025-01-06T02:58:17.241910Z",
     "shell.execute_reply.started": "2025-01-06T02:58:17.184876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# base_original = pd.read_csv('/kaggle/input/Customer-Churn-Records.csv', sep=',') #KAGGLE\n",
    "base_original = pd.read_csv('C:/Users/jgeov/iCloudDrive/Treinamento/Treinamento Data Science/Projetos/Customer-Churn-Records.csv',sep=',') #LOCAL\n",
    "\n",
    "#configs para nao quebrar linhas no print do  df\n",
    "pd.set_option('display.expand_frame_repr', False) \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#primeiras linhas \n",
    "base_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando primeiras impress√µes da base de dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:17.244145Z",
     "iopub.status.busy": "2025-01-06T02:58:17.243929Z",
     "iopub.status.idle": "2025-01-06T02:58:17.249149Z",
     "shell.execute_reply": "2025-01-06T02:58:17.248352Z",
     "shell.execute_reply.started": "2025-01-06T02:58:17.244127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Dimens√µes \n",
    "print(\"Numero de linhas:\", base_original.shape[0]) \n",
    "print(\"Numero de colunas:\", base_original.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:17.250489Z",
     "iopub.status.busy": "2025-01-06T02:58:17.250208Z",
     "iopub.status.idle": "2025-01-06T02:58:17.263709Z",
     "shell.execute_reply": "2025-01-06T02:58:17.262879Z",
     "shell.execute_reply.started": "2025-01-06T02:58:17.250461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tipos\n",
    "base_original.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:17.264717Z",
     "iopub.status.busy": "2025-01-06T02:58:17.264514Z",
     "iopub.status.idle": "2025-01-06T02:58:17.392219Z",
     "shell.execute_reply": "2025-01-06T02:58:17.391568Z",
     "shell.execute_reply.started": "2025-01-06T02:58:17.264700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#checando se h√° valores nulos \n",
    "base_original.isnull().sum()  \n",
    "#valores nulos nao encontrados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contando a quantidade de zeros em cada coluna para verificar se elas tem \n",
    "# informacao suficiente para entrar no modelo futuramente\n",
    "\n",
    "for col in base_original.columns:\n",
    "    zero_count = (base_original[col] == 0).sum()\n",
    "    print(\"\")\n",
    "    print(f\" '{col}': {zero_count} valores zero\")\n",
    "\n",
    "#nao foi constatao nada muito impactante, as variaveis com mais zero sao categoriacas (binarias) \n",
    "# Balance seria a unica a se preocupar, mas vamos manter. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:17.427362Z",
     "iopub.status.busy": "2025-01-06T02:58:17.427052Z",
     "iopub.status.idle": "2025-01-06T02:58:17.437914Z",
     "shell.execute_reply": "2025-01-06T02:58:17.437076Z",
     "shell.execute_reply.started": "2025-01-06T02:58:17.427328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#removidas por serem meramente identificadoras: RowNumber, CustomerId e Surname\n",
    "\n",
    "#removida Gender por poder inviesar o modelo de alguma formna descriminativa, √© uma boa pratica de LGPD nao usar dados sensiveis como esse etc. \n",
    " \n",
    "\n",
    "df = base_original[['CreditScore',\n",
    "                    #'Gender',\n",
    "                    'Geography',\n",
    "                    'Age',\n",
    "                    'Tenure',\n",
    "                    'Balance',\n",
    "                   'NumOfProducts',\n",
    "                    'HasCrCard',\n",
    "                    'IsActiveMember',\n",
    "                   'EstimatedSalary',\n",
    "                    'Complain',\n",
    "                    'Satisfaction Score',\n",
    "                   'Card Type',\n",
    "                    'Point Earned',\n",
    "                    'Exited'\n",
    "                   ]]\n",
    "\n",
    "# Resumo estat√≠stico \n",
    "quanti = df[['EstimatedSalary', 'Balance', 'CreditScore', 'Age', 'Tenure', 'Point Earned']]\n",
    "resumo_estati_quant = quanti.describe().style.format(lambda x: f'{x:,.1f}'.replace(',', 'X').replace('.', ',').replace('X', '.')) # Formata√ß√£o com 1 casa decimal e separadores invertidos\n",
    "\n",
    "resumo_estati_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Resumo estatistico de variaveis qualitativas (frequancias)\n",
    "\n",
    "* Os resumos estatisticos sao importantes para primeiras nocoes de desbalance, a amplitude e distribuicao de valores\n",
    "minimos maximos e um breve entendimento se serao necessarios tratamentos nessas variaveis, decorrentes dessas observacoes; \n",
    "\n",
    "* Podemos notar que a principio as ditribuicoes nao sao absurdas e o desbalance esta pricipalemnte nas variavies Complain e Exited (variavel alvo do estudo, a chamaremos de churn) indicando que sera necessario tratar isso;\n",
    "\n",
    "* Franca tem mais observacoes que os demais paises; \n",
    "\n",
    "* A maioria dos clientes tem cartao de credito; \n",
    "\n",
    "* A maioria dos clientes tem entre 1 e 2 produtos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:58:17.497672Z",
     "iopub.status.busy": "2025-01-06T02:58:17.497469Z",
     "iopub.status.idle": "2025-01-06T02:58:18.130862Z",
     "shell.execute_reply": "2025-01-06T02:58:18.130014Z",
     "shell.execute_reply.started": "2025-01-06T02:58:17.497645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Resumo estatistico \n",
    "\n",
    "#separando quali's para analise \n",
    "quali = df[['HasCrCard', \n",
    "            'IsActiveMember', \n",
    "            'Geography',\n",
    "            #'Gender',\n",
    "            'Complain',\n",
    "            'Exited',\n",
    "            'Card Type',\n",
    "            'NumOfProducts',\n",
    "            'Satisfaction Score']]\n",
    "\n",
    "quali = quali.astype('object')\n",
    "\n",
    "#quali.dtypes\n",
    "\n",
    "\n",
    "\n",
    "def add_value_labels(ax):\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        color = p.get_facecolor()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height / 2.,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='center', fontsize=20, color='white', fontweight='bold',\n",
    "                bbox=dict(facecolor=color, edgecolor='none', alpha=0.7,\n",
    "                          boxstyle='round,pad=0.4', linewidth=1))\n",
    "\n",
    "plt.figure(figsize=(20, 25))\n",
    "\n",
    "# Geography\n",
    "plt.subplot(5, 2, 1)\n",
    "ax1 = plt.gca()\n",
    "ax1.set_title('Geography', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Geography', hue='Geography', palette='viridis', data=base_original, ax=ax1, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax1)\n",
    "\n",
    "\n",
    "# Complain\n",
    "plt.subplot(5, 2, 2)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('Complain', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Complain', hue='Complain', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "# HasCrCard\n",
    "plt.subplot(5, 2, 3)\n",
    "ax5 = plt.gca()\n",
    "ax5.set_title('HasCrCard', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='HasCrCard', hue='HasCrCard', palette='viridis', data=base_original, ax=ax5, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax5)\n",
    "\n",
    "# IsActiveMember\n",
    "plt.subplot(5, 2, 4)\n",
    "ax6 = plt.gca()\n",
    "ax6.set_title('IsActiveMember', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='IsActiveMember', hue='IsActiveMember', palette='viridis', data=base_original, ax=ax6, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax6)\n",
    "\n",
    "# Card Type\n",
    "plt.subplot(5, 2, 5)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('Card Type', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Card Type', hue='Card Type', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "\n",
    "# NumOfProducts\n",
    "plt.subplot(5, 2, 6)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('NumOfProducts', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='NumOfProducts', hue='NumOfProducts', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "\n",
    "# Satisfaction Score\n",
    "plt.subplot(5, 2, 7)\n",
    "ax11 = plt.gca()\n",
    "ax11.set_title('Satisfaction Score', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Satisfaction Score', hue='Satisfaction Score', palette='viridis', data=base_original, ax=ax11, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax11)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exited\n",
    "plt.subplot(5, 2, 8)\n",
    "ax11 = plt.gca()\n",
    "ax11.set_title('Exited: Churn ', fontsize=22, fontweight='bold')\n",
    "custom_palette = ['green', 'red']\n",
    "sns.countplot(x='Exited', hue='Exited', palette=custom_palette, data=base_original, ax=ax11, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax11)\n",
    "\n",
    "ax11.set_xticks([0, 1])\n",
    "ax11.set_xticklabels(['N√£o', 'Sim'], fontsize=15, fontweight='bold')\n",
    "\n",
    "# Ajustando espa√ßamento\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualizando o comportmento da variavel alvo (exited) em relacao as demais variaveis; \n",
    "\n",
    "* Vemos claramente que existe o disbalance de classes na variavel churn, pela cor verde presente fortemente em todas variaveis, posteriormente isso sera tratado/mitigado; \n",
    "\n",
    "* Ja e possivel notar um forte indicio de alta correlacao entre churn e complain, posteriormente isso sera testado. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Vari√°vel alvo em rela√ß√£o as demais vari√°veis \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 25)) #tamanho do painel grafico\n",
    "\n",
    "#funcao de adicao de legenda no canto superior direito e garante rotulos \n",
    "def add_legend(ax):\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if not handles:\n",
    "        \n",
    "        # Se n√£o houver handles, forca a adicao\n",
    "        handles = [plt.Rectangle((0,0),1,1, color=c) for c in ['green', 'red']]\n",
    "        labels = ['Not Exited', 'Exited']\n",
    "        \n",
    "    # Adiciona a legenda fora da √°rea das barras\n",
    "    ax.legend(handles, labels, loc='upper left', fontsize=14, title='Exited', title_fontsize='13',  \n",
    "              bbox_to_anchor=(1.0, 1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Geography\n",
    "plt.subplot(5, 2, 1)\n",
    "counts = base_original.groupby(['Geography', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Geography', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Geography', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "\n",
    "# NumOfProducts\n",
    "plt.subplot(5, 2, 2)\n",
    "counts = base_original.groupby(['NumOfProducts', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by NumOfProducts', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('NumOfProducts', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# HasCrCard\n",
    "plt.subplot(5, 2, 3)\n",
    "counts = base_original.groupby(['HasCrCard', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by HasCrCard', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('HasCrCard', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# IsActiveMember\n",
    "plt.subplot(5, 2, 4)\n",
    "counts = base_original.groupby(['IsActiveMember', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca()) \n",
    "plt.title('Exited by IsActiveMember', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('IsActiveMember', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Complain\n",
    "plt.subplot(5, 2, 5)\n",
    "counts = base_original.groupby(['Complain', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Complain', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Complain', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Satisfaction Score\n",
    "plt.subplot(5, 2, 6)\n",
    "counts = base_original.groupby(['Satisfaction Score', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Satisfaction Score', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Satisfaction Score', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Card Type\n",
    "plt.subplot(5, 2, 7)\n",
    "counts = base_original.groupby(['Card Type', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Card Type', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Card Type', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Ajusta a dist√¢ncia entre os gr√°ficos\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variaveis Dummies\n",
    "\n",
    "* A maioria dos modelos necessita de transformar as variaveis categoricas em numericas, e o modelo atual e um deles; \n",
    "\n",
    "* A transformacao de categoricas em numericas precisa ser feita com processos adequados para nao cometer ponderacao arbitr√°ria no desenvolvimento. \n",
    "\n",
    "* foi aplicado one-hot encoding para isso. Esse processo e chamado de \"Dummizacao\". \n",
    "\n",
    "* foi necessario aplicar Ordinal-Encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumizando\n",
    "\n",
    "# Suprime todos os warnings de futuro (deixa mais clean)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Lista de vari√°veis a serem transformadas\n",
    "cols_to_transform = ['Geography']\n",
    "\n",
    "# Convertendo para string (somente a coluna \"Geography\")\n",
    "df.loc[:, cols_to_transform] = df.loc[:, cols_to_transform].astype(str)\n",
    "\n",
    "# Realizando o One-Hot Encoding \n",
    "df_dummies = pd.get_dummies(df, columns=cols_to_transform, dtype=int, drop_first=True) \n",
    "#se o modelo/modelos forem afetados drasticamntee por multicolineariadde drop_first=True deve ser melhor, pois dropa uma das variaveis dummie. Os demais nao feta tanto, mas e boa pratica\n",
    "\n",
    "# Ordinal-Encoder \n",
    "df_dummies['Card Type'] = OrdinalEncoder(categories=[[\"SILVER\", \"GOLD\", \"PLATINUM\", \"DIAMOND\"]], dtype=int).fit_transform(df_dummies[['Card Type']])\n",
    "\n",
    "# Vari√°vel alvo 'Exited' para o tipo num√©rico (se necess√°rio)\n",
    "df_dummies['Exited'] = df_dummies['Exited'].astype('int64')\n",
    "\n",
    "print(df_dummies.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separa√ß√£o Treino e Teste & Adicao de Features quadr√°ticas\n",
    "\n",
    "* A separacao em treino e teste alem de uma boa pratica e extreamente necessario na construcao de modelos de machine learning; \n",
    "\n",
    "* Tambem foram adicionadas variaveis quadraticas, ou seja, com operacao matematica aplicadas em variaveis originais gerando novas variaveis. Isso foi feito para capturar algum tipo de comportamento nao linear; \n",
    "\n",
    "* Foi considerado aplicar transformacao polinomial nas variaveis, por isso foi primeiro aplicado o termo quadratico, que nao apresentou melhoria significativa a ponto de aplicarmos polinomias; \n",
    "\n",
    "* Alem disso esse modelo captura naturalmente comportamentos nao lineares. O termo quadratico foi util para validacao durante o estudo mas a melhoria foi baixa, por isso mantemos apenas os termos quadraticos sem incluir interacoes entre variaveis (seria aplicacao Polinomias completo); \n",
    "\n",
    "\n",
    "\n",
    "* Tambem e possivel notar o desbalanceamento das classes  nas bases tanto em treino quanto em teste no grafico final; \n",
    "\n",
    "* Tambem garantimos a mesma proporcao (80/20) tanto em treino quanto em teste na separacao das bases , ou seja, equidade de divisao de dados e equilibrio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X ---> Vari√°veis explicativas \n",
    "#Y ---> Evento de estudo (vari√°vel TARGET, evento de estudo, ^y etc..)\n",
    "\n",
    "df_dummies = df_dummies.drop(columns=['Complain']) #correlacao altissima com a variavel alvo\n",
    "\n",
    "\n",
    "X = df_dummies .drop('Exited', axis=1)\n",
    "\n",
    "# VARIAVEIS QUADRATICAS \n",
    "X['Balance_Squared'] = X['Balance'] ** 2\n",
    "X['Age_Squared'] = X['Age'] ** 2\n",
    "X['CreditScore_Squared'] = X['CreditScore'] ** 2\n",
    "X['Tenure_Squared'] = X['Tenure'] ** 2\n",
    "X['EstimatedSalary_Squared'] = X['EstimatedSalary'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "y =  df_dummies['Exited']\n",
    "\n",
    "\n",
    "#separando em treino e teste \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Visualizando a propor√ß√£o de eventos de churn (TARGET) nas bases de TREINO e TESTE \n",
    "\n",
    "# Contando os valores \n",
    "churn_counts_train = y_train.value_counts()\n",
    "churn_counts_test = y_test.value_counts()\n",
    "\n",
    "\n",
    "# plot que contem os graficos\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle('Propor√ß√£o da Vari√°vel Churn entre Treino e Teste', fontsize=35)  \n",
    "cmap = plt.get_cmap('viridis', 2) #paleta de cores\n",
    "\n",
    "\n",
    "\n",
    "# Gr√°fico da base de treino\n",
    "bars_train = axs[0].bar(churn_counts_train.index, churn_counts_train.values, color=cmap(range(2)))\n",
    "axs[0].set_title('Base de Treino', fontsize=25)\n",
    "axs[0].set_xlabel('Churn', fontsize=20)\n",
    "axs[0].set_ylabel('Contagem', fontsize=20)\n",
    "axs[0].set_xticks([0, 1])\n",
    "axs[0].set_xticklabels(['0', '1'], fontsize=20)\n",
    "axs[0].set_yticklabels([]) # Ocultando os valores do eixo y\n",
    "\n",
    "# Adicionando r√≥tulos de dados\n",
    "total_train = churn_counts_train.sum()\n",
    "for bar in bars_train:\n",
    "    count = int(bar.get_height())\n",
    "    percentage = round(count / total_train * 100)  # Arredonda a porcentagem\n",
    "    label = f'{count} ({percentage}%)'  #valor absoluto e o percentual\n",
    "    axs[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, \n",
    "                label, ha='center', color='gray', fontsize=25, weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Gr√°fico da base de teste\n",
    "bars_test = axs[1].bar(churn_counts_test.index, churn_counts_test.values, color=cmap(range(2)))\n",
    "axs[1].set_title('Base de Teste', fontsize=25)\n",
    "axs[1].set_xlabel('Churn', fontsize=20)\n",
    "axs[1].set_ylabel('Contagem', fontsize=20)\n",
    "axs[1].set_xticks([0, 1])\n",
    "axs[1].set_xticklabels(['0', '1'], fontsize=20)\n",
    "axs[1].set_yticklabels([])# Ocultando os valores do eixo y\n",
    "\n",
    "# Adicionando r√≥tulos de dados\n",
    "total_test = churn_counts_test.sum()\n",
    "for bar in bars_test:\n",
    "    count = int(bar.get_height())\n",
    "    percentage = round(count / total_test * 100)  # Arredonda a porcentagem\n",
    "    label = f'{count} ({percentage}%)'  #valor absoluto e o percentual\n",
    "    axs[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, \n",
    "                label, ha='center', color='gray', fontsize=25, weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "# Ajusta o layout para evitar sobreposi√ß√£o\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # forca espaco para o titulo\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando correlacoes depois de construcao total de features e Dummizacao \n",
    "\n",
    "* Verificar as correlacoes e extreammente importante, elas podem indicar a famosa multicolinearidade, que atrapalha a maioria dos modelos; \n",
    "\n",
    "* no caso da deste modelo (pelo menos a presente aplicacao) ela nao afetou, a observei com atencao, mas nao impactou. Decidi manter as variveis mesmo com multicolinearidade em algumas (nao se assuste). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observando Multicolinearidade na base de treino\n",
    "\n",
    "teste_multco_treino = pd.concat([X_train,y_train], axis = 1)\n",
    "\n",
    "correlation_matrix_treino = teste_multco_treino.corr().round(2)\n",
    "correlation_matrix_treino\n",
    "\n",
    "# Matrix com  mapa de calor \n",
    "plt.figure(figsize=(30, 20))\n",
    "heatmap = sns.heatmap(correlation_matrix_treino, annot=True, fmt=\".2f\",\n",
    "                      cmap=plt.cm.viridis_r, # paleta de cores viridis (ou viridis_r para o inverso de cores) √© uma paleta especial \n",
    "                                             # para facilitar a visualizacao por pessoas com dificuldades visuais, como os daltonicos. \n",
    "                      annot_kws={'size': 15}, vmin=-1, vmax=1)\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=17)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=17)\n",
    "plt.title('Correla√ß√£o das Vari√°veis Quantitativas na Base de Treino',fontsize=25)\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise e tratamento de Outliers \n",
    "\n",
    "* Outliers sao numericos e podem afetar de diversas formas modelos; \n",
    "\n",
    "* para resolver sem perder dados, pois temos poucas observcoes para estudo, nao foram removidos como facilmente poderia fazer-se, ao inves foi aplicado winsorization; \n",
    "\n",
    "* winsorization e uma tecnica de limitacao dos outliers, ela substiui os valores de outlierns pelos limites superiores e inferiores; \n",
    "\n",
    "* Para isso, e calculado um intervalo de valores aceitos com base no primeiro quartil (Q1) e no terceiro quartil (Q3), valores abaixo do limite inferior ou acima do limite superior sao ajustados para os respectivos limites, corrigindo assim os outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% analise de outliers das variaveis na base de treino \n",
    "\n",
    "\n",
    "###############antes de tratamento############################# \n",
    "variaveis = [\n",
    "    'CreditScore',\n",
    "    'Age',\n",
    "    'Tenure',\n",
    "    'Balance',\n",
    "    'NumOfProducts',\n",
    "    'EstimatedSalary',\n",
    "    'Satisfaction Score',\n",
    "    'Point Earned',\n",
    "    #QUADRATICAS\n",
    "    'Balance_Squared',\n",
    "    'Age_Squared',\n",
    "    'CreditScore_Squared',\n",
    "    'Tenure_Squared',\n",
    "    'EstimatedSalary_Squared'\n",
    "]\n",
    "\n",
    "\n",
    "# definindo tamnhos de subplots \n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# loop de criacao de boxplots para cada variavel \n",
    "for i, var in enumerate(variaveis):\n",
    "    plt.subplot(5, 4, i + 1)  #determina a grade de plots \n",
    "    sns.boxplot(y=teste_multco_treino[var],\n",
    "               boxprops=dict(facecolor='lightblue'))  # Cor interna do boxplot \n",
    "    plt.title(f'Boxplot {var}', fontsize=12)\n",
    "    \n",
    "#  t√≠tulo geral\n",
    "plt.suptitle('An√°lise de Outliers nas Vari√°veis(treino) - antes de \"winsorization\" ', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95]) # Ajuste de layout\n",
    "plt.show()\n",
    "###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fun√ß√£o que aplica winsorization\n",
    "def tratar_outliers(df, coluna):\n",
    "    Q1 = df[coluna].quantile(0.25)\n",
    "    Q3 = df[coluna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    # Substitui outliers pelo limite inferior ou superior\n",
    "    df[coluna] = np.where(df[coluna] < limite_inferior, limite_inferior, df[coluna])\n",
    "    df[coluna] = np.where(df[coluna] > limite_superior, limite_superior, df[coluna])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############depois de tratamento############################# \n",
    "\n",
    "# Aplicando a fun√ß√£o nas vari√°veis \n",
    "variaveis_para_tratar = ['CreditScore',\n",
    "                        'Age',\n",
    "                        'Tenure',\n",
    "                        'Balance',\n",
    "                        'NumOfProducts',\n",
    "                        'EstimatedSalary',\n",
    "                        'Satisfaction Score',\n",
    "                        'Point Earned',\n",
    "                        #QUADRATICAS\n",
    "                        'Balance_Squared',\n",
    "                        'Age_Squared',\n",
    "                        'CreditScore_Squared',\n",
    "                        'Tenure_Squared',\n",
    "                        'EstimatedSalary_Squared'\n",
    "                         ]\n",
    "for variavel in variaveis_para_tratar:\n",
    "    tratar_outliers(teste_multco_treino, variavel)\n",
    "\n",
    "\n",
    "# subplot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "\n",
    "\n",
    "# loop de criacao de boxplots para cada variavel \n",
    "for i, var in enumerate(variaveis):\n",
    "    plt.subplot(5, 4, i + 1)  #determina a grade de plots \n",
    "    sns.boxplot(y=teste_multco_treino[var],\n",
    "               boxprops=dict(facecolor='green'))  # Cor interna do boxplot\n",
    "    plt.title(f'Boxplot {var}', fontsize=12)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "# t√≠tulo geral\n",
    "plt.suptitle('An√°lise de Outliers nas Vari√°veis(treino) - depois de \"winsorization\" ', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # ajusta layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE DE TREINO \n",
    "teste_multco_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE DE TESTE\n",
    "base_corrige_teste = pd.concat([X_test,y_test], axis = 1)\n",
    "base_corrige_teste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparacao dos dados  \n",
    "\n",
    "\n",
    "* Preparacao dos dados \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------')\n",
    "print(\" DATA Prep\")\n",
    "print('------------------------')\n",
    "\n",
    "# Desativando os warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='optuna')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='optuna')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "\n",
    "# in√≠cio\n",
    "start_time_utc = datetime.utcnow() - timedelta(hours=3)\n",
    "print('------------------------')\n",
    "print(\"In√≠cio:\", start_time_utc)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "######################################## PRE-PROCESSAMENTO E PREPARACAO NOS DADOS ################################################################### \n",
    "\n",
    "\n",
    "\n",
    "# Defini vari√°veis de treinamento\n",
    "X_train = teste_multco_treino.drop('Exited', axis=1)\n",
    "y_train = teste_multco_treino['Exited']\n",
    "\n",
    "# Verifica e conserta desalinhamento de √≠ndices caso tenha (X_test e y_test)\n",
    "if not X_test.index.equals(y_test.index):\n",
    "    print(\"√çndices de X_test e y_test n√£o estavam alinhados. Realinhando y_test.\")\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "else:\n",
    "    print(\"√çndices de X_test e y_test j√° estavam alinhados.\")\n",
    "\n",
    "# Concatena os dados corrigidos para criar a base de teste\n",
    "base_corrige_teste = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Redefini X_test e y_test com √≠ndices corrigidos e verificados\n",
    "X_test = base_corrige_teste.drop('Exited', axis=1)\n",
    "y_test = base_corrige_teste['Exited']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Criar e treinar o modelo Random Forest para selecionar as melhores variaveis \n",
    "#detalhe importante, nao tem predict(), somente o fit() \n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Obter a import√¢ncia das features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Criar DataFrame com os nomes das features e suas import√¢ncias\n",
    "feature_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Filtrar features com import√¢ncia maior que 0.01 (ou outro threshold desejado)\n",
    "selected_features = feature_df[feature_df['Importance'] > 0.01]['Feature'].tolist()\n",
    "\n",
    "# Ordenar as features pela import√¢ncia (do menor para o maior)\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Criar o gr√°fico de barras horizontais\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_df['Feature'], feature_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Import√¢ncia')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Import√¢ncia das Features Selecionadas (Random Forest)')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xticks(np.arange(0, max(feature_df['Importance'])+0.01, 0.01))\n",
    "\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizando preparacao e selecionando variaveis\n",
    "\n",
    "Variaveis selecionadas conforme o modelo classificador de importancia usado anteriormente (randomforest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Shape X_train antes de selecionar as fetuares importantes:\", X_train.shape)\n",
    "print(\"Shape y_train antes de selecionar as fetuares importantes:\", y_train.shape)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "\n",
    "\n",
    "start_time_utc = datetime.utcnow() - timedelta(hours=3)\n",
    "print(\"Tempo de In√≠cio:\", start_time_utc)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "X_train, y_train = X_train_selected , y_train\n",
    "X_test, y_test = X_test_selected , y_test\n",
    "\n",
    "print(\"Shape X_train (selecionadas):\", X_train.shape)\n",
    "print(\"Shape y_train (selecionadas):\", y_train.shape)\n",
    "\n",
    "print(\"Shape X_test (selecionadas):\", X_test.shape)\n",
    "print(\"Shape y_test (selecionadas):\", y_test.shape)\n",
    "\n",
    "\n",
    "#verificando alinhamneto\n",
    "assert X_train.shape[0] == y_train.shape[0], \"Erro: N√∫mero de amostras n√£o coincide\"\n",
    "assert X_test.shape[0] == y_test.shape[0], \"Erro: N√∫mero de amostras n√£o coincide\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble Model - Aplicando modelos salvos anteriormente e conferindo resultados** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catboost\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "modelo_path = r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\CATboost\\catboost_model.pkl\"\n",
    "scaler_path = r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\CATboost\\scaler.pkl\"\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "with open(modelo_path, 'rb') as file:\n",
    "    model_catboost = pickle.load(file)\n",
    "\n",
    "# Carregar o scaler\n",
    "with open(scaler_path, 'rb') as file:\n",
    "    scaler_catboost = pickle.load(file)\n",
    "\n",
    "# Normalizar os dados de teste\n",
    "X_test_scaled = scaler_catboost.transform(X_test_selected)\n",
    "\n",
    "# Fazer previs√µes\n",
    "y_pred = model_catboost.predict(X_test_scaled)\n",
    "y_pred_proba = model_catboost.predict_proba(X_test_scaled)[:, 1]  # Probabilidade da classe positiva\n",
    "\n",
    "# Converter para DataFrame para an√°lise\n",
    "#df_pred = pd.DataFrame({\n",
    "#    \"y_real\": y_test.values,  # Se `y_test` for DataFrame, pegar `.values`\n",
    "#    \"y_pred\": y_pred,\n",
    "#    \"y_pred_proba\": y_pred_proba\n",
    "#})\n",
    "\n",
    "# Exibir as primeiras linhas das previs√µes\n",
    "#print(df_pred.head())\n",
    "\n",
    "\n",
    "# Calcular m√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Exibir as m√©tricas no formato desejado\n",
    "print(\"\\nüìä M√âTRICAS DO MODELO CATboost nos dados de teste\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'M√©trica':<20}{'Valor final':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Acur√°cia{' ' * 11}{accuracy:.4f}\")\n",
    "print(f\"Precis√£o{' ' * 12}{precision:.4f}\")\n",
    "print(f\"Recall{' ' * 14}{recall:.4f}\")\n",
    "print(f\"F1-Score{' ' * 12}{f1:.4f}\")\n",
    "print(f\"AUC-ROC{' ' * 13}{auc_roc:.4f}\")\n",
    "print(f\"MCC{' ' * 17}{mcc:.4f}\")\n",
    "print(f\"Kappa de Cohen{' ' * 7}{kappa:.4f}\")\n",
    "print(f\"Acur√°cia Balanceada {balanced_acc:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criando a figura e os subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot da Matriz de Confus√£o\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negativo\", \"Positivo\"], \n",
    "            yticklabels=[\"Negativo\", \"Positivo\"], ax=axes[0])\n",
    "axes[0].set_xlabel(\"Predito\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confus√£o (TESTE) - CATboost\")\n",
    "\n",
    "# Plot da Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_roc_value = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_roc_value:.4f})\", color=\"blue\")\n",
    "axes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Linha diagonal\n",
    "axes[1].set_xlabel(\"Taxa de Falsos Positivos (FPR)\")\n",
    "axes[1].set_ylabel(\"Taxa de Verdadeiros Positivos (TPR)\")\n",
    "axes[1].set_title(\"Curva ROC (TESTE)- CATboost\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Distribui√ß√£o de probabilidades\n",
    "\n",
    "# classe positiva\n",
    "probs_pos = y_pred_proba  \n",
    "# classe negativa\n",
    "probs_neg = 1 - y_pred_proba\n",
    "\n",
    "# Plot classe positiva \n",
    "sns.kdeplot(probs_pos, color='blue', ax=axes[2], label='Classe Positiva', fill=True, alpha=0.6)\n",
    "\n",
    "# Plot classe negativa \n",
    "sns.kdeplot(probs_neg, color='red', ax=axes[2], label='Classe Negativa', fill=True, alpha=0.02, linewidth=0.30)\n",
    "\n",
    "# Ajustando o gr√°fico\n",
    "axes[2].set_title(\"Distribui√ß√£o das Probabilidades para as Classes Positiva e Negativa (TESTE) - CATboost\")\n",
    "axes[2].set_xlabel(\"Probabilidade\")\n",
    "axes[2].set_ylabel(\"Densidade\")\n",
    "axes[2].legend()\n",
    "# setando eixo de probabilidades entre 0 e 1\n",
    "axes[2].set_xlim(0, 1)\n",
    "\n",
    "\n",
    "# Ajuste de layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - PyTorch\n",
    "\n",
    "# Classe MLP do modelo (reconstru√ß√£o exata)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_sizes, activation, dropout_rate=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for units in hidden_layer_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, units))\n",
    "            self.layers.append(nn.Dropout(p=dropout_rate))\n",
    "            input_size = units\n",
    "        self.output = nn.Linear(input_size, 1)\n",
    "        self.activation_fn = self.get_activation_function(activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation_fn(layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        activation_dict = {\n",
    "            'relu': torch.relu,\n",
    "            'tanh': torch.tanh,\n",
    "            'sigmoid': torch.sigmoid,\n",
    "            'selu': torch.selu,\n",
    "            'gelu': torch.nn.functional.gelu,\n",
    "            'leaky_relu': torch.nn.functional.leaky_relu,\n",
    "            'swish': torch.nn.functional.silu,\n",
    "            'elu': torch.nn.functional.elu\n",
    "        }\n",
    "        return activation_dict.get(activation, torch.relu)\n",
    "\n",
    "# Definir o dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carregar o scaler\n",
    "scaler_mlp_torch = torch.load(r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\MLP-Pytorch\\scaler.pth\")\n",
    "\n",
    "#salvando para uso dentro desse notebook, uma copia com formato de joblib / pickle\n",
    "joblib.dump(scaler_mlp_torch, \"scaler_mlp_temp.pkl\")\n",
    "\n",
    "#definindo scaler definitivamente\n",
    "scaler_mlp_torch = joblib.load(\"scaler_mlp_temp.pkl\")\n",
    "\n",
    "\n",
    "#apagando scaler_mlp_temp para nao confundir com scaler scaler oficial do ensemble\n",
    "\n",
    "arquivo = \"scaler_mlp_temp.pkl\"\n",
    "if os.path.exists(arquivo):\n",
    "    os.remove(arquivo)\n",
    "    print(f\"Arquivo {arquivo} removido com sucesso.\")\n",
    "else:\n",
    "    print(f\"Arquivo {arquivo} n√£o encontrado.\")\n",
    "\n",
    "\n",
    "\n",
    "# Normalizar os dados de teste\n",
    "X_test_scaled = scaler_mlp_torch.transform(X_test)\n",
    "\n",
    "# Converter X_test para tensor do PyTorch\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Carregar os hiperpar√¢metros do modelo (ajuste isso se souber os valores exatos)\n",
    "hidden_layer_sizes = [950, 850]  # com base nos melhores par√¢metros encontrados no modelo carregado e treinado anteriormente \n",
    "activation = \"relu\"  # com base nos melhores par√¢metros encontrados no modelo carregado e treinado anteriormente \n",
    "dropout_rate = 0.35657230019086544  # com base nos melhores par√¢metros encontrados no modelo carregado e treinado anteriormente \n",
    "\n",
    "# Criar a inst√¢ncia do modelo com os hiperpar√¢metros corretos\n",
    "input_size = X_test.shape[1]\n",
    "model_mlp_torch = MLP(input_size, hidden_layer_sizes, activation, dropout_rate)\n",
    "\n",
    "# Carregar o modelo salvo\n",
    "model_mlp_torch = torch.load(r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\MLP-Pytorch\\best_model_inteiro.pth\", map_location=device)\n",
    "model_mlp_torch.to(device)\n",
    "model_mlp_torch.eval()\n",
    "\n",
    "# Fazer previs√µes\n",
    "with torch.no_grad():\n",
    "    # Aplicar a fun√ß√£o sigmoide para garantir que as probabilidades estejam no intervalo [0, 1]\n",
    "    y_pred_proba = torch.sigmoid(model_mlp_torch(X_test_tensor)).cpu().numpy().flatten()\n",
    "\n",
    "# Converter probabilidades para r√≥tulos bin√°rios (0 ou 1)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Calcular m√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Exibir as m√©tricas no formato desejado\n",
    "print(\"\\nüìä M√âTRICAS DO MODELO MLP nos dados de teste\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'M√©trica':<20}{'Valor final':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Acur√°cia{' ' * 11}{accuracy:.4f}\")\n",
    "print(f\"Precis√£o{' ' * 12}{precision:.4f}\")\n",
    "print(f\"Recall{' ' * 14}{recall:.4f}\")\n",
    "print(f\"F1-Score{' ' * 12}{f1:.4f}\")\n",
    "print(f\"AUC-ROC{' ' * 13}{auc_roc:.4f}\")\n",
    "print(f\"MCC{' ' * 17}{mcc:.4f}\")\n",
    "print(f\"Kappa de Cohen{' ' * 7}{kappa:.4f}\")\n",
    "print(f\"Acur√°cia Balanceada {balanced_acc:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criando a figura e os subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot da Matriz de Confus√£o\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negativo\", \"Positivo\"], \n",
    "            yticklabels=[\"Negativo\", \"Positivo\"], ax=axes[0])\n",
    "axes[0].set_xlabel(\"Predito\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confus√£o (TESTE) - MLP\")\n",
    "\n",
    "# Plot da Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_roc_value = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_roc_value:.4f})\", color=\"blue\")\n",
    "axes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Linha diagonal\n",
    "axes[1].set_xlabel(\"Taxa de Falsos Positivos (FPR)\")\n",
    "axes[1].set_ylabel(\"Taxa de Verdadeiros Positivos (TPR)\")\n",
    "axes[1].set_title(\"Curva ROC (TESTE) - MLP\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Distribui√ß√£o de probabilidades\n",
    "\n",
    "# classe positiva\n",
    "probs_pos = y_pred_proba  \n",
    "# classe negativa\n",
    "probs_neg = 1 - y_pred_proba\n",
    "\n",
    "# Plot classe positiva \n",
    "sns.kdeplot(probs_pos, color='blue', ax=axes[2], label='Classe Positiva', fill=True, alpha=0.6)\n",
    "\n",
    "# Plot classe negativa \n",
    "sns.kdeplot(probs_neg, color='red', ax=axes[2], label='Classe Negativa', fill=True, alpha=0.02, linewidth=0.30)\n",
    "\n",
    "# Ajustando o gr√°fico\n",
    "axes[2].set_title(\"Distribui√ß√£o das Probabilidades para as Classes Positiva e Negativa (TESTE) - MLP\")\n",
    "axes[2].set_xlabel(\"Probabilidade\")\n",
    "axes[2].set_ylabel(\"Densidade\")\n",
    "axes[2].legend()\n",
    "# setando eixo de probabilidades entre 0 e 1\n",
    "axes[2].set_xlim(0, 1)\n",
    "\n",
    "# Ajuste de layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost \n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "model_path = r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\Xgboost\\xgb_model.pkl\"\n",
    "scaler_path = r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\Xgboost\\scaler.pkl\"\n",
    "\n",
    "# Carregar o scaler\n",
    "scaler_xgb = joblib.load(scaler_path)\n",
    "\n",
    "# Normalizar os dados de teste\n",
    "X_test_scaled = scaler_xgb.transform(X_test)\n",
    "\n",
    "# Carregar o modelo XGBoost\n",
    "xgb_model = joblib.load(model_path)\n",
    "\n",
    "# Fazer previs√µes (probabilidades)\n",
    "y_pred_prob = xgb_model.predict(xgb.DMatrix(X_test_scaled))\n",
    "\n",
    "# Converter para classe bin√°ria (usando threshold 0.5)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Exibir m√©tricas no formato esperado\n",
    "print(\"\\nüìä M√âTRICAS DO MODELO XGBOOST nos dados de teste\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'M√©trica':<22} {'Valor final':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Acur√°cia':<22} {accuracy:.4f}\")\n",
    "print(f\"{'Precis√£o':<22} {precision:.4f}\")\n",
    "print(f\"{'Recall':<22} {recall:.4f}\")\n",
    "print(f\"{'F1-Score':<22} {f1:.4f}\")\n",
    "print(f\"{'AUC-ROC':<22} {roc_auc:.4f}\")\n",
    "print(f\"{'MCC':<22} {mcc:.4f}\")\n",
    "print(f\"{'Kappa de Cohen':<22} {kappa:.4f}\")\n",
    "print(f\"{'Acur√°cia Balanceada':<22} {balanced_acc:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criando a figura e os subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot da Matriz de Confus√£o\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negativo\", \"Positivo\"], \n",
    "            yticklabels=[\"Negativo\", \"Positivo\"], ax=axes[0])\n",
    "axes[0].set_xlabel(\"Predito\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "axes[0].set_title(\"Matriz de Confus√£o (TESTE) - XGBoost\")\n",
    "\n",
    "# Plot da Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc_value = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc_value:.4f})\", color=\"blue\")\n",
    "axes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Linha diagonal\n",
    "axes[1].set_xlabel(\"Taxa de Falsos Positivos (FPR)\")\n",
    "axes[1].set_ylabel(\"Taxa de Verdadeiros Positivos (TPR)\")\n",
    "axes[1].set_title(\"Curva ROC (TESTE) - XGBoost\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Distribui√ß√£o de probabilidades\n",
    "\n",
    "# classe positiva\n",
    "probs_pos = y_pred_prob  \n",
    "# classe negativa\n",
    "probs_neg = 1 - y_pred_prob\n",
    "\n",
    "# Plot classe positiva \n",
    "sns.kdeplot(probs_pos, color='blue', ax=axes[2], label='Classe Positiva', fill=True, alpha=0.6)\n",
    "\n",
    "# Plot classe negativa \n",
    "sns.kdeplot(probs_neg, color='red', ax=axes[2], label='Classe Negativa', fill=True, alpha=0.02, linewidth=0.30)\n",
    "\n",
    "# Ajustando o gr√°fico\n",
    "axes[2].set_title(\"Distribui√ß√£o das Probabilidades para as Classes Positiva e Negativa (TESTE) - XGBoost\")\n",
    "axes[2].set_xlabel(\"Probabilidade\")\n",
    "axes[2].set_ylabel(\"Densidade\")\n",
    "axes[2].legend()\n",
    "# setando eixo de probabilidades entre 0 e 1\n",
    "axes[2].set_xlim(0, 1)\n",
    "\n",
    "\n",
    "# Ajuste de layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score, confusion_matrix, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Normalizar os dados de teste e treino com os scalers corretos\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# XGBoost - Escalonamento com o scaler espec√≠fico para XGBoost\n",
    "X_train_scaled_xgb = scaler_xgb.transform(X_train)\n",
    "X_test_scaled_xgb = scaler_xgb.transform(X_test)\n",
    "\n",
    "# MLP (Torch) - Escalonamento com o scaler espec√≠fico para MLP\n",
    "X_train_scaled_mlp = scaler_mlp_torch.transform(X_train)\n",
    "X_test_scaled_mlp = scaler_mlp_torch.transform(X_test)\n",
    "\n",
    "# CatBoost - Escalonamento com o scaler espec√≠fico para CatBoost\n",
    "X_train_scaled_catboost = scaler_catboost.transform(X_train)\n",
    "X_test_scaled_catboost = scaler_catboost.transform(X_test)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. XGBoost\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Conjunto de treino\n",
    "# Previs√µes no conjunto de treino\n",
    "y_pred_prob_xgb_ensemble_train = xgb_model.predict(xgb.DMatrix(X_train_scaled_xgb))\n",
    "\n",
    "# Conjunto de teste\n",
    "# Fazer previs√µes (probabilidades)\n",
    "y_pred_prob_xgb_ensemble_test = xgb_model.predict(xgb.DMatrix(X_test_scaled_xgb))\n",
    "\n",
    "# Converter para classe bin√°ria (usando threshold 0.5)\n",
    "xgb_y_pred_xgb_ensemble_test_ = (y_pred_prob_xgb_ensemble_test >= 0.5).astype(int)\n",
    "\n",
    "# Calcular as m√©tricas\n",
    "xgb_ensemble_accuracy = accuracy_score(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "xgb_ensemble_precision = precision_score(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "xgb_ensemble_recall = recall_score(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "xgb_ensemble_f1 = f1_score(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "xgb_ensemble_auc_roc = roc_auc_score(y_test, y_pred_prob_xgb_ensemble_test)  # Corre√ß√£o aqui\n",
    "xgb_ensemble_mcc = matthews_corrcoef(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "xgb_ensemble_kappa = cohen_kappa_score(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "xgb_ensemble_balanced_acc = balanced_accuracy_score(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. MLP (Torch)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train_scaled = scaler_mlp_torch.transform(X_train)\n",
    "X_test_scaled = scaler_mlp_torch.transform(X_test)\n",
    "\n",
    "# Converter para tensor do PyTorch\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# Criar a inst√¢ncia do modelo com os hiperpar√¢metros corretos\n",
    "input_size = X_test.shape[1]\n",
    "model_mlp_torch = MLP(input_size, hidden_layer_sizes, activation, dropout_rate)\n",
    "\n",
    "# Carregar o modelo salvo\n",
    "model_mlp_torch = torch.load(r\"C:\\Users\\jgeov\\OneDrive\\Documentos\\GitHub\\Ciencia_de_dados-1\\Churn_predict\\MLP-Pytorch\\best_model_inteiro.pth\", map_location=device)\n",
    "model_mlp_torch.to(device)\n",
    "model_mlp_torch.eval()\n",
    "\n",
    "# Fazer previs√µes para os dados de teste\n",
    "with torch.no_grad():\n",
    "    # Aplicar a fun√ß√£o sigmoide para garantir que as probabilidades est√£o no intervalo [0, 1]\n",
    "    y_pred_proba_mlp_ensemble_test = torch.sigmoid(model_mlp_torch(X_test_tensor)).cpu().numpy().flatten()\n",
    "\n",
    "# Converter probabilidades para r√≥tulos bin√°rios (0 ou 1) no conjunto de teste\n",
    "y_pred_mlp_ensemble_test_ = (y_pred_proba_mlp_ensemble_test >= 0.5).astype(int)\n",
    "\n",
    "# Fazer previs√µes para os dados de treino\n",
    "with torch.no_grad():\n",
    "    # Aplicar a fun√ß√£o sigmoide para garantir que as probabilidades est√£o no intervalo [0, 1]\n",
    "    y_pred_proba_mlp_ensemble_train = torch.sigmoid(model_mlp_torch(X_train_tensor)).cpu().numpy().flatten()\n",
    "\n",
    "# Converter probabilidades para r√≥tulos bin√°rios (0 ou 1) no conjunto de treino\n",
    "y_pred_mlp_ensemble_train_ = (y_pred_proba_mlp_ensemble_train >= 0.5).astype(int)\n",
    "\n",
    "# Agora, temos as probabilidades e as classes para treino e teste\n",
    "\n",
    "# Calcular as m√©tricas\n",
    "mlp_ensemble_accuracy = accuracy_score(y_test, y_pred_mlp_ensemble_test_)\n",
    "mlp_ensemble_precision = precision_score(y_test, y_pred_mlp_ensemble_test_)\n",
    "mlp_ensemble_recall = recall_score(y_test, y_pred_mlp_ensemble_test_)\n",
    "mlp_ensemble_f1 = f1_score(y_test, y_pred_mlp_ensemble_test_)\n",
    "mlp_ensemble_auc_roc = roc_auc_score(y_test, y_pred_proba_mlp_ensemble_test)  # AUC-ROC agora usando as probabilidades (n√£o as classes)\n",
    "mlp_ensemble_mcc = matthews_corrcoef(y_test, y_pred_mlp_ensemble_test_)\n",
    "mlp_ensemble_kappa = cohen_kappa_score(y_test, y_pred_mlp_ensemble_test_)\n",
    "mlp_ensemble_balanced_acc = balanced_accuracy_score(y_test, y_pred_mlp_ensemble_test_)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. CatBoost\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Previs√µes e m√©tricas treino \n",
    "# Previs√µes no conjunto de treino\n",
    "catboost_y_pred_proba_ensemble_train = model_catboost.predict_proba(X_train_scaled_catboost)[:, 1]\n",
    "catboost_y_pred_ensemble_train_ = (catboost_y_pred_proba_ensemble_train >= 0.5).astype(int)\n",
    "\n",
    "# Previs√µes e m√©tricas teste \n",
    "catboost_y_pred_proba_ensemble_teste = model_catboost.predict_proba(X_test_scaled_catboost)[:, 1]\n",
    "catboost_y_pred_ensemble_teste_ = (catboost_y_pred_proba_ensemble_teste >= 0.5).astype(int)\n",
    "\n",
    "# Calcular as m√©tricas\n",
    "catboost_ensemble_accuracy = accuracy_score(y_test, catboost_y_pred_ensemble_teste_)\n",
    "catboost_ensemble_precision = precision_score(y_test, catboost_y_pred_ensemble_teste_)\n",
    "catboost_ensemble_recall = recall_score(y_test, catboost_y_pred_ensemble_teste_)\n",
    "catboost_ensemble_f1 = f1_score(y_test, catboost_y_pred_ensemble_teste_)\n",
    "catboost_ensemble_auc_roc = roc_auc_score(y_test, catboost_y_pred_proba_ensemble_teste)  # Corre√ß√£o aqui\n",
    "catboost_ensemble_mcc = matthews_corrcoef(y_test, catboost_y_pred_ensemble_teste_)\n",
    "catboost_ensemble_kappa = cohen_kappa_score(y_test, catboost_y_pred_ensemble_teste_)\n",
    "catboost_ensemble_balanced_acc = balanced_accuracy_score(y_test, catboost_y_pred_ensemble_teste_)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Exibir as m√©tricas para os tr√™s modelos\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Dados das m√©tricas de cada modelo\n",
    "data = {\n",
    "    'M√©trica': ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC', 'MCC', 'Kappa de Cohen', 'Acur√°cia Balanceada'],\n",
    "    'XGBoost': [xgb_ensemble_accuracy, xgb_ensemble_precision, xgb_ensemble_recall, xgb_ensemble_f1, xgb_ensemble_auc_roc, \n",
    "                xgb_ensemble_mcc, xgb_ensemble_kappa, xgb_ensemble_balanced_acc],\n",
    "    'MLP (Torch)': [mlp_ensemble_accuracy, mlp_ensemble_precision, mlp_ensemble_recall, mlp_ensemble_f1, mlp_ensemble_auc_roc, \n",
    "                    mlp_ensemble_mcc, mlp_ensemble_kappa, mlp_ensemble_balanced_acc],\n",
    "    'CatBoost': [catboost_ensemble_accuracy, catboost_ensemble_precision, catboost_ensemble_recall, catboost_ensemble_f1, catboost_ensemble_auc_roc, \n",
    "                 catboost_ensemble_mcc, catboost_ensemble_kappa, catboost_ensemble_balanced_acc]\n",
    "}\n",
    "\n",
    "# Criando o DataFrame\n",
    "metrics_df = pd.DataFrame(data)\n",
    "\n",
    "# Plotando a tabela com as m√©tricas\n",
    "fig, ax = plt.subplots(figsize=(10, 3))  # Aumentei a largura para caber melhor os dados\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table = ax.table(cellText=metrics_df.values, colLabels=metrics_df.columns, cellLoc='center', loc='center', colColours=[\"lightblue\"]*4)\n",
    "\n",
    "# Ajustando os par√¢metros da tabela\n",
    "table.auto_set_font_size(False)  # N√£o usar o tamanho autom√°tico da fonte\n",
    "table.set_fontsize(10)  # Definindo o tamanho da fonte para ser menor e caber melhor\n",
    "table.scale(1.2, 1.2)  # Aumentando o tamanho da tabela para ocupar mais espa√ßo\n",
    "\n",
    "# Ajustando o layout para remover qualquer espa√ßamento extra\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Plotar as Curvas ROC AUC dos tr√™s modelos, incluindo treino\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# ROC Curve - XGBoost (Treinamento)\n",
    "fpr_xgb_train, tpr_xgb_train, _ = roc_curve(y_train, xgb_model.predict(xgb.DMatrix(X_train_scaled_xgb)))\n",
    "plt.plot(fpr_xgb_train, tpr_xgb_train, label=f\"XGBoost Treinamento (AUC = {roc_auc_score(y_train, xgb_model.predict(xgb.DMatrix(X_train_scaled_xgb))):.4f})\", linestyle='--')\n",
    "\n",
    "# ROC Curve - XGBoost (teste)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_prob_xgb_ensemble_test)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f\"XGBoost Teste (AUC = {xgb_ensemble_auc_roc:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "# ROC Curve - MLP (PyTorch)  (Treinamento)\n",
    "fpr_mlp_train, tpr_mlp_train, _ = roc_curve(y_train, model_mlp_torch(torch.tensor(X_train_scaled_mlp, dtype=torch.float32).to(device)).cpu().detach().numpy())\n",
    "plt.plot(fpr_mlp_train, tpr_mlp_train, label=f\"MLP (PyTorch) Treinamento (AUC = {roc_auc_score(y_train, model_mlp_torch(torch.tensor(X_train_scaled_mlp, dtype=torch.float32).to(device)).cpu().detach().numpy()):.4f})\", linestyle='--')\n",
    "\n",
    "# ROC Curve - MLP (PyTorch) (teste)\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_pred_proba_mlp_ensemble_test)\n",
    "plt.plot(fpr_mlp, tpr_mlp, label=f\"MLP (PyTorch) Teste (AUC = {mlp_ensemble_auc_roc:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "# ROC Curve - CatBoost - (Treinamento)\n",
    "fpr_catboost_train, tpr_catboost_train, _ = roc_curve(y_train, model_catboost.predict_proba(X_train_scaled_catboost)[:, 1])\n",
    "plt.plot(fpr_catboost_train, tpr_catboost_train, label=f\"CatBoost Treinamento (AUC = {roc_auc_score(y_train, model_catboost.predict_proba(X_train_scaled_catboost)[:, 1]):.4f})\", linestyle='--')\n",
    "\n",
    "# ROC Curve - CatBoost (teste)\n",
    "fpr_catboost, tpr_catboost, _ = roc_curve(y_test, catboost_y_pred_proba_ensemble_teste)\n",
    "plt.plot(fpr_catboost, tpr_catboost, label=f\"CatBoost Teste (AUC = {catboost_ensemble_auc_roc:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "# Linha de Chance\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Linha diagonal\n",
    "\n",
    "plt.xlabel(\"Taxa de Falsos Positivos\")\n",
    "plt.ylabel(\"Taxa de Verdadeiros Positivos\")\n",
    "plt.title(\"Curvas ROC AUC (Teste e Treinamento)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Plotar as Matrizes de Confus√£o dos tr√™s modelos em Subplots\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Matriz de Confus√£o - XGBoost nos dados de teste\n",
    "xgb_cm = confusion_matrix(y_test, xgb_y_pred_xgb_ensemble_test_)\n",
    "sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0], \n",
    "            xticklabels=['Classe 0', 'Classe 1'], yticklabels=['Classe 0', 'Classe 1'])\n",
    "axes[0].set_title(\"Matriz de Confus√£o - XGBoost  (TESTE)\")\n",
    "axes[0].set_xlabel('Previs√µes')\n",
    "axes[0].set_ylabel('Valores Reais')\n",
    "\n",
    "# Matriz de Confus√£o - MLP nos dados de teste\n",
    "mlp_cm = confusion_matrix(y_test, y_pred_mlp_ensemble_test_)\n",
    "sns.heatmap(mlp_cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[1], \n",
    "            xticklabels=['Classe 0', 'Classe 1'], yticklabels=['Classe 0', 'Classe 1'])\n",
    "axes[1].set_title(\"Matriz de Confus√£o - MLP (PyTorch)  (TESTE)\")\n",
    "axes[1].set_xlabel('Previs√µes')\n",
    "axes[1].set_ylabel('Valores Reais')\n",
    "\n",
    "# Matriz de Confus√£o - CatBoost nos dados de teste \n",
    "catboost_cm = confusion_matrix(y_test, catboost_y_pred_ensemble_teste_)\n",
    "sns.heatmap(catboost_cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[2], \n",
    "            xticklabels=['Classe 0', 'Classe 1'], yticklabels=['Classe 0', 'Classe 1'])\n",
    "axes[2].set_title(\"Matriz de Confus√£o - CatBoost  (TESTE)\")\n",
    "axes[2].set_xlabel('Previs√µes')\n",
    "axes[2].set_ylabel('Valores Reais')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Gerar previs√µes dos 3 modelos (predicoes separadas)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Criar DataFrames de treino e teste para as previs√µes\n",
    "df_predicoes_train = pd.DataFrame({\n",
    "    'XGBoost_Prob': y_pred_prob_xgb_ensemble_train,\n",
    "    'MLP_Torch_Prob': y_pred_proba_mlp_ensemble_train,\n",
    "    'CatBoost_Prob': catboost_y_pred_proba_ensemble_train\n",
    "}, index=X_train.index)  # Mantendo o mesmo √≠ndice do X_train\n",
    "\n",
    "df_predicoes_test = pd.DataFrame({\n",
    "    'XGBoost_Prob': y_pred_prob_xgb_ensemble_test,\n",
    "    'MLP_Torch_Prob': y_pred_proba_mlp_ensemble_test,\n",
    "    'CatBoost_Prob': catboost_y_pred_proba_ensemble_teste\n",
    "}, index=X_test.index)  # Mantendo o mesmo √≠ndice do X_test\n",
    "\n",
    "# Concatenar com os datasets originais\n",
    "X_train_stacking = pd.concat([X_train, df_predicoes_train], axis=1)\n",
    "X_test_stacking = pd.concat([X_test, df_predicoes_test], axis=1)\n",
    "\n",
    "# Exibir as primeiras linhas para conferir se as probs estao vindo certo\n",
    "print(X_train_stacking.head())\n",
    "print(X_test_stacking.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Gerar previs√µes dos 3 modelos (medias das probabilidades)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Criar DataFrames de treino e teste para as previs√µes individuais\n",
    "#df_predicoes_train = pd.DataFrame({\n",
    "#    'XGBoost_Prob': y_pred_prob_xgb_ensemble_train,\n",
    "#    'MLP_Torch_Prob': y_pred_proba_mlp_ensemble_train,\n",
    "#    'CatBoost_Prob': catboost_y_pred_proba_ensemble_train\n",
    "#}, index=X_train.index)  # Mantendo o mesmo √≠ndice do X_train\n",
    "\n",
    "#df_predicoes_test = pd.DataFrame({\n",
    "#    'XGBoost_Prob': y_pred_prob_xgb_ensemble_test,\n",
    "#    'MLP_Torch_Prob': y_pred_proba_mlp_ensemble_test,\n",
    "#    'CatBoost_Prob': catboost_y_pred_proba_ensemble_teste\n",
    "#}, index=X_test.index)  # Mantendo o mesmo √≠ndice do X_test\n",
    "\n",
    "#print(df_predicoes_train.head())\n",
    "#print(df_predicoes_test.head())\n",
    "\n",
    "# Calcular a m√©dia das probabilidades para treino e teste (mantendo apenas a m√©dia)\n",
    "#df_predicoes_train_media = pd.DataFrame({\n",
    "#    'Media_Prob': df_predicoes_train.mean(axis=1)\n",
    "#}, index=X_train.index)  # Mantendo o mesmo √≠ndice do X_train\n",
    "\n",
    "#df_predicoes_test_media = pd.DataFrame({\n",
    "#    'Media_Prob': df_predicoes_test.mean(axis=1)\n",
    "#}, index=X_test.index)  # Mantendo o mesmo √≠ndice do X_test\n",
    "\n",
    "# Concatenar apenas as m√©dias com os datasets originais\n",
    "#X_train_stacking = pd.concat([X_train, df_predicoes_train_media], axis=1)\n",
    "#X_test_stacking = pd.concat([X_test, df_predicoes_test_media], axis=1)\n",
    "\n",
    "\n",
    "# Exibir as primeiras linhas para conferir se as probs estao vindo certo\n",
    "#print('---------------------------------------------------------------------------' ) #so pra separar os prints\n",
    "#print(X_train_stacking.head())\n",
    "#print(X_test_stacking.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#testando modelos para serrem metamodels(aqui so anota mesmo)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Criar o modelo de regress√£o log√≠stica # BOM MAS NEM TANTO\n",
    "#meta_model = LogisticRegression()\n",
    "\n",
    "# Criar o modelo XGBoost # Razoavel \n",
    "#meta_model = XGBClassifier(random_state=42) \n",
    "\n",
    "\n",
    "# Criar o modelo LightGBM# bom mas insatisfatorio \n",
    "#meta_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQUI PRECISA ACHAR UMA FORMA DE APLICAR O SCALE_POS_WEIGHT PARA O mlp kERAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando balanceametno final (morrer de certeza)\n",
    "print(round(y_train.value_counts(normalize=True)*100,1))\n",
    "#continuam naturalmente desbalanceadas \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Calcular o peso ideal para scale_pos_weight quando esta desbalanceado: pode ser pelo calculo de \n",
    "# raz√£o inversa das propor√ß√µes das classes (boa pratica pra dar um norte de onde comecar a explorar o hiperparametro scale_pos_weight):\n",
    "\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "pos_weight\n",
    " \n",
    "# isso significa que a classe 0 (Negativa) √© 3.86 vezes maior que a positiva (1)\n",
    "\n",
    "# o estudo de scale_pos_weight melhor foi necessario por o modelo estava propenso a definir a maioria dos casos como negativo por conta dos baixos valores de probabilidade (muito pelo threshold tbm). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# teste de Shapiro-Wilk nos dados de treinamento \n",
    "* Padroniza√ß√£o vs Normaliza√ß√£o:\n",
    "\n",
    "    * Padroniza√ß√£o (StandardScaler):\n",
    "\n",
    "            Utilizada quando os dados possuem distribui√ß√µes com outliers ou grandes varia√ß√µes.\n",
    "            Transforma√ß√£o para m√©dia 0 e desvio padr√£o 1, preservando a distribui√ß√£o dos dados.\n",
    "\n",
    "    * Normaliza√ß√£o (MinMaxScaler):\n",
    "\n",
    "            Recomend√°vel quando os dados t√™m uma faixa limitada de valores ou distribui√ß√£o assim√©trica.\n",
    "            √ötil para modelos com fun√ß√µes de ativa√ß√£o como sigmoide ou tanh que exigem entradas em um intervalo espec√≠fico.\n",
    "            Transforma os dados para um intervalo, geralmente [0, 1], garantindo que todas as vari√°veis fiquem na mesma escala.\n",
    "\n",
    "    * Conclus√£o do este de SHAPIRO: \n",
    "    \n",
    "            Dados n√£o apresentam normalidade, a normaliza√ß√£o (MinMaxScaler) foi a t√©cnica escolhida. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Definindo o n√∫mero de colunas para os subplots\n",
    "num_cols = 3  # O n√∫mero de colunas de subplots\n",
    "num_vars = len(X_train_stacking.select_dtypes(include=['float32','float64', 'int64']).columns)  # N√∫mero de vari√°veis\n",
    "num_rows = (num_vars // num_cols) + (num_vars % num_cols > 0)  # Calculando o n√∫mero de linhas de subplots\n",
    "\n",
    "# Ajustando o tamanho dos gr√°ficos (largura, altura) com base no n√∫mero de subgr√°ficos\n",
    "fig_width = 5 * num_cols  # Largura proporcional ao n√∫mero de colunas\n",
    "fig_height = 5 * num_rows  # Altura proporcional ao n√∫mero de linhas\n",
    "\n",
    "# Criando os subplots com o tamanho ajustado\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height))\n",
    "axes = axes.flatten()  # Para garantir que podemos indexar de forma unificada\n",
    "\n",
    "# Iterando pelas vari√°veis num√©ricas\n",
    "for i, column in enumerate(X_train_stacking.select_dtypes(include=['float32','float64', 'int64']).columns):\n",
    "    # Teste de Shapiro-Wilk\n",
    "    stat, p = shapiro(X_train_stacking[column])\n",
    "\n",
    "    # Plotando o histograma e KDE\n",
    "    sns.histplot(X_train_stacking[column], kde=True, ax=axes[i], color='skyblue', stat='density')\n",
    "    axes[i].set_title(f\"{column} | p-value = {p:.4f}\")\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel('Densidade')\n",
    "\n",
    "    # Adicionando texto sobre o p-valor\n",
    "    if p > 0.05:\n",
    "        axes[i].text(0.05, 0.95, f\"Normal: p > 0.05\", transform=axes[i].transAxes, fontsize=12, color='green')\n",
    "    else:\n",
    "        axes[i].text(0.05, 0.95, f\"N√£o normal: p < 0.05\", transform=axes[i].transAxes, fontsize=12, color='red')\n",
    "\n",
    "# Ajustando layout\n",
    "plt.tight_layout(pad=0.5)  # Ajustando o espa√ßamento entre os subgr√°ficos\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Verificar se h√° GPU dispon√≠vel\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU dispon√≠vel:\", gpus)\n",
    "else:\n",
    "    print(\"GPU n√£o dispon√≠vel.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definindo a barra de progresso\n",
    "n_trials_ = 150\n",
    "threshold= 0.5\n",
    "progress_bar = tqdm(total=n_trials_, desc=\"Otimiza√ß√£o em andamento\", unit=\"trial\")\n",
    "\n",
    "# Definindo os pesos das m√©tricas para otimiza√ß√£o\n",
    "weights_skf = {\n",
    "    'Accuracy': 0.00,\n",
    "    'f1': 0.10,\n",
    "    'precision': 0.10,\n",
    "    'recall': 0.60,\n",
    "    'auc': 0.10,\n",
    "    'balanced_acc': 0.10,\n",
    "    'mcc': 0.00\n",
    "}\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler() # inicia normalizador\n",
    "\n",
    "\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42) # Inicia do BorderlineSMOTE\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    global progress_bar\n",
    "    global threshold \n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "    n_hidden_units = trial.suggest_int('n_hidden_units', 128, 256)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.4, 0.7)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128, 256])\n",
    "    epochs = trial.suggest_int('epochs', 30, 80)\n",
    "    \n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'tanh'])\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop', 'nadam'])\n",
    "\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.0001, 0.1, log=True)\n",
    "    l2_regularization = trial.suggest_float('l2_regularization', 0.001, 0.1, log=True)\n",
    "    momentum = trial.suggest_float('momentum', 0.4, 0.95) if optimizer == 'sgd' else 0.0\n",
    "\n",
    "    verbose = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Criando o modelo MLP\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(X_train_stacking.shape[1],)))\n",
    "\n",
    "    # Definindo o inicializador de pesos com base na fun√ß√£o de ativa√ß√£o\n",
    "    for _ in range(n_layers):\n",
    "        if activation_function == 'relu':\n",
    "            initializer = 'he_normal'\n",
    "        elif activation_function == 'tanh':\n",
    "            initializer = 'glorot_uniform'\n",
    "        else:\n",
    "            initializer = 'glorot_uniform'  # Op√ß√£o padr√£o caso queira adicionar outras ativations\n",
    "\n",
    "        model.add(layers.Dense(n_hidden_units, activation=activation_function, \n",
    "                                kernel_initializer=initializer,  # Usando o inicializador com base na fun√ß√£o de ativa√ß√£o\n",
    "                                kernel_regularizer=keras.regularizers.l2(l2_regularization)))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Camada de sa√≠da\n",
    "\n",
    "\n",
    "    # Compilando o modelo\n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    elif optimizer == 'nadam':\n",
    "        opt = keras.optimizers.Nadam(learning_rate=learning_rate)  # com a taxa de aprendizado\n",
    "    else:\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC', 'accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Realizando a valida√ß√£o cruzada estratificada\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    global weights_skf\n",
    "    weighted_scores = []\n",
    "    auc_roc_list, accuracy_list, precision_list, recall_list, f1_list, mcc_list, kappa_list, balanced_acc_list = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_stacking, y_train):\n",
    "        X_train_fold, X_val_fold = X_train_stacking.iloc[train_idx], X_train_stacking.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Normalizando os dados de treino (sem vazamento de dados)\n",
    "        X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold_scaled = scaler.transform(X_val_fold)  # Aplica a transforma√ß√£o nos dados de valida√ß√£o\n",
    "\n",
    "        # Aplicando o BorderlineSMOTE para balancear a classe minorit√°ria no treino\n",
    "        X_train_fold_scaled, y_train_fold = smote.fit_resample(X_train_fold_scaled, y_train_fold)\n",
    "\n",
    "        # Treinando o modelo\n",
    "        model.fit(X_train_fold_scaled, y_train_fold, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "        # Prevendo as probabilidades da classe 1\n",
    "        y_pred_prob = model.predict(X_val_fold_scaled, verbose=verbose).flatten()\n",
    "        # Garantir que n√£o h√° NaN nas previs√µes\n",
    "        y_pred_prob = np.nan_to_num(y_pred_prob)\n",
    "\n",
    "        y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "        auc_roc_list.append(roc_auc_score(y_val_fold, y_pred_prob))\n",
    "        accuracy_list.append(accuracy_score(y_val_fold, y_pred))\n",
    "        precision_list.append(precision_score(y_val_fold, y_pred))\n",
    "        recall_list.append(recall_score(y_val_fold, y_pred))\n",
    "        f1_list.append(f1_score(y_val_fold, y_pred))\n",
    "        mcc_list.append(matthews_corrcoef(y_val_fold, y_pred))\n",
    "        kappa_list.append(cohen_kappa_score(y_val_fold, y_pred))\n",
    "        balanced_acc_list.append(balanced_accuracy_score(y_val_fold, y_pred))\n",
    "\n",
    "        # M√©tricas para otimizar\n",
    "        metrics_mean = {\n",
    "            \"AUC-ROC\": np.mean(auc_roc_list),\n",
    "            \"Accuracy\": np.mean(accuracy_list),\n",
    "            \"Precision\": np.mean(precision_list),\n",
    "            \"Recall\": np.mean(recall_list),\n",
    "            \"F1-Score\": np.mean(f1_list),\n",
    "            \"MCC\": np.mean(mcc_list),\n",
    "            \"Kappa\": np.mean(kappa_list),\n",
    "            \"Balanced Accuracy\": np.mean(balanced_acc_list),\n",
    "        }\n",
    "\n",
    "        weighted_score = (\n",
    "            weights_skf['f1'] * metrics_mean[\"F1-Score\"] +\n",
    "            weights_skf['precision'] * metrics_mean[\"Precision\"] +\n",
    "            weights_skf['recall'] * metrics_mean[\"Recall\"] +\n",
    "            weights_skf['auc'] * metrics_mean[\"AUC-ROC\"] +\n",
    "            weights_skf['balanced_acc'] * metrics_mean[\"Balanced Accuracy\"] +\n",
    "            weights_skf['Accuracy'] * metrics_mean[\"Accuracy\"] +\n",
    "            weights_skf['mcc'] * metrics_mean[\"MCC\"]\n",
    "        )\n",
    "        weighted_scores.append(weighted_score)\n",
    "\n",
    "    # Imprimir o resumo das m√©tricas\n",
    "    print(\"üìä M√âTRICAS DOS FOLDS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'M√©trica':<25} {'Valor final'}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for metric, value in metrics_mean.items():\n",
    "        print(f\"{metric:<25} {value:.4f}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    progress_bar.update(1)\n",
    "    return np.mean(weighted_scores)\n",
    "\n",
    "# Otimiza√ß√£o com Optuna\n",
    "\n",
    "sampler_ = optuna.samplers.TPESampler(n_startup_trials=10, \n",
    "                                      n_ei_candidates=50, \n",
    "                                      group=True,seed=42,\n",
    "                                      multivariate=True)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler_, pruner=optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(), patience=15))\n",
    "study.optimize(objective, n_trials=n_trials_)  # Certifique-se de que n_trials_ est√° bem definido\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "# Melhores hiperpar√¢metros\n",
    "print(\"üìä MELHORES HIPERPAR√ÇMETROS ENCONTRADOS\")\n",
    "print(\"‚ïê\" * 60)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param:<25}{value:<15}\")\n",
    "print(\"‚ïê\" * 60)\n",
    "\n",
    "\n",
    "### TREINAMETNO FINAL \n",
    "\n",
    "# Criando modelo final com os melhores par√¢metros obtidos pelo Optuna \n",
    "final_model = keras.Sequential()\n",
    "\n",
    "# Adicionando a camada de entrada\n",
    "final_model.add(layers.InputLayer(input_shape=(X_train_stacking.shape[1],)))\n",
    "\n",
    "# Inicializador de pesos para a camada final (sigmoid)\n",
    "weight_initializer_final = 'glorot_uniform'  # Usando Glorot Uniform para a camada de sa√≠da sigmoid\n",
    "\n",
    "# Inicializador de pesos para as camadas ocultas\n",
    "if best_params['activation_function'] == 'relu':\n",
    "    weight_initializer_hidden = 'he_normal'  # He Normal para ReLU\n",
    "elif best_params['activation_function'] == 'tanh':\n",
    "    weight_initializer_hidden = 'glorot_uniform'  # Xavier ou Glorot Uniform para Tanh\n",
    "\n",
    "# Adicionando as camadas ocultas com os melhores par√¢metros\n",
    "for _ in range(best_params['n_layers']):\n",
    "    final_model.add(layers.Dense(best_params['n_hidden_units'], \n",
    "                                 activation=best_params['activation_function'],\n",
    "                                 kernel_initializer=weight_initializer_hidden,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(best_params['l2_regularization'] + best_params['weight_decay'])))  # Aplicando weight_decay aqui\n",
    "    final_model.add(layers.Dropout(best_params['dropout_rate']))  # Dropout ap√≥s cada camada densa\n",
    "\n",
    "# Camada de sa√≠da\n",
    "final_model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=weight_initializer_final))  # Sa√≠da bin√°ria\n",
    "\n",
    "# Escolhendo o otimizador com base nos melhores par√¢metros\n",
    "if best_params['optimizer'] == 'adam':\n",
    "    opt = keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    opt = keras.optimizers.SGD(learning_rate=best_params['learning_rate'], momentum=best_params['momentum'])\n",
    "else:\n",
    "    opt = keras.optimizers.RMSprop(learning_rate=best_params['learning_rate'])\n",
    "\n",
    "# Compilando o modelo\n",
    "final_model.compile(optimizer=opt, \n",
    "                    loss='binary_crossentropy', \n",
    "                    metrics=['AUC', 'accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### TREINANDO MODELO FINAL\n",
    "\n",
    "\n",
    "# Normalizando os dados de treinamento final\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_stacking)\n",
    "\n",
    "# Aplicando o BorderlineSMOTE no treinamento final para balancear as classes\n",
    "smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Criando o callback EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitorando a perda de valida√ß√£o\n",
    "    patience=15,  # Se n√£o houver melhoria por 15 √©pocas, o treinamento ser√° interrompido\n",
    "    restore_best_weights=True,  # Restaurando os melhores pesos do modelo\n",
    "    verbose=0  # MENSAGENS \n",
    ")\n",
    "\n",
    "# Treinando o modelo final com valida√ß√£o e capturando o hist√≥rico\n",
    "history = final_model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    epochs=best_params['epochs'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=0,  # Omitindo os logs a cada √©poca\n",
    "    validation_split=0.2,  # Usando 20% dos dados para valida√ß√£o\n",
    "    callbacks=[early_stopping]  # Passando o callback de EarlyStopping\n",
    ")\n",
    "\n",
    "# Extraindo os valores da perda do hist√≥rico de treinamento\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(loss) + 1)\n",
    "\n",
    "# Criando o gr√°fico da perda por √©poca\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, loss, label='Loss - Treinamento', color='blue')\n",
    "plt.plot(epochs_range, val_loss, label='Loss - Valida√ß√£o', color='red', linestyle='dashed')\n",
    "\n",
    "# Configurando os r√≥tulos e t√≠tulo\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Perda (Loss)')\n",
    "plt.title('Evolu√ß√£o da Perda Durante o Treinamento')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Exibindo o gr√°fico\n",
    "plt.show()\n",
    "########################################### ALTERADO ATE AQUI (TESTANTO)\n",
    "\n",
    "\n",
    "# Normalizando os dados de teste\n",
    "X_test_scaled = scaler.transform(X_test_stacking)\n",
    "\n",
    "# Fazendo predi√ß√µes com o modelo final\n",
    "y_pred_final_prob = final_model.predict(X_test_scaled,verbose=0).flatten()  # Obtendo as probabilidades\n",
    "\n",
    "# Definindo o limiar para determinar a classe final\n",
    "y_pred_final = (y_pred_final_prob >= threshold).astype(int)  # O limiar padr√£o √© 0.5, mas pode ser ajustado\n",
    "\n",
    "\n",
    "\n",
    "# C√°lculo das m√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred_final)\n",
    "precision = precision_score(y_test, y_pred_final)\n",
    "recall = recall_score(y_test, y_pred_final)\n",
    "f1 = f1_score(y_test, y_pred_final)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_final_prob)\n",
    "mcc = matthews_corrcoef(y_test, y_pred_final)\n",
    "kappa = cohen_kappa_score(y_test, y_pred_final)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred_final)\n",
    "gini = (2 * auc_roc - 1) * 100\n",
    "\n",
    "# Exibir as m√©tricas finais\n",
    "print(\"üìä M√âTRICAS FINAIS - TESTE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'M√©trica':<25} {'Valor final'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Acur√°cia':<25} {accuracy:.4f}\")\n",
    "print(f\"{'Precis√£o':<25} {precision:.4f}\")\n",
    "print(f\"{'Recall':<25} {recall:.4f}\")\n",
    "print(f\"{'F1-Score':<25} {f1:.4f}\")\n",
    "print(f\"{'AUC-ROC':<25} {auc_roc:.4f}\")\n",
    "print(f\"{'MCC':<25} {mcc:.4f}\")\n",
    "print(f\"{'Kappa':<25} {kappa:.4f}\")\n",
    "print(f\"{'Acur√°cia Balanceada':<25} {balanced_acc:.4f}\")\n",
    "print(f\"{'Gini':<25} {gini:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criando os gr√°ficos\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot da Matriz de Confus√£o\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negativo', 'Positivo'], yticklabels=['Negativo', 'Positivo'],\n",
    "            cbar=False, ax=axes[0])\n",
    "axes[0].set_title('Matriz de Confus√£o (TESTE) - MLP')\n",
    "axes[0].set_xlabel('Previs√£o')\n",
    "axes[0].set_ylabel('Real')\n",
    "\n",
    "# Plot da curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_final_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Taxa de Falsos Positivos')\n",
    "axes[1].set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "axes[1].set_title('Curva ROC (TESTE) - MLP')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# Distribui√ß√£o de probabilidades\n",
    "\n",
    "# classe positiva\n",
    "probs_pos = y_pred_final_prob  \n",
    "# classe negativa\n",
    "probs_neg = 1 - y_pred_final_prob\n",
    "\n",
    "# Plot classe positiva \n",
    "sns.kdeplot(probs_pos, color='blue', ax=axes[2], label='Classe Positiva', fill=True, alpha=0.6)\n",
    "\n",
    "# Plot classe negativa \n",
    "sns.kdeplot(probs_neg, color='red', ax=axes[2], label='Classe Negativa', fill=True, alpha=0.02, linewidth=0.30)\n",
    "\n",
    "# Ajustando o gr√°fico\n",
    "axes[2].set_title(\"Distribui√ß√£o das Probabilidades para as Classes Positiva e Negativa (TESTE) - MLP Keras\")\n",
    "axes[2].set_xlabel(\"Probabilidade\")\n",
    "axes[2].set_ylabel(\"Densidade\")\n",
    "axes[2].legend()\n",
    "# setando eixo de probabilidades entre 0 e 1\n",
    "axes[2].set_xlim(0, 1)\n",
    "\n",
    "\n",
    "# Exibindo o gr√°fico\n",
    "plt.show()\n",
    "\n",
    "# Ajustando o layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#final_model.summary() #infos do modelo\n",
    "\n",
    "\n",
    "\n",
    "#NAO CONSGUI CONFIGURAR GPU, MUITA IMCOMPATIBILIDADE DE BIBLIOTECAS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicabilidade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Global\n",
    "\n",
    "* SHAP Global √© uma t√©cnica de explicabilidade que ajuda a entender a import√¢ncia de cada vari√°vel (feature) nas predi√ß√µes do modelo de maneira global, ou seja, avaliando o impacto m√©dio de cada feature ao longo de todas as inst√¢ncias do conjunto de dados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Cria√ß√£o do explicador SHAP\n",
    "\n",
    "background_samples = shap.kmeans(X_train_scaled, 50)  # 100 clusters representativos\n",
    "\n",
    "explainer = shap.KernelExplainer(lambda x: final_model.predict(x, verbose=0), background_samples,n_jobs=-1 )\n",
    "\n",
    "\n",
    "# Calculando os valores SHAP para o conjunto de teste\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Plotando o gr√°fico de import√¢ncia global com o SHAP\n",
    "shap.summary_plot(shap_values[0], X_test_scaled, feature_names=[f\"Feature {i+1}\" for i in range(X_test_scaled.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME (Local) \n",
    "\n",
    "* LIME (Local Interpretable Model-agnostic Explanations) √© uma t√©cnica de explicabilidade que busca interpretar predi√ß√µes de modelos de forma local, ou seja, explicar como o modelo chegou a uma decis√£o espec√≠fica para uma inst√¢ncia de dado individual. \n",
    "\n",
    "* Ao contr√°rio de m√©todos globais, como o SHAP, que explicam a import√¢ncia das features em todas as predi√ß√µes, o LIME foca na explica√ß√£o de uma √∫nica predi√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o explicador LIME para um modelo LightGBM\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_test_stacking.values,  # Dados de treino\n",
    "    training_labels=y_test.values,         # R√≥tulos de treino\n",
    "    mode=\"classification\",                  # Tipo de problema (classifica√ß√£o)\n",
    "    class_names=[\"Negativo\", \"Positivo\"],   # Nomes das classes\n",
    "    feature_names=X_test_stacking.columns, # Nomes das features\n",
    "    discretize_continuous=True              # Discretizar vari√°veis cont√≠nuas\n",
    ")\n",
    "\n",
    "# Selecionando uma inst√¢ncia para explicar\n",
    "idx = 200  # √çndice do exemplo/observa√ß√£o a ser explicado\n",
    "instance = X_test_stacking.iloc[idx].values.reshape(1, -1)\n",
    "\n",
    "# Fun√ß√£o para obter probabilidades usando LightGBM (para classifica√ß√£o bin√°ria)\n",
    "def predict_proba_fn(x):\n",
    "    raw_preds = final_model.predict(x)  # Usando o m√©todo predict do LightGBM\n",
    "    # Convertendo as margens (logits) para probabilidades\n",
    "    probabilities = 1 / (1 + np.exp(-raw_preds))  # Sigmoide para classifica√ß√£o bin√°ria\n",
    "    return np.array([1 - probabilities, probabilities]).T  # Retorna a probabilidade para cada classe\n",
    "\n",
    "# Gerando explica√ß√£o local com probabilidades de classe\n",
    "explanation = explainer.explain_instance(\n",
    "    instance.flatten(),  # Passando os valores da inst√¢ncia em um formato adequado\n",
    "    predict_proba_fn,     # Usando a fun√ß√£o que retorna as probabilidades\n",
    "    num_features=40      # N√∫mero de caracter√≠sticas a serem mostradas\n",
    ")\n",
    "\n",
    "# Plotando a explica√ß√£o\n",
    "fig = explanation.as_pyplot_figure()\n",
    "plt.show()\n",
    "\n",
    "# Gerando gr√°fico de import√¢ncia das caracter√≠sticas (se necess√°rio)\n",
    "# explanation.as_list()  # Para mostrar os valores, tipo print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3197960,
     "sourceId": 5550559,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
