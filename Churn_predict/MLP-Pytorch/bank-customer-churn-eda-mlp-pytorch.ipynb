{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 #instalando Pytorch com suporte a GPU\n",
    "#testando a instalacao \n",
    "#import torch\n",
    "#print(torch.__version__)  # Deve exibir a versão do PyTorch instalada\n",
    "#print(torch.cuda.is_available())  # Deve retornar True se a GPU estiver funcionando\n",
    "#print(torch.cuda.get_device_name())  # exibe sua placa de video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, balanced_accuracy_score, matthews_corrcoef, \n",
    "    f1_score, accuracy_score, roc_curve, auc, make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import logging\n",
    "import warnings\n",
    "from torch.optim import lr_scheduler\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "########################### para KAGGLE ################################################################################################################\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicinário de Dados \n",
    "\n",
    "\n",
    "| **Variável**         | **Tipo**   | **Descrição**                                                                                                                                     |\n",
    "|-----------------------|------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| RowNumber            | int64      | Número do registro (linhas), sem efeito na construção de modelos.                                                                                |\n",
    "| CustomerId           | int64      | ID do cliente, sem efeito sobre o estudo.                                                                                                       |\n",
    "| Surname              | object     | Sobrenome do cliente, sem impacto na análise.                                                                                                   |\n",
    "| CreditScore          | int64      | Pontuação de crédito, pode indicar tendência de permanência de clientes com pontuação alta.                                                     |\n",
    "| Geography            | object     | Localização do cliente, pode influenciar a decisão de evasão.                                                                                   |\n",
    "| Gender               | object     | Gênero do cliente, possível influência na evasão.                                                                                               |\n",
    "| Age                  | int64      | Idade do cliente, clientes mais velhos tendem a permanecer.                                                                                     |\n",
    "| Tenure               | int64      | Anos que o cliente está no banco, clientes novos têm maior chance de evasão.                                                                    |\n",
    "| Balance              | float64    | Saldo na conta, pessoas com saldos altos são menos propensas a sair.                                                                            |\n",
    "| NumOfProducts        | int64      | Número de produtos adquiridos pelo cliente.                                                                                                    |\n",
    "| HasCrCard            | int64      | Indica se o cliente tem cartão de crédito, clientes com cartão são menos propensos à evasão.                                                    |\n",
    "| IsActiveMember       | int64      | Clientes ativos têm menor chance de evasão.                                                                                                    |\n",
    "| EstimatedSalary      | float64    | Salário estimado, clientes com salários mais altos tendem a permanecer.                                                                         |\n",
    "| Exited               | int64      | Indica se o cliente saiu ou não do banco, variável de predição (“churn”).                                                                       |\n",
    "| Complain             | int64      | Indica se o cliente fez reclamação.                                                                                                             |\n",
    "| Satisfaction Score   | int64      | Pontuação de satisfação com a resolução de reclamação.                                                                                          |\n",
    "| Card Type            | object     | Tipo de cartão que o cliente possui.                                                                                                            |\n",
    "| Points Earned        | int64      | Pontos ganhos pelo cliente.                                                                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:51:49.217695Z",
     "iopub.status.busy": "2024-12-21T05:51:49.217244Z",
     "iopub.status.idle": "2024-12-21T05:51:49.222777Z",
     "shell.execute_reply": "2024-12-21T05:51:49.221466Z",
     "shell.execute_reply.started": "2024-12-21T05:51:49.217658Z"
    }
   },
   "source": [
    "#  Análise Exploratória (EDA) & Data Prep\n",
    "\n",
    "Primeiras linhas para primeiro contato com a base de dados \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base_original = pd.read_csv('/kaggle/input/Customer-Churn-Records.csv', sep=',') #KAGGLE\n",
    "base_original = pd.read_csv('C:/Users/jgeov/iCloudDrive/Treinamento/Treinamento Data Science/Projetos/Customer-Churn-Records.csv',sep=',') #LOCAL\n",
    "\n",
    "#configs para nao quebrar linhas no print do  df\n",
    "pd.set_option('display.expand_frame_repr', False) \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#primeiras linhas \n",
    "base_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensões, Tipos e checagem de missing values (nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensões \n",
    "print(\"Numero de linhas:\", base_original.shape[0]) \n",
    "print(\"Numero de colunas:\", base_original.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tipos\n",
    "base_original.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checando se há valores nulos \n",
    "base_original.isnull().sum()  \n",
    "#valores nulos nao encontrados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Limpando variaveis inuteis para analise\n",
    "* Resumo estatistico de veriaveis quantitativas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removidas por serem meramente identificadoras: RowNumber, CustomerId e Surname \n",
    "\n",
    "#removida Gender por poder inviesar o modelo de alguma formna descriminativa, é uma boa pratica de LGPD nao usar dados sensiveis como esse etc. \n",
    "\n",
    "df = base_original[['CreditScore',\n",
    "                    #'Gender',\n",
    "                    'Geography',\n",
    "                    'Age',\n",
    "                    'Tenure',\n",
    "                    'Balance',\n",
    "                   'NumOfProducts',\n",
    "                    'HasCrCard',\n",
    "                    'IsActiveMember',\n",
    "                   'EstimatedSalary',\n",
    "                    'Complain',\n",
    "                    'Satisfaction Score',\n",
    "                   'Card Type',\n",
    "                    'Point Earned',\n",
    "                    'Exited'\n",
    "                   ]]\n",
    "\n",
    "# Resumo estatístico \n",
    "quanti = df[['EstimatedSalary', 'Balance', 'CreditScore', 'Age', 'Tenure', 'Point Earned']]\n",
    "resumo_estati_quant = quanti.describe().style.format(lambda x: f'{x:,.1f}'.replace(',', 'X').replace('.', ',').replace('X', '.')) # Formatação com 1 casa decimal e separadores invertidos\n",
    "\n",
    "resumo_estati_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Resumo estatistico de variaveis qualitativas (frequancias)\n",
    "\n",
    "* Os resumos estatisticos sao importantes para primeiras nocoes de desbalance, a amplitude e distribuicao de valores\n",
    "minimos maximos e um breve entendimento se serao necessarios tratamentos nessas variaveis, decorrentes dessas observacoes; \n",
    "\n",
    "* Podemos notar que a principio as ditribuicoes nao sao absurdas e o desbalance esta pricipalemnte nas variavies Complain e Exited (variavel alvo do estudo, a chamaremos de churn) indicando que sera necessario tratar isso;\n",
    "\n",
    "* Franca tem mais observacoes que os demais paises; \n",
    "\n",
    "* A maioria dos clientes tem cartao de credito; \n",
    "\n",
    "* A maioria dos clientes tem entre 1 e 2 produtos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumo estatistico \n",
    "\n",
    "#separando quali's para analise \n",
    "quali = df[['HasCrCard', 'IsActiveMember', 'Geography',\n",
    "            #'Gender',\n",
    "             'Complain','Exited','Card Type','NumOfProducts','Satisfaction Score']]\n",
    "\n",
    "quali = quali.astype('object')\n",
    "\n",
    "#quali.dtypes\n",
    "\n",
    "\n",
    "\n",
    "def add_value_labels(ax):\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        color = p.get_facecolor()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height / 2.,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='center', fontsize=20, color='white', fontweight='bold',\n",
    "                bbox=dict(facecolor=color, edgecolor='none', alpha=0.7,\n",
    "                          boxstyle='round,pad=0.4', linewidth=1))\n",
    "\n",
    "plt.figure(figsize=(20, 25))\n",
    "\n",
    "# Geography\n",
    "plt.subplot(5, 2, 1)\n",
    "ax1 = plt.gca()\n",
    "ax1.set_title('Geography', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Geography', hue='Geography', palette='viridis', data=base_original, ax=ax1, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax1)\n",
    "\n",
    "\n",
    "# Complain\n",
    "plt.subplot(5, 2, 2)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('Complain', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Complain', hue='Complain', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "# HasCrCard\n",
    "plt.subplot(5, 2, 3)\n",
    "ax5 = plt.gca()\n",
    "ax5.set_title('HasCrCard', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='HasCrCard', hue='HasCrCard', palette='viridis', data=base_original, ax=ax5, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax5)\n",
    "\n",
    "# IsActiveMember\n",
    "plt.subplot(5, 2, 4)\n",
    "ax6 = plt.gca()\n",
    "ax6.set_title('IsActiveMember', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='IsActiveMember', hue='IsActiveMember', palette='viridis', data=base_original, ax=ax6, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax6)\n",
    "\n",
    "# Card Type\n",
    "plt.subplot(5, 2, 5)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('Card Type', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Card Type', hue='Card Type', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "\n",
    "# NumOfProducts\n",
    "plt.subplot(5, 2, 6)\n",
    "ax10 = plt.gca()\n",
    "ax10.set_title('NumOfProducts', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='NumOfProducts', hue='NumOfProducts', palette='viridis', data=base_original, ax=ax10, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax10)\n",
    "\n",
    "\n",
    "# Satisfaction Score\n",
    "plt.subplot(5, 2, 7)\n",
    "ax11 = plt.gca()\n",
    "ax11.set_title('Satisfaction Score', fontsize=22, fontweight='bold')\n",
    "sns.countplot(x='Satisfaction Score', hue='Satisfaction Score', palette='viridis', data=base_original, ax=ax11, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax11)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exited\n",
    "plt.subplot(5, 2, 8)\n",
    "ax11 = plt.gca()\n",
    "ax11.set_title('Exited: Churn ', fontsize=22, fontweight='bold')\n",
    "custom_palette = ['green', 'red']\n",
    "sns.countplot(x='Exited', hue='Exited', palette=custom_palette, data=base_original, ax=ax11, legend=False)\n",
    "plt.xlabel('') \n",
    "plt.ylabel('') \n",
    "plt.xticks(fontsize=15, rotation=0, fontweight='bold')\n",
    "plt.yticks([])\n",
    "add_value_labels(ax11)\n",
    "\n",
    "ax11.set_xticks([0, 1])\n",
    "ax11.set_xticklabels(['Não', 'Sim'], fontsize=15, fontweight='bold')\n",
    "\n",
    "# Ajustando espaçamento\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualizando o comportmento da variavel alvo (exited) em relacao as demais variaveis; \n",
    "\n",
    "* Vemos claramente que existe o disbalance de classes na variavel churn, pela cor verde presente fortemente em todas variaveis, posteriormente isso sera tratado/mitigado; \n",
    "\n",
    "* Ja e possivel notar um forte indicio de alta correlacao entre churn e complain, posteriormente isso sera testado. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Variável alvo em relação as demais variáveis \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 25)) #tamanho do painel grafico\n",
    "\n",
    "#funcao de adicao de legenda no canto superior direito e garante rotulos \n",
    "def add_legend(ax):\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if not handles:\n",
    "        \n",
    "        # Se não houver handles, forca a adicao\n",
    "        handles = [plt.Rectangle((0,0),1,1, color=c) for c in ['green', 'red']]\n",
    "        labels = ['Not Exited', 'Exited']\n",
    "        \n",
    "    # Adiciona a legenda fora da área das barras\n",
    "    ax.legend(handles, labels, loc='upper left', fontsize=14, title='Exited', title_fontsize='13',  \n",
    "              bbox_to_anchor=(1.0, 1)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Geography\n",
    "plt.subplot(5, 2, 1)\n",
    "counts = base_original.groupby(['Geography', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Geography', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Geography', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "\n",
    "# NumOfProducts\n",
    "plt.subplot(5, 2, 3)\n",
    "counts = base_original.groupby(['NumOfProducts', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by NumOfProducts', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('NumOfProducts', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# HasCrCard\n",
    "plt.subplot(5, 2, 4)\n",
    "counts = base_original.groupby(['HasCrCard', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by HasCrCard', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('HasCrCard', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# IsActiveMember\n",
    "plt.subplot(5, 2, 5)\n",
    "counts = base_original.groupby(['IsActiveMember', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca()) \n",
    "plt.title('Exited by IsActiveMember', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('IsActiveMember', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Complain\n",
    "plt.subplot(5, 2, 6)\n",
    "counts = base_original.groupby(['Complain', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Complain', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Complain', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Satisfaction Score\n",
    "plt.subplot(5, 2, 7)\n",
    "counts = base_original.groupby(['Satisfaction Score', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Satisfaction Score', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Satisfaction Score', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Card Type\n",
    "plt.subplot(5, 2, 8)\n",
    "counts = base_original.groupby(['Card Type', 'Exited']).size().unstack().fillna(0)\n",
    "ax = counts.plot(kind='bar', stacked=True, color=['green', 'red'], ax=plt.gca())  \n",
    "plt.title('Exited by Card Type', fontsize=22, fontweight='bold')\n",
    "plt.xlabel('Card Type', fontsize=16)\n",
    "plt.ylabel('', fontsize=16)\n",
    "plt.xticks(fontsize=14, rotation=0,fontweight='bold')\n",
    "plt.yticks(fontsize=14)\n",
    "add_legend(ax)\n",
    "\n",
    "# Ajusta a distância entre os gráficos\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variaveis Dummies\n",
    "\n",
    "* A maioria dos modelos necessita de transformar as variaveis categoricas em numericas, e o modelo MLP (Multilayer Perceptron) e um deles; \n",
    "\n",
    "* A transformacao de categoricas em numericas precisa ser feita com processos adequados para nao cometer ponderacao arbitrária no desenvolvimento. \n",
    "\n",
    "* foi aplicado one-hot encoding para isso. Esse processo e chamado de \"Dummizacao\". \n",
    "\n",
    "* foi necessario aplicar Ordinal-Encoder. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumizando\n",
    "\n",
    "# Suprime todos os warnings de futuro (deixa mais clean)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Lista de variáveis a serem transformadas\n",
    "cols_to_transform = ['Geography']\n",
    "\n",
    "# Convertendo para string (somente a coluna \"Geography\")\n",
    "df.loc[:, cols_to_transform] = df.loc[:, cols_to_transform].astype(str)\n",
    "\n",
    "# Realizando o One-Hot Encoding \n",
    "df_dummies = pd.get_dummies(df, columns=cols_to_transform, dtype=int, drop_first=False)\n",
    "\n",
    "# Ordinal-Encoder \n",
    "df_dummies['Card Type'] = OrdinalEncoder(categories=[[\"SILVER\", \"GOLD\", \"PLATINUM\", \"DIAMOND\"]], dtype=int).fit_transform(df_dummies[['Card Type']])\n",
    "\n",
    "# Variável alvo 'Exited' para o tipo numérico (se necessário)\n",
    "df_dummies['Exited'] = df_dummies['Exited'].astype('int64')\n",
    "\n",
    "print(df_dummies.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separação Treino e Teste & Adicao de Features quadráticas\n",
    "\n",
    "* A separacao em treino e teste alem de uma boa pratica e extreamente necessario na construcao de modelos de machine learning; \n",
    "\n",
    "* Tambem foram adicionadas variaveis quadraticas, ou seja, com operacao matematica aplicadas em variaveis originais gerando novas variaveis. Isso foi feito para capturar algum tipo de comportamento nao linear; \n",
    "\n",
    "* Foi considerado aplicar transformacao polinomial nas variaveis, por isso foi primeiro aplicado o termo quadratico, que nao apresentou melhoria significativa a ponto de aplicarmos polinomias; \n",
    "\n",
    "* Alem disso MLPs capturam naturalmente comportamentos nao lineares. O termo quadratico foi util para validacao durante o estudo mas a melhoria foi baixa, por isso mantemos apenas os termos quadraticos sem incluir interacoes entre variaveis (seria aplicacao Polinomias completo); \n",
    "\n",
    "\n",
    "\n",
    "* Tambem e possivel notar o desbalanceamento das classes  nas bases tanto em treino quanto em teste no grafico final; \n",
    "\n",
    "* Tambem garantimos a mesma proporcao (80/20) tanto em treino quanto em teste na separacao das bases , ou seja, equidade de divisao de dados e equilibrio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X ---> Variáveis explicativas \n",
    "#Y ---> Evento de estudo (variável TARGET, evento de estudo, ^y etc..)\n",
    "\n",
    "df_dummies = df_dummies.drop(columns=['Complain'])\n",
    "\n",
    "\n",
    "X = df_dummies .drop('Exited', axis=1)\n",
    "\n",
    "# VARIAVEIS QUADRATICAS \n",
    "X['Balance_Squared'] = X['Balance'] ** 2\n",
    "X['Age_Squared'] = X['Age'] ** 2\n",
    "X['CreditScore_Squared'] = X['CreditScore'] ** 2\n",
    "X['Tenure_Squared'] = X['Tenure'] ** 2\n",
    "X['EstimatedSalary_Squared'] = X['EstimatedSalary'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "y =  df_dummies['Exited']\n",
    "\n",
    "\n",
    "#separando em treino e teste \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Visualizando a proporção de eventos de churn (TARGET) nas bases de TREINO e TESTE \n",
    "\n",
    "# Contando os valores \n",
    "churn_counts_train = y_train.value_counts()\n",
    "churn_counts_test = y_test.value_counts()\n",
    "\n",
    "\n",
    "# plot que contem os graficos\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle('Proporção da Variável Churn entre Treino e Teste', fontsize=35)  \n",
    "cmap = plt.get_cmap('viridis', 2) #paleta de cores\n",
    "\n",
    "\n",
    "\n",
    "# Gráfico da base de treino\n",
    "bars_train = axs[0].bar(churn_counts_train.index, churn_counts_train.values, color=cmap(range(2)))\n",
    "axs[0].set_title('Base de Treino', fontsize=25)\n",
    "axs[0].set_xlabel('Churn', fontsize=20)\n",
    "axs[0].set_ylabel('Contagem', fontsize=20)\n",
    "axs[0].set_xticks([0, 1])\n",
    "axs[0].set_xticklabels(['0', '1'], fontsize=20)\n",
    "axs[0].set_yticklabels([]) # Ocultando os valores do eixo y\n",
    "\n",
    "# Adicionando rótulos de dados\n",
    "total_train = churn_counts_train.sum()\n",
    "for bar in bars_train:\n",
    "    count = int(bar.get_height())\n",
    "    percentage = round(count / total_train * 100)  # Arredonda a porcentagem\n",
    "    label = f'{count} ({percentage}%)'  #valor absoluto e o percentual\n",
    "    axs[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, \n",
    "                label, ha='center', color='gray', fontsize=25, weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Gráfico da base de teste\n",
    "bars_test = axs[1].bar(churn_counts_test.index, churn_counts_test.values, color=cmap(range(2)))\n",
    "axs[1].set_title('Base de Teste', fontsize=25)\n",
    "axs[1].set_xlabel('Churn', fontsize=20)\n",
    "axs[1].set_ylabel('Contagem', fontsize=20)\n",
    "axs[1].set_xticks([0, 1])\n",
    "axs[1].set_xticklabels(['0', '1'], fontsize=20)\n",
    "axs[1].set_yticklabels([])# Ocultando os valores do eixo y\n",
    "\n",
    "# Adicionando rótulos de dados\n",
    "total_test = churn_counts_test.sum()\n",
    "for bar in bars_test:\n",
    "    count = int(bar.get_height())\n",
    "    percentage = round(count / total_test * 100)  # Arredonda a porcentagem\n",
    "    label = f'{count} ({percentage}%)'  #valor absoluto e o percentual\n",
    "    axs[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, \n",
    "                label, ha='center', color='gray', fontsize=25, weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "# Ajusta o layout para evitar sobreposição\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # forca espaco para o titulo\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando correlacoes depois de construcao total de features e Dummizacao \n",
    "\n",
    "* Verificar as correlacoes e extreammente importante, elas podem indicar a famosa multicolinearidade, que atrapalha a maioria dos modelos; \n",
    "\n",
    "* no caso da MLP (pelo menos a presente aplicacao) ela nao afetou, a observei com atencao, mas nao impactou. Decidi manter as variveis mesmo com multicolinearidade em algumas (nao se assuste). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observando Multicolinearidade na base de treino\n",
    "\n",
    "teste_multco_treino = pd.concat([X_train,y_train], axis = 1)\n",
    "\n",
    "correlation_matrix_treino = teste_multco_treino.corr().round(2)\n",
    "correlation_matrix_treino\n",
    "\n",
    "# Matrix com  mapa de calor \n",
    "plt.figure(figsize=(30, 20))\n",
    "heatmap = sns.heatmap(correlation_matrix_treino, annot=True, fmt=\".2f\",\n",
    "                      cmap=plt.cm.viridis_r, # paleta de cores viridis (ou viridis_r para o inverso de cores) é uma paleta especial \n",
    "                                             # para facilitar a visualizacao por pessoas com dificuldades visuais, como os daltonicos. \n",
    "                      annot_kws={'size': 15}, vmin=-1, vmax=1)\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=17)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=17)\n",
    "plt.title('Correlação das Variáveis Quantitativas na Base de Treino',fontsize=25)\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analise e tratamento de Outliers \n",
    "\n",
    "* Outliers sao numericos e podem afetar de diversas formas modelos de MLP porque eles utilizam backpropagation e descida do gradiente, entre outras coisas que podem atrapalhar a capacidade preditiva do modelo; \n",
    "\n",
    "* para resolver sem perder dados, pois temos poucas observcoes para estudo, nao foram removidos como facilmente poderia fazer-se, ao inves foi aplicado winsorization; \n",
    "\n",
    "* winsorization e uma tecnica de limitacao dos outliers, ela substiui os valores de outlierns pelos limites superiores e inferiores; \n",
    "\n",
    "* Para isso, e calculado um intervalo de valores aceitos com base no primeiro quartil (Q1) e no terceiro quartil (Q3), valores abaixo do limite inferior ou acima do limite superior sao ajustados para os respectivos limites, corrigindo assim os outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% analise de outliers das variaveis na base de treino \n",
    "\n",
    "\n",
    "###############antes de tratamento############################# \n",
    "variaveis = [\n",
    "    'CreditScore',\n",
    "    'Age',\n",
    "    'Tenure',\n",
    "    'Balance',\n",
    "    'NumOfProducts',\n",
    "    'EstimatedSalary',\n",
    "    'Satisfaction Score',\n",
    "    'Point Earned',\n",
    "    #QUADRATICAS\n",
    "    'Balance_Squared',\n",
    "    'Age_Squared',\n",
    "    'CreditScore_Squared',\n",
    "    'Tenure_Squared',\n",
    "    'EstimatedSalary_Squared'\n",
    "]\n",
    "\n",
    "\n",
    "# definindo tamnhos de subplots \n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# loop de criacao de boxplots para cada variavel \n",
    "for i, var in enumerate(variaveis):\n",
    "    plt.subplot(5, 4, i + 1)  #determina a grade de plots \n",
    "    sns.boxplot(y=teste_multco_treino[var],\n",
    "               boxprops=dict(facecolor='lightblue'))  # Cor interna do boxplot \n",
    "    plt.title(f'Boxplot {var}', fontsize=12)\n",
    "    \n",
    "#  título geral\n",
    "plt.suptitle('Análise de Outliers nas Variáveis(treino) - antes de \"winsorization\" ', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95]) # Ajuste de layout\n",
    "plt.show()\n",
    "###############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Função que aplica winsorization\n",
    "def tratar_outliers(df, coluna):\n",
    "    Q1 = df[coluna].quantile(0.25)\n",
    "    Q3 = df[coluna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    # Substitui outliers pelo limite inferior ou superior\n",
    "    df[coluna] = np.where(df[coluna] < limite_inferior, limite_inferior, df[coluna])\n",
    "    df[coluna] = np.where(df[coluna] > limite_superior, limite_superior, df[coluna])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############depois de tratamento############################# \n",
    "\n",
    "# Aplicando a função nas variáveis \n",
    "variaveis_para_tratar = ['CreditScore',\n",
    "                        'Age',\n",
    "                        'Tenure',\n",
    "                        'Balance',\n",
    "                        'NumOfProducts',\n",
    "                        'EstimatedSalary',\n",
    "                        'Satisfaction Score',\n",
    "                        'Point Earned',\n",
    "                        #QUADRATICAS\n",
    "                        'Balance_Squared',\n",
    "                        'Age_Squared',\n",
    "                        'CreditScore_Squared',\n",
    "                        'Tenure_Squared',\n",
    "                        'EstimatedSalary_Squared'\n",
    "                         ]\n",
    "for variavel in variaveis_para_tratar:\n",
    "    tratar_outliers(teste_multco_treino, variavel)\n",
    "\n",
    "\n",
    "# subplot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "\n",
    "\n",
    "# loop de criacao de boxplots para cada variavel \n",
    "for i, var in enumerate(variaveis):\n",
    "    plt.subplot(5, 4, i + 1)  #determina a grade de plots \n",
    "    sns.boxplot(y=teste_multco_treino[var],\n",
    "               boxprops=dict(facecolor='green'))  # Cor interna do boxplot\n",
    "    plt.title(f'Boxplot {var}', fontsize=12)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "# título geral\n",
    "plt.suptitle('Análise de Outliers nas Variáveis(treino) - depois de \"winsorization\" ', fontsize=20)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # ajusta layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* verificando as bases de treino e teste uma ultima vez para garantir que estao corretas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE DE TREINO \n",
    "teste_multco_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE DE TESTE\n",
    "base_corrige_teste = pd.concat([X_test,y_test], axis = 1)\n",
    "base_corrige_teste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem: aplicando MLP com Pytorch\n",
    "\n",
    "* O PyTorch é uma biblioteca de aprendizado profundo de código aberto, usada principalmente para o treinamento de redes neurais artificiais. Ele oferece flexibilidade e eficiência, permitindo a criação de modelos complexos e o treinamento acelerado com GPUs. O PyTorch é popular entre os pesquisadores devido à sua abordagem dinâmica de construção de grafos computacionais, facilitando a experimentação.\n",
    "\n",
    "* Para mais informações, você pode acessar a documentação oficial do PyTorch:\n",
    "https://pytorch.org/docs/stable/\n",
    "\n",
    "\n",
    "* Uma MLP (Multilayer Perceptron) é um tipo de rede neural artificial composta por múltiplas camadas de neurônios, usada principalmente para tarefas de classificação e regressão.\n",
    "\n",
    "\n",
    "* **OBS: Para conseguir usar GPU com CUDA fora do kaggle é necessario verificar se sua placa de video suporta (da um google ai) caso suporte e esteja configurado, o codigo ira habilitar automaticamente. O codigo tambem executa em CPU, basta nao ativar a GPU (CUDA) que ele seleciona a CPU automaticamente. A primeira celula tem comentado um pouco de como ativei.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️. **Pré-processamento e Preparação dos Dados:**\n",
    "* definir X_train e y_train;\n",
    "\n",
    "2️. **Seleção de Features com Random Forest:**\n",
    "* Modelo Random Forest treinado para identificar variáveis mais relevantes;\n",
    "* variaveis com importancia observada maior que 0.01 foram selecionadas para participarem do modelo;\n",
    "\n",
    "3️. **Escalonamento dos Dados:**\n",
    "* StandardScaler para normalizar as features selecionadas ( X_train e X_test);\n",
    "\n",
    "4️. **Preparação para o Treinamento com PyTorch:**\n",
    "* Configuração do dispositivo (GPU CUDA ou CPU);\n",
    "* Conversão dos dados escalonados em tensores PyTorch;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------')\n",
    "print(\"MLP - Multi-Layer Perceptron \")\n",
    "print('------------------------')\n",
    "\n",
    "# Desativando os warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='optuna')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='optuna')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "\n",
    "# início\n",
    "start_time_utc = datetime.utcnow() - timedelta(hours=3)\n",
    "print('------------------------')\n",
    "print(\"Início:\", start_time_utc)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "######################################## PRE-PROCESSAMENTO E PREPARACAO NOS DADOS ################################################################### \n",
    "\n",
    "\n",
    "# Defini variáveis de treinamento\n",
    "X_train = teste_multco_treino.drop('Exited', axis=1)\n",
    "y_train = teste_multco_treino['Exited']\n",
    "\n",
    "# Verifica e conserta desalinhamento de índices caso tenha (X_test e y_test)\n",
    "if not X_test.index.equals(y_test.index):\n",
    "    print(\"Índices de X_test e y_test não estavam alinhados. Realinhando y_test.\")\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "else:\n",
    "    print(\"Índices de X_test e y_test já estavam alinhados.\")\n",
    "\n",
    "# Concatena os dados corrigidos para criar a base de teste\n",
    "base_corrige_teste = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Redefini X_test e y_test com índices corrigidos e verificados\n",
    "X_test = base_corrige_teste.drop('Exited', axis=1)\n",
    "y_test = base_corrige_teste['Exited']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Criar e treinar o modelo Random Forest para selecionar as melhores variaveis \n",
    "\n",
    "# Mais estavel que o mutual_info_classif e melhor que ANOVA\n",
    "# (ele captura bem relacoes nao lineares e nao sofre variacoes como mutual_info)\n",
    "\n",
    "#detalhe importante, nao tem predict(), somente o fit() \n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Obter a importância das features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Criar DataFrame com os nomes das features e suas importâncias\n",
    "feature_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Filtrar features com importância maior que 0.01 (ou outro threshold desejado)\n",
    "selected_features = feature_df[feature_df['Importance'] > 0.01]['Feature'].tolist()\n",
    "\n",
    "# Ordenar as features pela importância (do menor para o maior)\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Criar o gráfico de barras horizontais\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_df['Feature'], feature_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importância')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Importância das Features Selecionadas (Random Forest)')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xticks(np.arange(0, max(feature_df['Importance'])+0.01, 0.01))\n",
    "\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()\n",
    "\n",
    "# Aplicar a seleção das mesmas features ao conjunto de treino e teste\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Escalonar os dados de treino e teste selecionados\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Configurar dispositivo para PyTorch (verifica a disponibilidade de GPU, caso contrário, usa CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # CUDA ou CPU\n",
    "# device = torch.device(\"cpu\")  # Forçar uso da CPU, se necessário\n",
    "\n",
    "print(\"Dispositivo configurado:\", device)\n",
    "\n",
    "\n",
    "bar = tqdm(total=1, desc=\"Processando tensores\", ncols=100, position=0) #barra de progresso plotada, ajuda a monitorar\n",
    "\n",
    "# Convertendo para tensores PyTorch e movendo para o dispositivo\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)  \n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)  \n",
    "\n",
    "bar.update(1) #incrementa barra\n",
    "bar.close()#fecha barra\n",
    "\n",
    "# Verificando os shapes dos tensores depois de criados e dispositivo ativado em cada um, \n",
    "# para ter certeza da compatibilidade de processamento e tamanho\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape} | device: {X_train_tensor.device}\")\n",
    "print(f\"y_train_tensor shape: {y_train_tensor.shape} | device: {y_train_tensor.device}\")\n",
    "print(f\"X_test_tensor shape: {X_test_tensor.shape} | device: {X_test_tensor.device}\")\n",
    "print(f\"y_test_tensor shape: {y_test_tensor.shape} | device: {y_test_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Definição da Arquitetura do Modelo (MLP):**\n",
    "\n",
    "* Configuração das camadas ocultas e da de saída;\n",
    "* Aplicação de função de ativação selecionável (ReLU, Tanh, etc.); \n",
    "* Uso de Dropout para reduzir overfitting;\n",
    "* Função de perda: BCEWithLogitsLoss com ponderação para classe minoritária; \n",
    "* Otimização: Suporte a vários otimizadores (Adam, SGD, etc.); \n",
    "* Agendador de taxa de aprendizado (StepLR, CosineAnnealing, ReduceLROnPlateau, etc.);\n",
    "* Salvamento do melhor modelo para aproveitamento futuro (torch.save); \n",
    "\n",
    "6. **Otimização de Hiperparâmetros com Optuna:**\n",
    "\n",
    "* Busca de hiperparâmetros usando Optuna (similar ao GridSearch, mas mais eficiente);\n",
    "* Definição de hiperparâmetros para estudo do optuna: taxa de aprendizado, peso de regularização (alpha e weight_decay), dropout, número de épocas etc; \n",
    "\n",
    "7. **Validação Cruzada com Stratified K-Fold:** \n",
    "\n",
    "* Divisão dos dados em folds estratificados;\n",
    "* Rebalanceamento da classe minoritária com ADASYN (oversampling adaptativo); \n",
    "* Uso de CUDA (GPU) para acelerar o treinamento;\n",
    "\n",
    "8. **Métricas de Avaliação dentro do fluxo de melhoria:** \n",
    "* AUC-ROC: Avaliação da separabilidade entre classes;\n",
    "* Recall: Importante para classificar corretamente a classe minoritária;\n",
    "* F1-Score: Equilíbrio entre precisão e recall ( é calculado mas nao usado para otimizar, nao afetou muito coloca-la a principio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##########  MODELAGEM  ##################')\n",
    "\n",
    "bar = tqdm(total=1, desc=\"Criando modelo\", ncols=100, position=0)# Inicializando a barra de progresso\n",
    "\n",
    "\n",
    "# classe MLP do modelo\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, activation, dropout_rate=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_size = X_train_tensor.shape[1]  \n",
    "        for units in hidden_layer_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, units))\n",
    "            self.layers.append(nn.Dropout(p=dropout_rate))  #dropout\n",
    "            input_size = units\n",
    "        self.output = nn.Linear(input_size, 1)  #Camada de saída\n",
    "        self.activation_fn = self.get_activation_function(activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation_fn(layer(x))  # Ativação após cada camada linear\n",
    "        x = self.output(x)  \n",
    "        return x\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        activation_dict = {\n",
    "            'relu':torch.relu,\n",
    "            'tanh':torch.tanh,\n",
    "            'sigmoid':torch.sigmoid,\n",
    "            'selu':torch.selu,\n",
    "            'gelu':torch.nn.functional.gelu,\n",
    "            'leaky_relu':torch.nn.functional.leaky_relu,\n",
    "            'swish':torch.nn.functional.silu,\n",
    "            'elu':torch.nn.functional.elu\n",
    "        }\n",
    "        return activation_dict.get(activation, torch.relu)  # Default e Relu so pra morrer de certeza que nao va da erro e a relu e a mais comum\n",
    "\n",
    "bar.update(1)  \n",
    "bar.close()\n",
    "\n",
    "\n",
    "\n",
    "#Função que cria o modelo \n",
    "def create_model(\n",
    "    hidden_layer_sizes,\n",
    "    activation,\n",
    "    solver,\n",
    "    alpha,\n",
    "    learning_rate_init,\n",
    "    max_iter,\n",
    "    batch_size,\n",
    "    momentum,\n",
    "    learning_rate_scheduler,\n",
    "    weight_decay,\n",
    "    step_size,\n",
    "    gamma,\n",
    "    early_stopping,\n",
    "    validation_fraction,\n",
    "    dropout_rate,\n",
    "    patience,\n",
    "    imbalance_ratio=1.0  \n",
    "):\n",
    "\n",
    "    # Calcular pesos da classe para aplicacao da funcao de peso para classe minoritaria (BCEWithLogitsLoss)\n",
    "    count_neg = (y_train_tensor == 0).sum().item()  #classe 0\n",
    "    count_pos = (y_train_tensor == 1).sum().item()  #classe 1\n",
    "    #pos_weight = torch.tensor([count_neg / count_pos], device=device)  # Ponderação que favorece MUITO para a classe minoritária\n",
    "    pos_weight = torch.tensor([torch.sqrt(torch.tensor(count_neg / count_pos, dtype=torch.float32))], device=device)# Suavizar o cálculo do pos_weight usando raiz quadrada da razão\n",
    "        \n",
    "    # Instancia o modelo\n",
    "    model = MLP(hidden_layer_sizes, activation, dropout_rate).to(device)\n",
    "    \n",
    "    # Funcao de perda com peso para a classe minoritária\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Configura o otimizador\n",
    "    optimizers = {\n",
    "        'adam': optim.Adam(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay),\n",
    "        'sgd': optim.SGD(model.parameters(), lr=learning_rate_init, momentum=momentum, weight_decay=weight_decay),\n",
    "        'rmsprop': optim.RMSprop(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay),\n",
    "        'nadam': optim.NAdam(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay),\n",
    "        'lbfgs': optim.LBFGS(model.parameters(), lr=learning_rate_init, max_iter=20, history_size=10)\n",
    "    }\n",
    "    optimizer = optimizers.get(solver, optim.Adam(model.parameters(), lr=learning_rate_init, weight_decay=weight_decay))\n",
    "    \n",
    "    # Configura o scheduler\n",
    "    schedulers = {\n",
    "        'StepLR': lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma),\n",
    "        'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iter),\n",
    "        'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=gamma, patience=15, verbose=True),\n",
    "        'CosineAnnealingWarmRestarts': lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2),\n",
    "        'ExponentialLR': lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    }\n",
    "\n",
    "    scheduler = schedulers.get(learning_rate_scheduler, None)\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n",
    "\n",
    "\n",
    "\n",
    "# Função para salvar o melhor modelo ('torch.save')\n",
    "def save_best_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Modelo salvo em {filepath}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################Função objetivo para o Optuna#######################################################################\n",
    "\n",
    "#O Optuna e semelhante ao Gridsearch, costuma ser mais performatico e ate melhor em busca\n",
    "bar = tqdm(total=1, desc=\"Criando hiperparametros Optuna E validação cruzada\", ncols=100, position=0) \n",
    "\n",
    "bar2=None\n",
    "def objective(trial):\n",
    "    #print(\"Iniciando o trial\", trial.number)\n",
    "    global bar2\n",
    "    try:\n",
    "\n",
    "        #Hiperparametros \n",
    "        hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(900, 800), (950, 850), (975, 875), (990, 890), (1000, 900)])\n",
    "        activation = trial.suggest_categorical('activation', ['leaky_relu', 'swish', 'gelu', 'relu', 'elu', 'selu', 'tanh'])\n",
    "        solver = trial.suggest_categorical('solver', ['nadam', 'adam', 'sgd', 'rmsprop', 'lbfgs'])\n",
    "        alpha = trial.suggest_loguniform('alpha', 0.0001, 0.002)  # Ajustado para maior flexibilidade\n",
    "        learning_rate_init = trial.suggest_loguniform('learning_rate_init', 0.0005, 0.2)  # Ajustado para equiparar ao XGBoost\n",
    "        max_iter = trial.suggest_categorical('max_iter', [5000, 6000, 8000, 10000])\n",
    "        batch_size = trial.suggest_categorical('batch_size', [512, 1024])\n",
    "        momentum = trial.suggest_uniform('momentum', 0.7, 0.75)\n",
    "        learning_rate_scheduler = trial.suggest_categorical('learning_rate_scheduler', ['ReduceLROnPlateau'])\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 0.0005, 0.0008)  # Mantido, pois já se assemelha ao XGBoost\n",
    "        step_size = trial.suggest_int('step_size', 30, 40)\n",
    "        gamma = trial.suggest_uniform('gamma', 0.0001, 0.5)  # Equiparado ao XGBoost\n",
    "        early_stopping = trial.suggest_categorical('early_stopping', [True])\n",
    "        validation_fraction = trial.suggest_uniform('validation_fraction', 0.25, 0.3)\n",
    "        dropout_rate = trial.suggest_uniform('dropout_rate', 0.3, 0.4)\n",
    "        patience = trial.suggest_int('patience', 15, 20)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "        # Cria modelo ja inserindo os hiperparametros do Optuna\n",
    "        model, criterion, optimizer, scheduler = create_model(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            max_iter=max_iter,\n",
    "            batch_size=batch_size,\n",
    "            momentum=momentum,\n",
    "            learning_rate_scheduler=learning_rate_scheduler,\n",
    "            weight_decay=weight_decay,\n",
    "            step_size=step_size,\n",
    "            gamma=gamma,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            dropout_rate=dropout_rate,\n",
    "            patience=patience)\n",
    "        \n",
    "        \n",
    "        # Função de treinamento e validação cruzada\n",
    "        def train_and_evaluate_cross_validation(epochs, patience):\n",
    "            skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            auc_scores = []\n",
    "            recall_scores = []\n",
    "            f1_scores = []\n",
    "            precision_scores = []\n",
    "            balanced_acc_scores = []\n",
    "            mcc_scores = []\n",
    "            \n",
    "            device = next(model.parameters()).device  # modelo e dados estão no mesmo dispositivo\n",
    "            model.to(device)\n",
    "\n",
    "            for train_index, val_index in skf.split(X_train_tensor.cpu().numpy(), y_train_tensor.cpu().numpy()):\n",
    "                X_train_cv, X_val_cv = X_train_tensor[train_index].to(device), X_train_tensor[val_index].to(device)\n",
    "                y_train_cv, y_val_cv = y_train_tensor[train_index].to(device), y_train_tensor[val_index].to(device)\n",
    "\n",
    "                X_train_cv_res = torch.tensor(X_train_cv).float().to(device)\n",
    "                y_train_cv_res = torch.tensor(y_train_cv).float().to(device)\n",
    "\n",
    "                train_dataset = TensorDataset(X_train_cv_res, y_train_cv_res)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                # Implementação de early stopping\n",
    "                best_val_loss = float('inf')\n",
    "                patience_counter = 0\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(X_batch).squeeze()\n",
    "                        loss = criterion(output, y_batch)\n",
    "                        loss.backward()\n",
    "\n",
    "                        # Condicional para o solver 'lbfgs'\n",
    "                        if solver == 'lbfgs':\n",
    "                            def closure():\n",
    "                                optimizer.zero_grad()\n",
    "                                loss = criterion(model(X_batch).squeeze(), y_batch)\n",
    "                                loss.backward()\n",
    "                                return loss\n",
    "                            optimizer.step(closure)\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_preds = model(X_val_cv).squeeze()  # Mantendo no dispositivo CUDA\n",
    "                        val_labels = y_val_cv\n",
    "                        val_loss = criterion(val_preds, val_labels).item()\n",
    "                        \n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            patience_counter = 0\n",
    "                        else:\n",
    "                            patience_counter += 1\n",
    "                        if patience_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_logits = model(X_val_cv).squeeze()  \n",
    "                        val_labels = y_val_cv\n",
    "\n",
    "                        # Função sigmoide para obter as probabilidades\n",
    "                        val_preds = torch.sigmoid(val_logits)\n",
    "\n",
    "                        threshold = 0.5\n",
    "                        val_preds_binary = (val_preds > threshold).float()\n",
    "\n",
    "                        # Converter tensores para numpy\n",
    "                        val_preds_cpu = val_preds_binary.cpu().numpy()\n",
    "                        val_labels_cpu = val_labels.cpu().numpy()\n",
    "\n",
    "                        # Calcular métricas\n",
    "                        auc = roc_auc_score(val_labels_cpu, val_preds_cpu)\n",
    "                        recall = recall_score(val_labels_cpu, val_preds_cpu)\n",
    "                        f1 = f1_score(val_labels_cpu, val_preds_cpu)\n",
    "                        precision = precision_score(val_labels_cpu, val_preds_cpu)\n",
    "                        balanced_acc = balanced_accuracy_score(val_labels_cpu, val_preds_cpu)\n",
    "                        mcc = matthews_corrcoef(val_labels_cpu, val_preds_cpu)\n",
    "\n",
    "                        # Guardar métricas\n",
    "                        auc_scores.append(auc)\n",
    "                        recall_scores.append(recall)\n",
    "                        f1_scores.append(f1)\n",
    "                        precision_scores.append(precision)\n",
    "                        balanced_acc_scores.append(balanced_acc)\n",
    "                        mcc_scores.append(mcc)\n",
    "\n",
    "            return (\n",
    "                np.mean(auc_scores), np.mean(recall_scores), np.mean(f1_scores), \n",
    "                np.mean(precision_scores), np.mean(balanced_acc_scores), np.mean(mcc_scores)\n",
    "            )\n",
    "\n",
    "        # Validação cruzada\n",
    "        mean_auc, mean_recall, mean_f1, mean_precision, mean_balanced_acc, mean_mcc = train_and_evaluate_cross_validation(epochs=100, patience=15)\n",
    "\n",
    "        # Ajustando os pesos da métrica composta\n",
    "        score = (0.30 * mean_auc + 0.30 * mean_recall + 0.20 * mean_f1 + \n",
    "         0.10 * mean_precision + 0.05 * mean_balanced_acc + 0.05 * mean_mcc)\n",
    "\n",
    "        # Resultados ao Optuna\n",
    "        trial.report(score, 0)  # step 0\n",
    "        trial.report(mean_recall, 1)  \n",
    "        trial.report(mean_f1, 2)\n",
    "        trial.report(mean_precision, 3)\n",
    "        trial.report(mean_balanced_acc, 4)\n",
    "        trial.report(mean_mcc, 5)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} foi interrompido devido ao pruning.\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Salvando o modelo no trial para ser acessado após a otimização\n",
    "        trial.set_user_attr('model', model)\n",
    "\n",
    "        bar2.update(1)\n",
    "\n",
    "        # Retorna a métrica composta\n",
    "        return score\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        \n",
    "        # Captura o tipo de erro e a mensagem de erro\n",
    "        error_type = type(e).__name__\n",
    "        error_message = str(e)\n",
    "        \n",
    "        # Captura o traceback completo para mais detalhes\n",
    "        error_traceback = traceback.format_exc()\n",
    "\n",
    "        # Exibe o erro completo no console\n",
    "        print(f\"Erro específico encontrado: {error_type} - {error_message}\")\n",
    "        print(\"Traceback completo:\")\n",
    "        print(error_traceback)\n",
    "    \n",
    "        \n",
    "        # Retorna valor negativo infinito para continuar a otimização\n",
    "        return float('-inf')\n",
    "\n",
    "     \n",
    "    # Salvar o modelo após a otimização\n",
    "    torch.save(model.state_dict(), 'best_model.pth') #salva apenas os parametros \n",
    "\n",
    "    \n",
    "bar.update(1)\n",
    "bar.close()  # Fechar a barra\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('########################################Definindo o estudo do Optuna############################################################')\n",
    "\n",
    "#otimização bayesiana baseado em Tree-structured Parzen Estimators (TPE)\n",
    "\n",
    "#O TPESampler é semelhante ao GridSearch, mas em vez de testar exaustivamente todas as combinações possíveis de hiperparâmetros, \n",
    "#ele utiliza uma abordagem probabilística para explorar de forma mais eficiente o espaço de busca e maximizar a função objetivo.\n",
    "\n",
    "\n",
    "#Depois de encontrados lugares/lugar otimo nos hiperparametros e ruduzir o range de estudo deles, aplicar abordagem de maximizacao (exploitation)\n",
    "sampler_ = optuna.samplers.TPESampler(\n",
    "    n_startup_trials=20,      # Define o número de tentativas iniciais aleatórias antes de usar o algoritmo TPE\n",
    "    n_ei_candidates=7,       # Número de candidatos gerados pelo TPE antes de escolher o melhor.\n",
    "    seed=42,                  # semente, Define um número fixo para o gerador de números aleatórios.\n",
    "    multivariate=True,        # Considera correlação entre hiperparâmetros\n",
    "    group=True,               # Melhor para alta dimensionalidade (>10 parâmetros)\n",
    "    consider_magic_clip=False # Mantém como False para não limitar artificialmente\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_trials = 150\n",
    "bar2 = tqdm(total=n_trials, desc=\"Otimização em andamento\", ncols=100, position=0)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=sampler_) \n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "best_model = study.best_trial.user_attrs['model']\n",
    "\n",
    "# Salvando o melhor modelo otimizado\n",
    "save_best_model(best_model, 'best_model.pth')\n",
    "\n",
    "# Resultados\n",
    "best_params = study.best_params\n",
    "print(f\"Melhores parâmetros: {best_params}\")\n",
    "\n",
    "bar2.close()\n",
    "bar.close()\n",
    "\n",
    "print('########################################FIM do estudo do Optuna############################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Treinamento e validacao final:**\n",
    "\n",
    "* epocas fixas, com variaveis ja utilizadas e melhores hiperparametros carregados no melhor modelo salvo; \n",
    "\n",
    "* grafico de perda de treino e validacao ao longo das epocas; \n",
    "\n",
    "* Matriz de confusao;\n",
    "\n",
    "* grafico de metricas de desempenho observdas; \n",
    "\n",
    "* Threshold 0.5 fixo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializando a barra de progresso\n",
    "epochs = 100 #quantidade de epocas\n",
    "bar = tqdm(total=epochs, desc=\"########### Treinamento final ##############\", ncols=100, position=0)\n",
    "\n",
    "X = X[selected_features] # variaveis com importancia determinada pelo randomforest\n",
    "\n",
    "# Divisão dos dados \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalização \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Ajusta no treino\n",
    "X_val = scaler.transform(X_val)   # Ajusta no validacao\n",
    "\n",
    "# Convertendo para tensores \n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "# tensores de treinamento e validação\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Criando os DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Função de treinamento de uma época (modelo ja esta treinado)\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clipping de gradientes para evitar explosão\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step()\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "# Função de validação\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "    return val_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Inicializando o modelo, otimizador e scheduler\n",
    "model, criterion, optimizer, scheduler = create_model(**best_params)\n",
    "model.to(device)  # (GPU ou CPU)\n",
    "\n",
    "\n",
    "\n",
    "# armazenar as perdas em listas\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Loop de treinamento final\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Ajuste do scheduler com a perda de validação, se aplicável\n",
    "    if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Salvando o melhor modelo\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\") #aqui salva o OrderedDict (sopmente os parametros, bom pra usar no mesmo codigo enquanto treina e testa)\n",
    "        torch.save(model, 'best_model_inteiro.pth') #salva o modelo INTEIRO com dict e tudo, bom para aplicar o modelo em producao\n",
    "        torch.save(scaler, 'scaler.pth')  # Salvar scaler usado no treino\n",
    "\n",
    "\n",
    "    bar.update(1)\n",
    "\n",
    "bar.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Carregar o melhor modelo após o treinamento\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "\n",
    "# Plotando as perdas por época\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', color='blue', linestyle='-', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange', linestyle='-', marker='o')\n",
    "plt.title('Training and Validation Loss per Epoch', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "print(\"###########################Treinamento final concluído com sucesso!######################################\")\n",
    "print(f\"Menor perda de validação: {best_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Avaliação final\n",
    "print(\"###########################Avaliando no conjunto de teste!######################################\")\n",
    "# Carregando o melhor modelo\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Avaliando no conjunto de teste\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(X_test_tensor.to(device)).cpu().numpy().squeeze()  # logits\n",
    "    y_pred_prob_test = torch.sigmoid(torch.tensor(y_pred_logits)).cpu().numpy()  # Aplicando a sigmoide para obter probabilidades\n",
    "\n",
    "\n",
    "\n",
    "#threshold \n",
    "# Fixando em 0.5\n",
    "threshold = 0.5\n",
    "y_pred_class_test = (y_pred_prob_test > threshold).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Calculando métricas e exibindo métricas\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_class_test)\n",
    "precision = precision_score(y_test, y_pred_class_test)\n",
    "recall = recall_score(y_test, y_pred_class_test)\n",
    "f1 = f1_score(y_test, y_pred_class_test)\n",
    "print(f\"ROC AUC (Teste): {roc_auc:.4f}\")\n",
    "print(f\"Acurácia (Teste): {accuracy:.4f}\")\n",
    "print(f\"Precisão (Teste): {precision:.4f}\")\n",
    "print(f\"Recall (Teste): {recall:.4f}\")\n",
    "print(f\"F1-Score (Teste): {f1:.4f}\")\n",
    "\n",
    "#matriz de confusao\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_class_test)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Exited', 'Exited'], yticklabels=['Não Exited', 'Exited'])\n",
    "plt.xlabel('Predição')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão no teste')\n",
    "plt.show()\n",
    "\n",
    "# Cálculo da curva ROC para o conjunto de teste\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_prob_test)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "# Gráfico ROC teste\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='red', label=f'ROC Curve - Teste (AUC = {roc_auc_test:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('Curva ROC - Teste')\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Simulacao em producao:**\n",
    "* Aplicado modelo na base inteira (foi o mais proximo de dados novos que pude testar); \n",
    "\n",
    "* Graficos de desempenho e matriz de confusao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('################FAZENDO PREDIÇÕES SIMULANDO UMA APLICAÇÃO EM PRODUÇÃO EM DADOS NOVOS ##############################')\n",
    "\n",
    "# Carregar o modelo treinado inteiro e em modo de avaliação\n",
    "model = torch.load('best_model_inteiro.pth')\n",
    "model.eval()\n",
    "\n",
    "# Carregar o scaler \n",
    "scaler = torch.load('scaler.pth')\n",
    "\n",
    "# Carregar os novos dados \n",
    "df_new = pd.concat([base_corrige_teste,teste_multco_treino], axis = 0)\n",
    "\n",
    "# Selecionar fetures (geradas pelo randonforest)\n",
    "#df_new = df_new.head(5000) # para testar em apenas 5000 observações\n",
    "X_new = df_new[selected_features]  \n",
    "\n",
    "# Separar as labels (y) e features (X)\n",
    "y_true = df_new['Exited'].values  \n",
    "\n",
    "# Normalizar\n",
    "X_scaled = scaler.transform(X_new)\n",
    "\n",
    "# tensor \n",
    "input_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "# (GPU ou CPU) \n",
    "input_tensor = input_tensor.to(device)  \n",
    "\n",
    "# Obter predições do modelo\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(input_tensor)  # Logits \n",
    "    y_pred_prob = torch.sigmoid(y_pred_logits)  # sigmoide para obter probabilidades\n",
    "\n",
    "# Converter para numpy (aqui nao da pra usar GPU, nao tem jeito)\n",
    "y_pred_logits = y_pred_logits.cpu().numpy()\n",
    "y_pred_prob = y_pred_prob.cpu().numpy()\n",
    "\n",
    "# Converter probabilidades em classes (0 ou 1)\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calcular  e visualizar matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_class)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão - Dados de simulacao de Producao')\n",
    "plt.show()\n",
    "\n",
    "# Calcular e plotar curva ROC AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Linha de referência\n",
    "plt.xlabel('Falso Positivo Rate')\n",
    "plt.ylabel('Verdadeiro Positivo Rate')\n",
    "plt.title('Curva ROC - Dados de simulacao de Producao')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calcular a acurácia \n",
    "accuracy_positive = accuracy_score(y_true, y_pred_class)\n",
    "print(f\"Acurácia total - Dados de simulacao de Producao: {accuracy_positive * 100:.2f}%\")\n",
    "\n",
    "# Calcular o recall \n",
    "positive_class_recall = recall_score(y_true, y_pred_class)\n",
    "print(f\"Recall da classe positiva - Dados de simulacao de Producao: {positive_class_recall * 100:.2f}%\")\n",
    "\n",
    "# Métricas de avaliação de classificação\n",
    "metrics = {\n",
    "    \"Acurácia\": accuracy_score,\n",
    "    \"Precisão\": precision_score,\n",
    "    \"Recall\": recall_score,\n",
    "    \"F1-Score\": f1_score,\n",
    "    \"AUC-ROC\": roc_auc_score,\n",
    "    \"MCC\": matthews_corrcoef,\n",
    "    \"Acurácia Balanceada\": balanced_accuracy_score\n",
    "}\n",
    "\n",
    "metric_results = {}\n",
    "\n",
    "for metric_name, metric_function in metrics.items():\n",
    "    if metric_name == \"AUC-ROC\":\n",
    "        score = metric_function(y_true, y_pred_prob)\n",
    "    else:\n",
    "        score = metric_function(y_true, y_pred_class)\n",
    "    metric_results[metric_name] = score\n",
    "\n",
    "# Printar métricas \n",
    "print(\"📊 MÉTRICAS DO MODELO - SIMULA PRODUCAO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Métrica':<20}{'Valor final':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for metric_name, score in metric_results.items():\n",
    "    print(f\"{metric_name:<20}{score:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fim\n",
    "end_time_utc = datetime.utcnow() - timedelta(hours=3)\n",
    "print('------------------------')\n",
    "print(\"Início:\", start_time_utc)\n",
    "print(\"Fim:   \", end_time_utc)\n",
    "print('------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusao:\n",
    "\n",
    "**Modelo treinado e testado com bons resultados preditivos, apesar de muitos falsos positivos, que podem ser um problema dependendo da ação a ser tomada com os resultados do modelo. Em cenários onde as ações para evitar o churn sejam caras, ou em cenários de comunicação onde há risco de importunação ao cliente ou algum tipo de aborrecimento que tenha o efeito contrário ao de evitar o churn, esses falsos positivos serão preocupantes. Há a possibilidade de alterar as métricas de maximização para aumentar esse equilíbrio entre verdadeiros positivos e falsos positivos. Ainda assim, o mais provável é que ainda se necessite da adição de variáveis que expliquem melhor o evento de churn e mais observações para melhorar a capacidade preditiva. Anteriormente a esse modelo, foram testados modelos de árvore, como CatBoost, XGBoost e regressão logística (nao e arvore); os resultados foram bem próximos, o que corrobora que sejam necessárias novas variáveis e observações que possam, de fato, melhorar o estudo nessa base de dados.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE END** "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3197960,
     "sourceId": 5550559,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
