{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransactionSentinel: Proteção inteligente contra fraudes em transações de cartão de crédito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Este notebook apresenta a construção de um estudo e um modelo de ML para detecção de fraudes em dados financeiros fictícios, seguindo a metodologia CRISP-DM.\n",
    "\n",
    "* A abordagem será estruturada em cinco das seis etapas da metodologia. A etapa de Deploy (Implantação) não será totalmente executada; no entanto, o modelo será salvo como se estivesse pronto para produção.\n",
    "    * Etapas: \n",
    "\n",
    "        * **Compreensão do Negócio** – Definição do problema e dos objetivos do projeto.\n",
    "\n",
    "        * **Compreensão dos Dados** – Exploração inicial para entender a estrutura e qualidade dos dados.\n",
    "\n",
    "        * **Preparação dos Dados** – Tratamento, limpeza e transformação dos dados para a modelagem.\n",
    "\n",
    "        * **Modelagem** – Aplicação de algoritmos de machine learning para detectar padrões de fraude.\n",
    "\n",
    "        * **Avaliação** – Medição do desempenho do modelo para garantir sua eficácia.\n",
    "\n",
    "        * **Deploy** (Implantação) – Integração do modelo em um ambiente operacional para uso real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Compreensão do Negócio\n",
    "\n",
    "* A detecção de fraudes em transações financeiras é um desafio essencial para instituições bancárias e operadoras de cartões de crédito. A identificação eficiente de fraudes reduz perdas financeiras e protege clientes contra atividades fraudulentas.\n",
    "\n",
    "* Objetivo do Projeto:\n",
    "\n",
    "    * 1- Desenvolver uma análise exploratória que forneça informações sobre o comportamento dos eventos fraudulentos e não fraudulentos. \n",
    "\n",
    "    * 2-Desenvolver um modelo de Machine Learning capaz de identificar transações fraudulentas com alto desempenho, garantindo um equilíbrio entre segurança e experiência do usuário.\n",
    "\n",
    "\n",
    "\n",
    "        * O modelo será avaliado com as seguintes métricas:\n",
    "\n",
    "        * **Recall ≥ 70% ** – Para minimizar a quantidade de fraudes não detectadas.\n",
    "\n",
    "        * ** AUC-ROC ≥ 85% ** – Para garantir uma boa distinção entre transações legítimas e fraudulentas.\n",
    "\n",
    "        * ** F1-score ≥ 74% ** – Para garantir um bom equilíbrio entre precisão e recall, considerando a importância de minimizar tanto os falsos positivos quanto os falsos negativos.\n",
    "\n",
    "* Restrições de Negócio\n",
    "\n",
    "    Para atender a requisitos e garantir um modelo confiável:\n",
    "\n",
    "    * Imparcialidade e Prevenção de Discriminação: \n",
    "    \n",
    "        O modelo não deve apresentar tendências discriminatórias baseadas em atributos como gênero, idade, localização ou outros fatores socioeconômicos. Se esse tipo de varivel for interessante ao evento em estudo, deve se aplicar transformacoes nessas variaveis para que seja mitigada chance de vies descriminativo, ex: idade pode ser transformada em faixas etarias, localizacao pode se tornar distancia entre residencia do titular e local do estabelecimento (verificando distancia de tempo entre duas ou mais transacoes) para identificar algum padrao nas fraudes. \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    * Explicabilidade e Transparência\n",
    "\n",
    "        O modelo deve ser interpretável tanto globalmente quanto localmente, garantindo que especialistas possam entender seus critérios de decisão. Técnicas como SHAP (SHapley Additive Explanations) e LIME (Local Interpretable Model-agnostic Explanations) serão aplicadas para fornecer insights sobre as previsões do modelo.\n",
    "\n",
    "* Escopo da Implantação\n",
    "    * O modelo final será salvo para futuras implementações, mas a fase de Deploy não será completamente executada neste estudo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Compreensão dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import warnings\n",
    "from scipy.stats import chi2_contingency\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importando e conhecendo os dados inicialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############para Kaggle\n",
    "\n",
    "# Load the training dataset\n",
    "#train_data = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv') \n",
    "\n",
    "# Load the testing dataset\n",
    "#test_data = pd.read_csv('/kaggle/input/fraud-detection/fraudTest.csv')\n",
    "\n",
    "# Display the first few rows of both datasets\n",
    "#print(\"First 5 rows of the training dataset:\")\n",
    "#print(train_data.head())\n",
    "\n",
    "#print(\"\\nFirst 5 rows of the testing dataset:\")\n",
    "#print(test_data.head())\n",
    "\n",
    "\n",
    "# Carregando arquivos em parquet\n",
    "df_orig_train = pd.read_parquet('C:/Users/jgeov/OneDrive/Documentos/GitHub/Ciencia_de_dados-1/Fraud_detection/fraudTrain.parquet')\n",
    "df_orig_test = pd.read_parquet('C:/Users/jgeov/OneDrive/Documentos/GitHub/Ciencia_de_dados-1/Fraud_detection/fraudTest.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_treino dimesoes: (1296675, 23)\n",
      "df_teste dimesoes: (555719, 23)\n"
     ]
    }
   ],
   "source": [
    "print('df_treino dimesoes:',df_orig_train.shape) #batem com o original, sem perdas de dados\n",
    "\n",
    "print('df_teste dimesoes:',df_orig_test.shape)  #batem com o original, sem perdas de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>28654</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>99160</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>83252</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num                         merchant       category     amt      first     last gender                        street            city state    zip      lat      long  city_pop                                job         dob                         trans_num   unix_time  merch_lat  merch_long  is_fraud\n",
       "0           0   2019-01-01 00:00:18  2703186189652095       fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer    Banks      F                561 Perry Cove  Moravian Falls    NC  28654  36.0788  -81.1781      3495          Psychologist, counselling  1988-03-09  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315         0\n",
       "1           1   2019-01-01 00:00:44      630423337322  fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie     Gill      F  43039 Riley Greens Suite 393          Orient    WA  99160  48.8878 -118.2105       149  Special educational needs teacher  1978-06-21  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462         0\n",
       "2           2   2019-01-01 00:00:51    38859492057661             fraud_Lind-Buckridge  entertainment  220.11     Edward  Sanchez      M      594 White Dale Suite 530      Malad City    ID  83252  42.1808 -112.2620      4154        Nature conservation officer  1962-01-19  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se você quiser combiná-los (por exemplo, por concatenação)\n",
    "df_total = pd.concat([df_orig_train, df_orig_test], ignore_index=True)\n",
    "\n",
    "# Exibindo as primeiras linhas do DataFrame combinado\n",
    "\n",
    "#configs para nao quebrar linhas no print do  df\n",
    "pd.set_option('display.expand_frame_repr', False) \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "df_total.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1296675</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-21 12:14:25</td>\n",
       "      <td>2291163933867244</td>\n",
       "      <td>fraud_Kirlin and Sons</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>M</td>\n",
       "      <td>351 Darlene Green</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>SC</td>\n",
       "      <td>29209</td>\n",
       "      <td>33.9659</td>\n",
       "      <td>-80.9355</td>\n",
       "      <td>333497</td>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>1968-03-19</td>\n",
       "      <td>2da90c7d74bd46a0caf3777415b3ebd3</td>\n",
       "      <td>1371816865</td>\n",
       "      <td>33.986391</td>\n",
       "      <td>-81.200714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>28654</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296676</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-21 12:14:33</td>\n",
       "      <td>3573030041201292</td>\n",
       "      <td>fraud_Sporer-Keebler</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>29.84</td>\n",
       "      <td>Joanne</td>\n",
       "      <td>Williams</td>\n",
       "      <td>F</td>\n",
       "      <td>3638 Marsh Union</td>\n",
       "      <td>Altonah</td>\n",
       "      <td>UT</td>\n",
       "      <td>84002</td>\n",
       "      <td>40.3207</td>\n",
       "      <td>-110.4360</td>\n",
       "      <td>302</td>\n",
       "      <td>Sales professional, IT</td>\n",
       "      <td>1990-01-17</td>\n",
       "      <td>324cc204407e99f51b0d6ca0055005e7</td>\n",
       "      <td>1371816873</td>\n",
       "      <td>39.450498</td>\n",
       "      <td>-109.960431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>99160</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>83252</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0 trans_date_trans_time            cc_num                         merchant       category     amt      first      last gender                        street            city state    zip      lat      long  city_pop                                job         dob                         trans_num   unix_time  merch_lat  merch_long  is_fraud\n",
       "1296675           0   2020-06-21 12:14:25  2291163933867244            fraud_Kirlin and Sons  personal_care    2.86       Jeff   Elliott      M             351 Darlene Green        Columbia    SC  29209  33.9659  -80.9355    333497                Mechanical engineer  1968-03-19  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714         0\n",
       "0                 0   2019-01-01 00:00:18  2703186189652095       fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer     Banks      F                561 Perry Cove  Moravian Falls    NC  28654  36.0788  -81.1781      3495          Psychologist, counselling  1988-03-09  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315         0\n",
       "1296676           1   2020-06-21 12:14:33  3573030041201292             fraud_Sporer-Keebler  personal_care   29.84     Joanne  Williams      F              3638 Marsh Union         Altonah    UT  84002  40.3207 -110.4360       302             Sales professional, IT  1990-01-17  324cc204407e99f51b0d6ca0055005e7  1371816873  39.450498 -109.960431         0\n",
       "1                 1   2019-01-01 00:00:44      630423337322  fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie      Gill      F  43039 Riley Greens Suite 393          Orient    WA  99160  48.8878 -118.2105       149  Special educational needs teacher  1978-06-21  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462         0\n",
       "2                 2   2019-01-01 00:00:51    38859492057661             fraud_Lind-Buckridge  entertainment  220.11     Edward   Sanchez      M      594 White Dale Suite 530      Malad City    ID  83252  42.1808 -112.2620      4154        Nature conservation officer  1962-01-19  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A coluna \"Unnamed: 0\" representa apenas a contagem dos datasets de treino e teste. Ao concatená-los para a compreensão dos dados, essa coluna foi duplicada.\n",
    "#Como se trata apenas de um índice sem valor informativo para a análise, e não será utilizada em nenhuma etapa do estudo, ela será removida já nesta fase.\n",
    "df_total.sort_values(by=\"Unnamed: 0\", ascending=True).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>28654</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>99160</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>83252</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>MT</td>\n",
       "      <td>59632</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>VA</td>\n",
       "      <td>24433</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date_trans_time            cc_num                            merchant       category     amt      first     last gender                        street            city state    zip      lat      long  city_pop                                job         dob                         trans_num   unix_time  merch_lat  merch_long  is_fraud\n",
       "0   2019-01-01 00:00:18  2703186189652095          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer    Banks      F                561 Perry Cove  Moravian Falls    NC  28654  36.0788  -81.1781      3495          Psychologist, counselling  1988-03-09  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315         0\n",
       "1   2019-01-01 00:00:44      630423337322     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie     Gill      F  43039 Riley Greens Suite 393          Orient    WA  99160  48.8878 -118.2105       149  Special educational needs teacher  1978-06-21  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462         0\n",
       "2   2019-01-01 00:00:51    38859492057661                fraud_Lind-Buckridge  entertainment  220.11     Edward  Sanchez      M      594 White Dale Suite 530      Malad City    ID  83252  42.1808 -112.2620      4154        Nature conservation officer  1962-01-19  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481         0\n",
       "3   2019-01-01 00:01:16  3534093764340240  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy    White      M   9443 Cynthia Court Apt. 038         Boulder    MT  59632  46.2306 -112.1138      1939                    Patent attorney  1967-01-12  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071         0\n",
       "4   2019-01-01 00:03:06   375534208663984                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   Garcia      M              408 Bradley Rest        Doe Hill    VA  24433  38.4207  -79.4629        99     Dance movement psychotherapist  1986-03-28  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removendo \n",
    "df_total = df_total.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "#resetando indice (morrer de certeza, depois da concatenacao pode ter duplicado tambem em algum ponto)\n",
    "df_total = df_total.reset_index(drop=True)\n",
    "\n",
    "#ordenando os dados pelo indice\n",
    "df_total = df_total.sort_index(ascending=True)\n",
    "\n",
    "\n",
    "df_total.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_total dimesoes: (1852394, 22)\n"
     ]
    }
   ],
   "source": [
    "print('df_total dimesoes:',df_total.shape) #comparacao com soma dos dfs de treino e teste (soma bate) ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt',\n",
      "       'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat',\n",
      "       'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat',\n",
      "       'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "\n",
      "trans_date_trans_time     object\n",
      "cc_num                     int64\n",
      "merchant                  object\n",
      "category                  object\n",
      "amt                      float64\n",
      "first                     object\n",
      "last                      object\n",
      "gender                    object\n",
      "street                    object\n",
      "city                      object\n",
      "state                     object\n",
      "zip                        int64\n",
      "lat                      float64\n",
      "long                     float64\n",
      "city_pop                   int64\n",
      "job                       object\n",
      "dob                       object\n",
      "trans_num                 object\n",
      "unix_time                  int64\n",
      "merch_lat                float64\n",
      "merch_long               float64\n",
      "is_fraud                   int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#cnhecendo as colunas e tipos de dados\n",
    "print(df_total.columns)\n",
    "print(\"\")\n",
    "print(df_total.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trans_date_trans_time    0\n",
       "cc_num                   0\n",
       "merchant                 0\n",
       "category                 0\n",
       "amt                      0\n",
       "first                    0\n",
       "last                     0\n",
       "gender                   0\n",
       "street                   0\n",
       "city                     0\n",
       "state                    0\n",
       "zip                      0\n",
       "lat                      0\n",
       "long                     0\n",
       "city_pop                 0\n",
       "job                      0\n",
       "dob                      0\n",
       "trans_num                0\n",
       "unix_time                0\n",
       "merch_lat                0\n",
       "merch_long               0\n",
       "is_fraud                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checando os valores null em cada variavel \n",
    "\n",
    "#checando se há valores nulos \n",
    "df_total.isnull().sum()  \n",
    "#valores nulos nao encontrados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 'trans_date_trans_time': 0 valores zero\n",
      "\n",
      " 'cc_num': 0 valores zero\n",
      "\n",
      " 'merchant': 0 valores zero\n",
      "\n",
      " 'category': 0 valores zero\n",
      "\n",
      " 'amt': 0 valores zero\n",
      "\n",
      " 'first': 0 valores zero\n",
      "\n",
      " 'last': 0 valores zero\n",
      "\n",
      " 'gender': 0 valores zero\n",
      "\n",
      " 'street': 0 valores zero\n",
      "\n",
      " 'city': 0 valores zero\n",
      "\n",
      " 'state': 0 valores zero\n",
      "\n",
      " 'zip': 0 valores zero\n",
      "\n",
      " 'lat': 0 valores zero\n",
      "\n",
      " 'long': 0 valores zero\n",
      "\n",
      " 'city_pop': 0 valores zero\n",
      "\n",
      " 'job': 0 valores zero\n",
      "\n",
      " 'dob': 0 valores zero\n",
      "\n",
      " 'trans_num': 0 valores zero\n",
      "\n",
      " 'unix_time': 0 valores zero\n",
      "\n",
      " 'merch_lat': 0 valores zero\n",
      "\n",
      " 'merch_long': 0 valores zero\n",
      "\n",
      " 'is_fraud': 1842743 valores zero\n"
     ]
    }
   ],
   "source": [
    "#contando a quantidade de zeros em cada coluna para verificar se elas tem \n",
    "# informacao suficiente para entrar no modelo futuramente\n",
    "\n",
    "for col in df_total.columns:\n",
    "    zero_count = (df_total[col] == 0).sum()\n",
    "    print(\"\")\n",
    "    print(f\" '{col}': {zero_count} valores zero\")\n",
    "\n",
    "    #nenhuma variavel contem valores zerados, a nao ser a variavel alvo,\n",
    "    #  e aqui ja podemos ver que se trata de um estudo de enventos raros realmente. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Analisando e Descrevendo: Análise Exploratória (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dicionario de dados e acoes previamente ja determinadas de acordo com a natureza da varivel. \n",
    "* O oficial nao foi divulgado, entao com base no nome das variaveis foi determinado: \n",
    "\n",
    "\n",
    "\n",
    "| **Nome da Variável**        | **Descrição**                                                                 | **Transformação Necessária** |\n",
    "|-----------------------------|-------------------------------------------------------------------------------|-----------------------------|\n",
    "| **trans_date_trans_time**    | Data e hora da transação (`yyyy-mm-dd hh:mm:ss`).                            | Extrair hora, dia da semana, mês, periodo da transacao etc.|\n",
    "| **cc_num** | Número do cartão de crédito utilizado na transação.<br> Pode ser útil para identificar padrões de uso suspeitos e anomalias.<br> Contudo, é um dado sensível e deve ser tratado para garantir conformidade com normas de privacidade. |  Extrair padrões relevantes, como:<br>  - Extrair primeiros dígitos (BIN) que identificam o banco das transações <br>  - Contagem de transações por cartão em um período de tempo. <br> - Contagem de vezes que o cartao foi usado|\n",
    "| **merchant**                 | Nome do comerciante.                                                          |  |\n",
    "| **category**                 | Categoria da transação (`misc_net`, `grocery_pos`, etc.).                     |  |\n",
    "| **amt**                      | Valor da transação.                                                           |  |\n",
    "| **first**                    | Primeiro nome do titular.                                                     | Remover (Irrelevante). |\n",
    "| **last**                     | Sobrenome do titular.                                                         | Remover (Irrelevante). |\n",
    "| **gender**                   | Gênero do titular (`F` ou `M`).                                               | Remover (Possível viés discriminatório).|\n",
    "| **street**                   | Endereço do titular.                                                          | Remover (Irrelevante). |\n",
    "| **city**                     | Cidade do titular.                                                            | Remover (Já há `lat` e `long`). |\n",
    "| **state**                    | Estado do titular.                                                            | Remover (Já há `lat` e `long`). |\n",
    "| **zip**                      | Código postal (CEP).                                                          | Remover (Já há `lat` e `long`). |\n",
    "| **lat**                      | Latitude da localização do titular.                                           |  |\n",
    "| **long**                     | Longitude da localização do titular.                                          |  |\n",
    "| **city_pop**                 | População da cidade do titular.                                               |  |\n",
    "| **job**                      | Profissão do titular.                                                         |  |\n",
    "| **dob**                      | Data de nascimento (`yyyy-mm-dd`).                                            | Converter para idade. |\n",
    "| **trans_num**                | Identificador único da transação.                                             | Remover (Irrelevante). |\n",
    "| **unix_time**                | Timestamp Unix (segundos desde 1970).                                         | Remover - reduntande ja temos trans_date_trans_time  |\n",
    "| **merch_lat**                | Latitude da localização do comerciante.                                       |  |\n",
    "| **merch_long**               | Longitude da localização do comerciante.                                      |  |\n",
    "| **is_fraud**                 | Indicador de fraude (`1` = fraudulenta, `0` = legítima).                      | **Variável alvo** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_total.dtypes)\n",
    "\n",
    "df_anl_num = df_total[['amt','city_pop','is_fraud']] #df so de numericas elegiveis para analise (APENAS ANALISE)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format  # config 2 casas decimais para configurar o describe\n",
    "df_anl_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificando relacao entre a variavel alvo e as variaveis explicativas numericas\n",
    "    * sem muita correlacao inicialmente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suprimir todos os warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Criar a figura com 1 subgráfico (apenas o gráfico de correlação)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# 1. Gráfico de Correlação (Matriz de Correlação)\n",
    "sns.heatmap(df_anl_num.corr(), annot=True, cmap=\"viridis\", fmt=\".2f\", ax=axes, vmin=-1, vmax=1)\n",
    "axes.set_title('Matriz de Correlação')\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificando relacao entre a variavel alvo e as variaveis explicativas categoricas (\"frequencias\")\n",
    "    * Aqui ja podemos ver como se trata de um evento raro; \n",
    "    * as cateogrias de compras (variavel category) com \"_net\" sao de transacoes de compra online, as \"_pos\" sao trasacoes de compra presenciais. Elas detem a maioria das fraudes dentre as categorias, o que é naturalmente compreensivel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anl_cat = df_total[['category','gender','state','is_fraud']] #df so de categoricas elegiveis para analise (APENAS ANALISE)\n",
    "\n",
    "\n",
    "# Criar a figura com 3 subgráficos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Gráfico de contagem para a variável 'category'\n",
    "sns.countplot(data=df_anl_cat, x='category', hue='is_fraud', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Contagem de Fraude por Categoria')\n",
    "\n",
    "# 2. Gráfico de contagem para a variável 'gender'\n",
    "sns.countplot(data=df_anl_cat, x='gender', hue='is_fraud', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Contagem de Fraude por Gênero')\n",
    "\n",
    "# 3. Gráfico de contagem para a variável 'state'\n",
    "sns.countplot(data=df_anl_cat, x='state', hue='is_fraud', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Contagem de Fraude por Estado')\n",
    "\n",
    "# Aplicar rotação de 45 graus em todos os rótulos do eixo x\n",
    "for ax in axes.flat:\n",
    "    ax.tick_params(axis='x', rotation=45)  # Rotaciona os rótulos do eixo x para 45 graus\n",
    "\n",
    "# Ajustar layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificando a relacao entre as variaveis numericas explicativas (sem variavel alvo)\n",
    "    * Vemos um agrupamento em valores pequenos para ambas variaveis;\n",
    "    * embora a variavel alvo esteja na legenda, é meramente para vermos onde se encontrar as observacoes de fraude entre as variaveis, nao e muito conclusivo mas da uma ideia de onde se \"agrupam\" em termos de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o pairplot\n",
    "g = sns.pairplot(\n",
    "    df_anl_num, \n",
    "    hue='is_fraud',\n",
    "    diag_kind='kde',\n",
    "    height=2.5,\n",
    "    aspect=1.2,\n",
    "    plot_kws={'alpha': 0.3}\n",
    ")\n",
    "\n",
    "# Formata os eixos \n",
    "for ax in g.axes.flatten():\n",
    "    # Formatação(sem notação científica automatica)\n",
    "    ax.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "    \n",
    "    ax.tick_params(axis='x', rotation=45) # eixo x em 45 graus\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificando a relacao entre as variaveis explicativas categoricas (sem variavel alvo)\n",
    "    * Aqui verificamos se ha associacao estatistica entre as variveis cetegoricas (sem a alvo) pelo teste de Qui2\n",
    "    * Verificamos e classificamos em baixa media e alta a forca das associaicoes entre as variaveis categoricas, e plotamos os resultados pelo heatmap de V de Cramer tambem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ de categoricas sem a variavel alvo\n",
    "df_cat = df_anl_cat[['category', 'gender', 'state']]\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Calcula o V de Cramer entre duas variáveis categóricas.\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))\n",
    "    rcorr = r - ((r-1)**2) / (n-1)\n",
    "    kcorr = k - ((k-1)**2) / (n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "# Função para categorizar a força do V de Cramer\n",
    "def categorize_cramers_v(value):\n",
    "    if value < 0.10:\n",
    "        return \"Baixa Força de\"\n",
    "    elif value < 0.30:\n",
    "        return \"Média Força de\"\n",
    "    else:\n",
    "        return \"Alta Força de\"\n",
    "\n",
    "# Inicializa um DataFrame para armazenar os resultados\n",
    "results = pd.DataFrame(index=df_cat.columns, columns=df_cat.columns)\n",
    "\n",
    "for col1 in df_cat.columns:\n",
    "    for col2 in df_cat.columns:\n",
    "        if col1 == col2:\n",
    "            results.loc[col1, col2] = 1.0  # Correlação perfeita com ela mesma\n",
    "        else:\n",
    "            table = pd.crosstab(df_cat[col1], df_cat[col2])\n",
    "            chi2, p, _, _ = chi2_contingency(table)\n",
    "            v_cramer = cramers_v(df_cat[col1], df_cat[col2])\n",
    "\n",
    "            # Interpretação do p-valor\n",
    "            significance = \"há evidência de associação\" if p < 0.05 else \"não há evidência de associação\"\n",
    "            \n",
    "            # Classificação da força do V de Cramer\n",
    "            strength = categorize_cramers_v(v_cramer)\n",
    "\n",
    "            print(f'Teste Qui-Quadrado entre {col1} e {col2}:')\n",
    "            print(f'Qui²={chi2:.2f}, p-valor={p:.4f} ({\"menor\" if p < 0.05 else \"maior\"} que 0.05, {significance}).')\n",
    "            print(f'V de Cramer={v_cramer:.2f} ({strength} associação)\\n')\n",
    "\n",
    "            results.loc[col1, col2] = v_cramer\n",
    "\n",
    "\n",
    "# Converte os valores para float\n",
    "results = results.astype(float)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(results, annot=True, cmap='viridis', fmt='.2f', vmin=0, vmax=1)\n",
    "plt.title('Heatmap do V de Cramer entre Variáveis Categóricas')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aqui evidencia-se o desbalance das classes da variavel alvo, trata-se de um evento raro, conforme ja haviam indicios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a quantidade de cada classe\n",
    "fraud_counts = df_total[\"is_fraud\"].value_counts()\n",
    "\n",
    "# Criar o gráfico de pizza\n",
    "plt.figure(figsize=(6, 6))\n",
    "wedges, texts = plt.pie(\n",
    "    fraud_counts, labels=[\"Não Fraude\", \"Fraude\"], \n",
    "    colors=[\"green\", \"red\"], startangle=90, wedgeprops={\"edgecolor\": \"black\"}\n",
    ")\n",
    "\n",
    "# Adicionar os percentuais como rótulos ao lado das fatias\n",
    "for text, pct in zip(texts, fraud_counts / fraud_counts.sum() * 100):\n",
    "    text.set_text(f\"{text.get_text()} ({pct:.2f}%)\")\n",
    "\n",
    "# Adicionar título\n",
    "plt.title(\"Distribuição da Variável-Alvo (is_fraud)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificando localizacao das transacoes e dos titulares dos cartoes \n",
    "    * Aqui foi dado foco nos maiores ofensores de fraudes: as variaveis de compra presencial \"_pos\" e online \"_net\" pois apresentaram grande parte das fraudes \n",
    "    * Ha maior concentracao de transacoes do lado da Costa Leste \n",
    "    * Existem transacoes no Havai, Canadá e no Alasca, mas sao em pequenos volumes, ainda assim sao estranhas(principalemnte Alasca). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas transações fraudulentas E que sejam online (_net)\n",
    "df_fraude_net = df_total[(df_total[\"is_fraud\"] == 1) & (df_total[\"category\"].str.contains(\"_net\", na=False))]\n",
    "\n",
    "# Criar um DataFrame com as coordenadas SOMENTE de fraudes online\n",
    "df_mapa = pd.DataFrame({\n",
    "    \"Latitude\": list(df_fraude_net[\"lat\"]) + list(df_fraude_net[\"merch_lat\"]),\n",
    "    \"Longitude\": list(df_fraude_net[\"long\"]) + list(df_fraude_net[\"merch_long\"]),\n",
    "    \"Tipo\": [\"Titular\"] * len(df_fraude_net) + [\"Estabelecimento\"] * len(df_fraude_net)\n",
    "})\n",
    "\n",
    "# Amostrar 50% para evitar sobrecarga (ajuste conforme volume de dados)\n",
    "df_mapa_sample = df_mapa.sample(frac=0.5, random_state=42) if len(df_mapa) > 1000 else df_mapa\n",
    "\n",
    "# Criar o mapa com os pontos das fraudes online\n",
    "fig = px.scatter_mapbox(df_mapa_sample, lat=\"Latitude\", lon=\"Longitude\",\n",
    "                        color=\"Tipo\",  \n",
    "                        mapbox_style=\"carto-positron\",\n",
    "                        zoom=3, \n",
    "                        color_discrete_map={\"Titular\": \"orange\", \"Estabelecimento\": \"blue\"}  # Define cores personalizadas\n",
    "                        )\n",
    "\n",
    "# Ajustar layout com margem superior maior para exibir o título\n",
    "fig.update_layout(\n",
    "    width=1700,  \n",
    "    height=700,  \n",
    "    margin={\"r\":0, \"t\":50, \"b\":0, \"l\":0},\n",
    "    title={\n",
    "        \"text\": \"AMOSTRA de Distribuição das Transações Fraudulentas ONLINE por Localização do Titular e Estabelecimento\",\n",
    "        \"x\": 0.5,  # Centraliza o título\n",
    "        \"xanchor\": \"center\",  # Garante alinhamento centralizado\n",
    "        \"yanchor\": \"top\",\n",
    "        \"font\": {\"size\": 20, \"family\": \"Arial Black\"}  # Aumenta o tamanho e deixa em negrito\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar apenas transações fraudulentas E que sejam presenciais (_pos)\n",
    "df_fraude_net = df_total[(df_total[\"is_fraud\"] == 1) & (df_total[\"category\"].str.contains(\"_pos\", na=False))]\n",
    "\n",
    "# Criar um DataFrame com as coordenadas SOMENTE de fraudes presenciais\n",
    "df_mapa = pd.DataFrame({\n",
    "    \"Latitude\": list(df_fraude_net[\"lat\"]) + list(df_fraude_net[\"merch_lat\"]),\n",
    "    \"Longitude\": list(df_fraude_net[\"long\"]) + list(df_fraude_net[\"merch_long\"]),\n",
    "    \"Tipo\": [\"Titular\"] * len(df_fraude_net) + [\"Estabelecimento\"] * len(df_fraude_net)\n",
    "})\n",
    "\n",
    "# Amostrar 50% para evitar sobrecarga (ajuste conforme volume de dados)\n",
    "df_mapa_sample = df_mapa.sample(frac=0.5, random_state=42) if len(df_mapa) > 1000 else df_mapa\n",
    "\n",
    "# Criar o mapa com os pontos das fraudes presenciais, definindo cores específicas\n",
    "fig = px.scatter_mapbox(df_mapa_sample, lat=\"Latitude\", lon=\"Longitude\",\n",
    "                        color=\"Tipo\",  \n",
    "                        mapbox_style=\"carto-positron\",\n",
    "                        zoom=3,\n",
    "                        color_discrete_map={\"Titular\": \"orange\", \"Estabelecimento\": \"blue\"}  # Define cores personalizadas\n",
    ")\n",
    "\n",
    "# Ajustar layout\n",
    "fig.update_layout(\n",
    "    width=1700,  \n",
    "    height=700,  \n",
    "    margin={\"r\":0, \"t\":50, \"b\":0, \"l\":0},\n",
    "    title={\n",
    "        \"text\": \"AMOSTRA de Distribuição das Transações Fraudulentas PRESENCIAIS por Localização do Titular e Estabelecimento\",\n",
    "        \"x\": 0.5,  \n",
    "        \"xanchor\": \"center\",  \n",
    "        \"yanchor\": \"top\",\n",
    "        \"font\": {\"size\": 20, \"family\": \"Arial Black\"}  \n",
    "    }\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Preparação dos Dados: Feature Engineering\n",
    "\n",
    "\n",
    "* estudar a criacao de uma variavel que identifica a distancia (se e anormal) entre duas transacoes, acho que usar o racional da variavel de contagem de vezes que o cartao foi usado na ultima hora (trans_count_last_hour) \n",
    "ex: uma transacao feita presencialemnte seguida de outra presencialmente em locais muito distantes em 1 hora (ou outro periodo se for o caso) podem indicar uma possivel fraude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* criando variavel de distancia em Km entre estabeleciemnto e titular do cartao para compras presencias. \n",
    "    * Foi usada a distância entre os pontos pela fórmula de Haversine, pois ela considera a curvatura da Terra e retorna a distância real em km, diferentemente da Euclidiana, que assume um espaço plano (2D) e não converte diretamente em quilômetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover variáveis desnecessarias\n",
    "def Limpa_df(df, colunas_para_excluir):\n",
    "    df = df.drop(columns=colunas_para_excluir, errors='ignore')  # ignora colunas que não existem\n",
    "    #df = df.dropna()  # remove linhas com valores ausentes\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mover_target_para_final(df, target):\n",
    "    \"\"\"\n",
    "    Move a variavel alvo para o final do df\n",
    "\n",
    "    Isso nao interfere em resultados, e para manter o padrao de legibilidade e facilitar a visualizacao da target\n",
    "    \"\"\"\n",
    "    colunas = [col for col in df.columns if col != target] + [target]\n",
    "    return df[colunas]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando variavel de distancia em Km entre estabeleciemnto e titular do cartao para compras presencias (nao faz sentido para compras online que podem e muito provavlemente sera, bem distantes)\n",
    "#essa variavel permitira entender se a compra presencial foi muito distante do local do titular do cartao, podendo indicar possivel fraude, isso sera estudado\n",
    "\n",
    "\n",
    "# Definindo as categorias presenciais\n",
    "categorias_presenciais = [\n",
    "    \"misc_pos\", \"grocery_pos\", \"gas_transport\", \n",
    "    \"shopping_pos\", \"personal_care\", \"health_fitness\"\n",
    "]\n",
    "\n",
    "\n",
    "# Função para calcular distância Haversine em KM\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Raio médio da Terra em km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(delta_phi / 2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Aplicar linha a linha\n",
    "df_total[\"distancia_km\"] = df_total.apply(\n",
    "    lambda row: haversine(row[\"lat\"], row[\"long\"], row[\"merch_lat\"], row[\"merch_long\"])\n",
    "    if row[\"category\"] in categorias_presenciais else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "#trata NAN \n",
    "df_total[\"distancia_km\"] = df_total[\"distancia_km\"].fillna(0)\n",
    "\n",
    "\n",
    "df_total.head(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testando a distancia entre as variaveis em um mapa \n",
    "    * sugiro conferir em https://www.movable-type.co.uk/scripts/latlong.html tambem, para ter cereteza da logica de Haversine implantada (comparar os resultados da variavel de distancia pegando as coord e jogando la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observacao=3 #deve ser observacao presencial as demais nao teram valor para essa variavel\n",
    "# Selecionar a primeira observação do df_total\n",
    "obs = df_total[df_total[\"cc_num\"] == df_total[\"cc_num\"].iloc[n_observacao]].iloc[0]\n",
    "\n",
    "# Extrair coordenadas\n",
    "lat1, lon1 = obs['lat'], obs['long']\n",
    "lat2, lon2 = obs['merch_lat'], obs['merch_long']\n",
    "distancia_km = obs['distancia_km']\n",
    "\n",
    "# Criar DataFrame com os dois pontos\n",
    "df_pontos = pd.DataFrame({\n",
    "    'Nome': ['Titular', 'Estabelecimento'],\n",
    "    'Latitude': [lat1, lat2],\n",
    "    'Longitude': [lon1, lon2]\n",
    "})\n",
    "\n",
    "# Criar figura\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar os dois pontos\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=df_pontos['Latitude'],\n",
    "    lon=df_pontos['Longitude'],\n",
    "    mode='markers+text',\n",
    "    text=df_pontos['Nome'],\n",
    "    marker=dict(size=12, color=['purple', 'blue']),\n",
    "    textposition=\"top center\",\n",
    "    name='Pontos'\n",
    "))\n",
    "\n",
    "# Linha entre os pontos\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    lat=[lat1, lat2],\n",
    "    lon=[lon1, lon2],\n",
    "    mode='lines',\n",
    "    line=dict(width=2, color='gray'),\n",
    "    name='Distância reta'\n",
    "))\n",
    "\n",
    "# Ponto médio com rótulo da distância\n",
    "if pd.notna(distancia_km):\n",
    "    lat_meio = (lat1 + lat2) / 2\n",
    "    lon_meio = (lon1 + lon2) / 2\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        lat=[lat_meio],\n",
    "        lon=[lon_meio],\n",
    "        mode='markers+text',\n",
    "        text=[f'{round(distancia_km, 2)} km'],\n",
    "        marker=dict(size=1, color='white'),  # marcador minúsculo e invisível\n",
    "        textfont=dict(size=14, color='black'),\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "# Layout do mapa\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    mapbox_zoom=3,\n",
    "    mapbox_center={\"lat\": (lat1 + lat2) / 2, \"lon\": (lon1 + lon2) / 2},\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "    height=500,\n",
    "    title=f\"Mapa: {obs['cc_num']} | Categoria: {obs['category']}\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificando as quantidades por \"categoria\" (criada so pra analise, nao e uma feature) de distancias, para identificar concentracoes em distancias maiores para transacoes presenciais. \n",
    "    * Distancias muito grandes para compras presenciais e um estabelecimento \"pode\" pontar um padrao de fraudes;\n",
    "    * Ha maior concentracao em fraudes presenciais de 50 a 100 km de distancia entre endereco do titular e o estabelecimento, e uma distancia aceitavel, pode significar viagens, trabalho ... Nada muito discrepante. \n",
    "    * Ha poucas variaveis com km acima de 120, nao parecem ser indcios de padrao de fraude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contagens por faixa\n",
    "faixas = ['0 a 50 km', '50 a 100 km', '100 a 120 km', '120 a 130 km', 'Acima de 130 km']\n",
    "contagens = [\n",
    "    ((df_total[\"distancia_km\"] > 0) & (df_total[\"distancia_km\"] <= 50) & (df_total[\"is_fraud\"] == 1)).sum(),\n",
    "    ((df_total[\"distancia_km\"] > 50) & (df_total[\"distancia_km\"] <= 100) & (df_total[\"is_fraud\"] == 1)).sum(),\n",
    "    ((df_total[\"distancia_km\"] > 100) & (df_total[\"distancia_km\"] <= 120) & (df_total[\"is_fraud\"] == 1)).sum(),\n",
    "    ((df_total[\"distancia_km\"] > 120) & (df_total[\"distancia_km\"] <= 130) & (df_total[\"is_fraud\"] == 1)).sum(),\n",
    "    ((df_total[\"distancia_km\"] > 130) & (df_total[\"is_fraud\"] == 1)).sum()\n",
    "]\n",
    "\n",
    "# Plotando o gráfico\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(faixas, contagens, color='steelblue')\n",
    "\n",
    "# Adicionando rótulos com as contagens\n",
    "for bar, count in zip(bars, contagens):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), str(count), \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.title('Contagem de transações fraudulentas por faixa de distância (em km)')\n",
    "plt.ylabel('Número de transações')\n",
    "plt.xlabel('Faixas de distância')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Total de transações fraudulentas com distância >= 0 km:\", \n",
    "      ((df_total[\"is_fraud\"] == 1) & (df_total[\"distancia_km\"] >= 0)).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia a coluna de trans_num para preserva-la comom coluna de dados no df \n",
    "df_total[\"trans_num_copy\"] = df_total[\"trans_num\"]\n",
    "\n",
    "# Define o índice como trans_num \n",
    "df_total.set_index(\"trans_num_copy\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.index.name = None #tira o cabecalho da variavel trans_num da primeira linha do df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando colunas desnecessarias\n",
    "colunas_excluir = ['unix_time', 'zip', 'state', 'city', 'street', 'gender', 'last', 'first','job','merchant']\n",
    "df_total = Limpa_df(df_total, colunas_excluir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformar variaveis com transformacao relevante (inicialmente) no dicionario de dados CONTINUAR\n",
    "\n",
    "# Converter 'dob' e 'trans_date...' para datetime\n",
    "df_total[\"dob\"] = pd.to_datetime(df_total[\"dob\"])\n",
    "df_total[\"trans_date_trans_time\"] = pd.to_datetime(df_total[\"trans_date_trans_time\"])\n",
    "\n",
    "#calculando a idade\n",
    "#usando a diferenca entre o nascimento e o momento da transacao para evitar distorcoes (usar a data atual criaria uma idade flutuante ao longo do tempo)\n",
    "df_total[\"age\"] = df_total.apply(lambda linha: linha[\"trans_date_trans_time\"].year - linha[\"dob\"].year, axis=1) #funcao lambda \"linha\" que aplica a subtracao de datas linha a linha no df_total atraves do apply()\n",
    "\n",
    "#remove data de nascimento, nao e mais util\n",
    "df_total.drop(columns=[\"dob\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando variavel 'BIN' que corresponde ao codigo do banco da transacao com base no numero do cartao 'cc_num'\n",
    "df_total[\"bin\"] = df_total[\"cc_num\"].astype(str).str[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraindo variaveis do horario da transacao\n",
    "\n",
    "# Certificando que a coluna 'trans_date_trans_time' está no formato datetime\n",
    "df_total['trans_date_trans_time'] = pd.to_datetime(df_total['trans_date_trans_time'])\n",
    "\n",
    "# Extraindo o dia da semana (0=segunda, 1=terça, ..., 6=domingo)\n",
    "df_total['day_of_week'] = df_total['trans_date_trans_time'].dt.dayofweek\n",
    "\n",
    "# Extraindo o mês\n",
    "df_total['month'] = df_total['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Extraindo o horário completo (hora:minuto:segundo)\n",
    "df_total['time'] = df_total['trans_date_trans_time'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "def classify_period(hour):\n",
    "    if 0 <= hour < 3:\n",
    "        return 'Madrugada Início'\n",
    "    elif 3 <= hour < 6:\n",
    "        return 'Madrugada Final'\n",
    "    elif 6 <= hour < 9:\n",
    "        return 'Manhã Início'\n",
    "    elif 9 <= hour < 12:\n",
    "        return 'Manhã Final'\n",
    "    elif 12 <= hour < 15:\n",
    "        return 'Tarde Início'\n",
    "    elif 15 <= hour < 18:\n",
    "        return 'Tarde Final'\n",
    "    elif 18 <= hour < 21:\n",
    "        return 'Noite Início'\n",
    "    elif 21 <= hour < 24:\n",
    "        return 'Noite Final'\n",
    "\n",
    "\n",
    "# Extraindo a hora da transação\n",
    "df_total['hour'] = df_total['trans_date_trans_time'].dt.hour\n",
    "\n",
    "# Aplicando a função para classificar o período\n",
    "df_total['period'] = df_total['hour'].apply(classify_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARAIVEL DE CONTAGEM DE VEZES QUE O CARTAO FOI USADO NAS ULTIMAS 1 HORA \n",
    "# (PRECISA INVESTIGAR SE 1 HORA E MUITO OU POUCO PARA ESSA VARIAVEL, A IDEIA E PEGAR O PADRAO DE TEMPO ENTRE UMA E OUTRA TRANSACAO FRAUDULENTA)\n",
    "\n",
    "# Converter para datetime e ordenar\n",
    "df_total['trans_date_trans_time'] = pd.to_datetime(df_total['trans_date_trans_time'])\n",
    "df_total = df_total.sort_values(by=['trans_date_trans_time', 'cc_num'])\n",
    "\n",
    "# Resetar o índice temporariamente para permitir o uso com numpy (evita erro com string como índice)\n",
    "df_total_reset = df_total.reset_index()  # trans_num vira coluna\n",
    "\n",
    "# Criar array para armazenar a contagem\n",
    "trans_count_list = np.zeros(len(df_total_reset), dtype=int)\n",
    "\n",
    "# Aplicar a contagem eficiente usando searchsorted()\n",
    "for card, group in df_total_reset.groupby('cc_num'):\n",
    "    timestamps = group['trans_date_trans_time'].values\n",
    "    idx = np.searchsorted(timestamps, timestamps - np.timedelta64(1, 'h'), side='left')\n",
    "    trans_count_list[group.index] = np.arange(len(group)) - idx\n",
    "\n",
    "# Atribuir os valores ao DataFrame\n",
    "df_total_reset['trans_count_last_hour'] = trans_count_list\n",
    "\n",
    "# Restaurar o índice original 'trans_num'\n",
    "df_total = df_total_reset.set_index('trans_num')\n",
    "\n",
    "\n",
    "\n",
    "#validando logica (comparar as horas entre as transacoes e se a quantidade bate)\n",
    "#df_filtro = df_total[df_total['cc_num'] == \t4613314721966]\n",
    "#df_filtro = df_filtro[df_filtro['is_fraud'] == 1\t]\n",
    "\n",
    "#df_filtro = df_filtro.sort_values(by='trans_date_trans_time', ascending=False)  # Ordenar do maior para o menor\n",
    "\n",
    "#df_filtro.head(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIAVEL DE QUANTIDADE DE VEZES QUE O CARTAO DE CADA TRANSACAO FOI USADO DURANTE PERIODO TOTAL (AMOSTRA INTEIRA)\n",
    "\n",
    "# Contar quantas vezes cada cartão aparece na base\n",
    "df_qtd_uso_cartoes = df_total.groupby('cc_num').size().reset_index(name='card_usage_count')\n",
    "\n",
    "# Juntar essa informação de volta ao DataFrame original\n",
    "df_total = df_total.merge(df_qtd_uso_cartoes, on='cc_num', how='left')\n",
    "\n",
    "\n",
    "#validando logica\n",
    "#df_filtro_uso = df_qtd_uso_cartoes[df_qtd_uso_cartoes['cc_num']==4613314721966]\n",
    "#df_filtro_uso.head(10)\n",
    "\n",
    "# Define o índice novamente (foi desconfigurado nos processos anteriores por necessidade)\n",
    "df_total.set_index(\"index\", inplace=True)\n",
    "\n",
    "df_total.index.name = None #tira o cabecalho da variavel da primeira linha do df\n",
    "\n",
    "\n",
    "#Aplica def de reorganizar colunas \n",
    "df_total = mover_target_para_final(df_total, 'is_fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformação de Horário em Variáveis Cíclicas (Seno e Cosseno)\n",
    "\n",
    "A variável time, que representa o horário da transação, possui natureza cíclica, ou seja, após 23:59 o ciclo recomeça em 00:00. Modelos de Machine Learning não entendem esse padrão circular por padrão, e tratam 23h e 0h como distantes, quando na verdade são muito próximas.\n",
    "\n",
    "Para capturar essa ciclicidade corretamente, transformamos a hora em duas novas variáveis usando funções trigonométricas:\n",
    "\n",
    "time_sin = sin(2π * hora / 24)\n",
    "\n",
    "time_cos = cos(2π * hora / 24)\n",
    "\n",
    "Essas variáveis projetam o horário em um círculo unitário, permitindo que o modelo entenda a transição natural entre horários e aprenda padrões temporais com mais precisão.\n",
    "\n",
    "Essa técnica é especialmente útil em modelos lineares, onde relações cíclicas não são captadas automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converter a coluna 'time' de string para datetime.time\n",
    "df_total['time'] = pd.to_datetime(df_total['time'], format='%H:%M:%S').dt.time\n",
    "\n",
    "# Extrair a hora, minuto e segundo como número decimal de hora\n",
    "df_total['hora_decimal'] = df_total['time'].apply(lambda x: x.hour + x.minute/60 + x.second/3600)\n",
    "\n",
    "# Codificação cíclica: seno e cosseno da hora do dia\n",
    "df_total['time_sin'] = np.sin(2 * np.pi * df_total['hora_decimal'] / 24)\n",
    "df_total['time_cos'] = np.cos(2 * np.pi * df_total['hora_decimal'] / 24)\n",
    "\n",
    "\n",
    "\n",
    "#dropando variavel 'time' apos transformaca, caso ela seja necessaia para calcular o tempo entre trnasacoes (se essa variavel for viabilizada, esta em analise se faz sentido) \n",
    "#basta comentar essa parte do codigo que ela se mantem \n",
    "\n",
    "# Dropando colunas desnecessarias\n",
    "colunas_excluir = ['time']\n",
    "df_total = Limpa_df(df_total, colunas_excluir)\n",
    "\n",
    "\n",
    "#visualizando nova feature\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_total.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* verificando os tipos das variaveis e a contagem de categorias das categoricas \n",
    "    * das 3 categoricas e possivel notar que a vartaivel bin que corresponde ao codigo do suposto banco de cada transacao, tem muitas categorias (muitos bancos) isso torna inviavel one hot encoder\n",
    "    * Entao na celula seguinte, foi verificado dos bancos mais ofensores em percentual de fraudes por transacao, para encontrar algum padrao \n",
    "    * Dado isso, será aplicada uma transformação de Target Encoding (neste caso, Mean Encoding), que substitui cada categoria de bin pela média do target (proporção média de fraude) dentro daquela categoria. \n",
    "        * Para evitar overfitting e vazamento de dados, a codificação será aplicada separadamente dentro de cada fold durante a validação cruzada e, posteriormente, no conjunto de teste de forma independente. \n",
    "        * Além disso, será utilizado o parâmetro smoothing, que atua como uma forma de regularização. O smoothing realiza um balanceamento entre a média do target por categoria e a média global do target, dando mais peso à média global em categorias com poucas observações. Isso ajuda a suavizar os valores atribuídos a categorias raras e reduz o risco de superestimar seu efeito, tornando o modelo mais robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VERIFICANDO OS TIPOS PARA POSSIVEL TRANSFORMACAO \n",
    "print(df_total.dtypes)\n",
    "\n",
    "print('')\n",
    "# Selecionar colunas do tipo object\n",
    "object_cols = df_total.select_dtypes(include='object').columns\n",
    "\n",
    "# Contar categorias únicas em cada uma\n",
    "for col in object_cols:\n",
    "    print(f\"{col}: {df_total[col].nunique()} categorias únicas\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamento por bin com os principais indicadores\n",
    "resumo_bin = (\n",
    "    df_total\n",
    "    .groupby('bin')\n",
    "    .agg(\n",
    "        proporcao_fraude=('is_fraud', 'mean'),\n",
    "        contagem_fraude=('is_fraud', 'sum'),\n",
    "        total_transacoes=('is_fraud', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by='proporcao_fraude', ascending=False)\n",
    ")\n",
    "\n",
    "# Auemntar para uns 100 para ver a quebra dos bancos mais ofensores em percentual\n",
    "resumo_bin.head(10) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicar a selecao de fatures por importancia antes do modelo (CONTINUAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem (separar um pouco essa fase, aqui tem separacao de treino e teste, optuna, aplicao final... separa para nao ficar pesada a leitura e correcoes, tipo modularizar mesmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* estudar a criacao de uma variavel que identifica a distancia (se e anormal) entre duas transacoes, acho que usar o racional da variavel de contagem de vezes que o cartao foi usado na ultima hora (trans_count_last_hour) \n",
    "ex: uma transacao feita presencialemnte seguida de outra presencialmente em locais muito distantes em 1 hora (ou outro periodo se for o caso) podem indicar uma possivel fraude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "import optuna\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 1. Split em treino e teste\n",
    "# ====================\n",
    "X = df_total.drop('is_fraud', axis=1)\n",
    "y = df_total['is_fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Junta X_train e y_train para uso no Optuna\n",
    "df_train = X_train.copy()\n",
    "df_train['is_fraud'] = y_train\n",
    "\n",
    "target = 'is_fraud'\n",
    "\n",
    "# ====================\n",
    "# 2. Target Encoding\n",
    "# ====================\n",
    "def apply_target_encoding(train, val_or_test, col, target, smoothing=15):\n",
    "    global_mean = train[target].mean()\n",
    "    stats = train.groupby(col)[target].agg(['mean', 'count'])\n",
    "    smooth = (stats['mean'] * stats['count'] + global_mean * smoothing) / (stats['count'] + smoothing)\n",
    "    encoded_col = val_or_test[col].map(smooth).fillna(global_mean)\n",
    "    return encoded_col\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 3. Transformações de Colunas\n",
    "# ====================\n",
    "# One-Hot Encoding para colunas categóricas\n",
    "def fit_transform_ohe(train_df, val_df, col):\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    train_encoded = ohe.fit_transform(train_df[[col]])\n",
    "    val_encoded = ohe.transform(val_df[[col]])\n",
    "\n",
    "    train_ohe = pd.DataFrame(train_encoded, columns=[f\"{col}_{cat}\" for cat in ohe.categories_[0]], index=train_df.index)\n",
    "    val_ohe = pd.DataFrame(val_encoded, columns=[f\"{col}_{cat}\" for cat in ohe.categories_[0]], index=val_df.index)\n",
    "\n",
    "    return train_ohe, val_ohe, ohe\n",
    "\n",
    "# Colunas categóricas e contínuas\n",
    "categorical_features = ['category', 'period']\n",
    "numeric_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 4. Pipeline de Pré-processamento\n",
    "# ====================\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', Pipeline([\n",
    "            ('target_encoding', apply_target_encoding),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_features),\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 5. Otimização com Optuna\n",
    "# ====================\n",
    "\n",
    "#variaveis Globais \n",
    "threshold = 0.5\n",
    "\n",
    "n_trials_= 10\n",
    "\n",
    "n_splits_ = 5\n",
    "\n",
    "weights_skf = {\n",
    "    'Accuracy': 0.00,\n",
    "    'f1': 0.15,\n",
    "    'precision': 0.00,\n",
    "    'recall': 0.70,\n",
    "    'auc': 0.15,\n",
    "    'balanced_acc': 0.0,\n",
    "    'mcc': 0.00\n",
    "}\n",
    "\n",
    "\n",
    "def generate_folds_and_train(trial, df_train, target, n_splits=n_splits_):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metric_sums = {\n",
    "        'Accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "        'auc': 0,\n",
    "        'balanced_acc': 0,\n",
    "        'mcc': 0\n",
    "    }\n",
    "\n",
    "    for train_idx, val_idx in skf.split(df_train, df_train[target]):\n",
    "        train_fold = df_train.iloc[train_idx].copy()\n",
    "        val_fold = df_train.iloc[val_idx].copy()\n",
    "\n",
    "        # ====== Target Encoding ======\n",
    "        train_fold['bin_target_enc'] = apply_target_encoding(train_fold, train_fold, 'bin', target)\n",
    "        val_fold['bin_target_enc'] = apply_target_encoding(train_fold, val_fold, 'bin', target)\n",
    "\n",
    "        # OneHot Encoding nas colunas 'category' e 'period'\n",
    "        train_ohe_cat, val_ohe_cat, _ = fit_transform_ohe(train_fold, val_fold, 'category')\n",
    "        train_ohe_period, val_ohe_period, _ = fit_transform_ohe(train_fold, val_fold, 'period')\n",
    "\n",
    "        # ====== Features Finais ======\n",
    "        X_train = pd.concat([train_fold[['bin_target_enc']], train_ohe_cat, train_ohe_period], axis=1)\n",
    "        X_val = pd.concat([val_fold[['bin_target_enc']], val_ohe_cat, val_ohe_period], axis=1)\n",
    "\n",
    "        # ====== Scaling ======\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        y_train = train_fold[target]\n",
    "        y_val = val_fold[target]\n",
    "\n",
    "        # Calcula scale_pos_weight com base no desequilíbrio (para cada fold e feito o mesmo no treianmetno final se alterar la tem que altera aqui e vice versa )\n",
    "        neg, pos = np.bincount(y_train)\n",
    "        scale_pos_weight = neg / pos\n",
    "\n",
    "        # Modelo\n",
    "        model = XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            tree_method='gpu_hist',  # ou 'hist' se não usar GPU\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 2, 15),\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            gamma=trial.suggest_float(\"gamma\", 0, 5),\n",
    "            reg_alpha=trial.suggest_float(\"reg_alpha\", 0, 5),\n",
    "            reg_lambda=trial.suggest_float(\"reg_lambda\", 0, 5),\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "        # ====== MÉTRICAS ======\n",
    "        metric_sums['Accuracy']     += accuracy_score(y_val, preds)\n",
    "        metric_sums['f1']           += f1_score(y_val, preds)\n",
    "        metric_sums['precision']    += precision_score(y_val, preds, zero_division=0)\n",
    "        metric_sums['recall']       += recall_score(y_val, preds)\n",
    "        metric_sums['auc']          += roc_auc_score(y_val, probs)\n",
    "        metric_sums['balanced_acc'] += balanced_accuracy_score(y_val, preds)\n",
    "        metric_sums['mcc']          += matthews_corrcoef(y_val, preds)\n",
    "\n",
    "    # Média das métricas\n",
    "    mean_metrics = {k: v / n_splits for k, v in metric_sums.items()}\n",
    "    final_score = sum(mean_metrics[k] * weights_skf[k] for k in weights_skf)\n",
    "\n",
    "    return final_score\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 6. Optuna - Otimização\n",
    "# ====================\n",
    "\n",
    "\n",
    "#variaveis globais \n",
    "\n",
    "def objective(trial):\n",
    "    return generate_folds_and_train(trial, df_train, target)\n",
    "\n",
    "\n",
    "sampler_ = optuna.samplers.TPESampler(n_startup_trials=15, \n",
    "                                      n_ei_candidates=30, \n",
    "                                      group=True,seed=42,\n",
    "                                      multivariate=True)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler_, pruner=optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(), patience=15))\n",
    "study.optimize(objective, n_trials=n_trials_)  \n",
    "best_params = study.best_params\n",
    "\n",
    "# Melhores hiperparâmetros\n",
    "print(\"📊 MELHORES HIPERPARÂMETROS ENCONTRADOS\")\n",
    "print(\"═\" * 60)\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param:<25}{value:<15}\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ===========================\n",
    "# 1. Transforma o X_test com base no df_train completo\n",
    "# ===========================\n",
    "def apply_encoding_and_scaling(df_train, df_test, target):\n",
    "    df_test_transformed = df_test.copy()\n",
    "\n",
    "    # Target Encoding\n",
    "    df_test_transformed['bin_target_enc'] = apply_target_encoding(df_train, df_test_transformed, 'bin', target)\n",
    "\n",
    "    # One Hot Encoding\n",
    "    _, test_ohe_category, _ = fit_transform_ohe(df_train, df_test_transformed, 'category')\n",
    "    _, test_ohe_period, _ = fit_transform_ohe(df_train, df_test_transformed, 'period')\n",
    "\n",
    "    X_test = pd.concat([df_test_transformed[['bin_target_enc']], test_ohe_category, test_ohe_period], axis=1)\n",
    "\n",
    "    # Treina transformação no df_train completo\n",
    "    df_train_transf = df_train.copy()\n",
    "    df_train_transf['bin_target_enc'] = apply_target_encoding(df_train, df_train, 'bin', target)\n",
    "    _, train_ohe_category, _ = fit_transform_ohe(df_train, df_train, 'category')\n",
    "    _, train_ohe_period, _ = fit_transform_ohe(df_train, df_train, 'period')\n",
    "\n",
    "    X_train_transf = pd.concat([df_train_transf[['bin_target_enc']], train_ohe_category, train_ohe_period], axis=1)\n",
    "\n",
    "    # Escalador treinado no X_train completo\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_transf)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled = scaler.transform(X_train_transf)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, df_train[target]\n",
    "\n",
    "# ===========================\n",
    "# 2. Transforma treino e teste\n",
    "# ===========================\n",
    "X_train_final, X_test_final, y_train_final = apply_encoding_and_scaling(df_train, X_test, target)\n",
    "\n",
    "# ===========================\n",
    "# 3. Treina o modelo final com os melhores parâmetros do Optuna\n",
    "# ===========================\n",
    "\n",
    "neg, pos = np.bincount(y_train_final)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "\n",
    "model_final = XGBClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    gamma=best_params['gamma'],\n",
    "    reg_alpha=best_params['reg_alpha'],\n",
    "    reg_lambda=best_params['reg_lambda'],\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='gpu_hist'  # ou 'hist' se não for usar GPU\n",
    ")\n",
    "\n",
    "model_final.fit(X_train_final, y_train_final)\n",
    "\n",
    "# ===========================\n",
    "# 4. Gera a predição no treino\n",
    "# ===========================\n",
    "y_train_pred_proba = model_final.predict_proba(X_train_final)[:, 1]\n",
    "\n",
    "# ===========================\n",
    "# 5. Predição no conjunto de teste\n",
    "# ===========================\n",
    "y_test_pred_proba = model_final.predict_proba(X_test_final)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC no conjunto de teste: {roc_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelo(y_train_true, y_train_proba, y_test_true, y_test_proba, threshold=threshold):\n",
    "    \"\"\"\n",
    "    Avalia o desempenho de um modelo de classificação binária com curvas ROC e Precision-Recall.\n",
    "\n",
    "    Parâmetros:\n",
    "        y_train_true (array-like): Valores reais do conjunto de treino.\n",
    "        y_train_proba (array-like): Probabilidades previstas no treino.\n",
    "        y_test_true (array-like): Valores reais do conjunto de teste.\n",
    "        y_test_proba (array-like): Probabilidades previstas no teste.\n",
    "        threshold (float): Limite para converter probabilidades em classes (default=0.5).\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve, roc_auc_score,\n",
    "        precision_recall_curve,\n",
    "        confusion_matrix, ConfusionMatrixDisplay,\n",
    "        accuracy_score, precision_score, recall_score, f1_score\n",
    "    )\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Predição binária com threshold\n",
    "    y_test_pred = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "    # ===== CURVA ROC - Treino e Teste =====\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train_true, y_train_proba)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_true, y_test_proba)\n",
    "    auc_train = roc_auc_score(y_train_true, y_train_proba)\n",
    "    auc_test = roc_auc_score(y_test_true, y_test_proba)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr_train, tpr_train, label=f\"Treino (AUC = {auc_train:.2f})\", color=\"blue\")\n",
    "    plt.plot(fpr_test, tpr_test, label=f\"Teste (AUC = {auc_test:.2f})\", color=\"darkorange\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Curva ROC - Treino vs Teste\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ===== PRECISION vs RECALL =====\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test_true, y_test_proba)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(recall, precision, color=\"green\", lw=2)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision vs Recall Curve (Teste)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ===== MATRIZ DE CONFUSÃO (Teste) =====\n",
    "    cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=\"Blues\", values_format='d')\n",
    "    plt.title(\"Matriz de Confusão (Teste)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ===== MÉTRICAS BINÁRIAS (Teste) =====\n",
    "    accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "    precision_val = precision_score(y_test_true, y_test_pred, zero_division=0)\n",
    "    recall_val = recall_score(y_test_true, y_test_pred)\n",
    "    f1 = f1_score(y_test_true, y_test_pred)\n",
    "\n",
    "    print(\"==== MÉTRICAS DE TESTE ====\")\n",
    "    print(f\"Acurácia:  {accuracy:.4f}\")\n",
    "    print(f\"Precisão:  {precision_val:.4f}\")\n",
    "    print(f\"Recall:    {recall_val:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC AUC:   {auc_test:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#chamndo funcao de avaliar modelo com as metricas \n",
    "\n",
    "avaliar_modelo(y_train, y_train_pred_proba, y_test, y_test_pred_proba, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
